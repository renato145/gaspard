{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1629d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae282a",
   "metadata": {},
   "source": [
    "# Gaspard's source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116139",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect, typing, mimetypes, base64, json, ast, io, os, time, proto\n",
    "import filetype as ft\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types.generation_types import GenerateContentResponse, GenerationConfig\n",
    "from google.generativeai.protos import FunctionCall, Content, FunctionResponse, FunctionDeclaration\n",
    "from google.generativeai.protos import GenerateContentResponse as GCR\n",
    "from google.generativeai import protos\n",
    "\n",
    "import toolslm\n",
    "from toolslm.funccall import *\n",
    "\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.utils import *\n",
    "\n",
    "from collections import abc\n",
    "\n",
    "from proto.marshal.collections.maps import MapComposite\n",
    "from proto.marshal.collections.repeated import RepeatedComposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0764659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc\n",
    "\n",
    "try: from IPython import display\n",
    "except: display=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "UsageMetadata = GCR.UsageMetadata\n",
    "empty = inspect.Parameter.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "models = ('gemini-2.0-flash-exp',\n",
    "          'gemini-exp-1206',\n",
    "          'learnlm-1.5-pro-experimental',\n",
    "          'gemini-exp-1121',\n",
    "          'gemini-1.5-pro',\n",
    "          'gemini-1.5-flash',\n",
    "          'gemini-1.5-flash-8b'\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a095bc",
   "metadata": {},
   "source": [
    "These are the latest version of Gemini models available at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c81236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99b6d9",
   "metadata": {},
   "source": [
    "We'll use the new gemini-2.0-flash for the examples since it's awesome, faster and cheaper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba835d",
   "metadata": {},
   "source": [
    "## Gemini SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a343d78",
   "metadata": {},
   "source": [
    "Follow the [instructions](https://aistudio.google.com/app/apikey) to generate an API key, and set it as an evironment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = genai.GenerativeModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62f521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.23590449725880341\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 8\n",
       "- candidates_token_count: 34\n",
       "- total_token_count: 42\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.23590449725880341\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 8,\n",
       "        \"candidates_token_count\": 34,\n",
       "        \"total_token_count\": 42\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.generate_content(\"Hi, I'm Faisal!\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae6c0b",
   "metadata": {},
   "source": [
    "## Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97739d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def find_block(r:abc.Mapping, # The message to look in\n",
    "              ):\n",
    "    \"Find the content in `r`.\"\n",
    "    m = nested_idx(r, 'candidates', 0)\n",
    "    if not m: return m\n",
    "    if hasattr(m, 'content'): return m.content \n",
    "    else: return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b4144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parts {\n",
       "  text: \"Hi Faisal! It\\'s nice to meet you. I\\'m an AI, and I\\'m here to help. What can I do for you today?\\n\"\n",
       "}\n",
       "role: \"model\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_block(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def contents(r):\n",
    "    \"Helper to get the contents from response `r`.\"\n",
    "    blk = find_block(r)\n",
    "    if not blk: return r\n",
    "    if hasattr(blk, 'parts'): return getattr(blk,'parts')[0].text\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d7340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch()\n",
    "def _repr_markdown_(self:GenerateContentResponse):\n",
    "    met = list(self.to_dict()['candidates'][0].items()) + list(self.to_dict()['usage_metadata'].items())\n",
    "    det = '\\n- '.join(f'{k}: {v}' for k,v in met)\n",
    "    res = contents(self)\n",
    "    if not res: return f\"- {det}\"\n",
    "    return f\"\"\"{contents(self)}\\n<details>\\n\\n- {det}\\n\\n</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3a3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.23590449725880341\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 8\n",
       "- candidates_token_count: 34\n",
       "- total_token_count: 42\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.23590449725880341\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 8,\n",
       "        \"candidates_token_count\": 34,\n",
       "        \"total_token_count\": 42\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 34; Total: 42"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61918cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # Number of input tokens\n",
    "          out=0  # Number of output tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `Usage`.\"\n",
    "    return UsageMetadata(prompt_token_count=inp, candidates_token_count=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443d1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 0; Total: 5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch(as_prop=True)\n",
    "def total(self:UsageMetadata): return self.prompt_token_count+self.candidates_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:UsageMetadata): return f'In: {self.prompt_token_count}; Out: {self.candidates_token_count}; Total: {self.total}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6780c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 34; Total: 42"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fafcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:UsageMetadata, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.prompt_token_count+b.prompt_token_count, self.candidates_token_count+b.candidates_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 16; Out: 68; Total: 84"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage_metadata+r.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bc41b",
   "metadata": {},
   "source": [
    "## Creating messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msg(content, role='user', **kw):\n",
    "    if isinstance(content, GenerateContentResponse):\n",
    "        blk = find_block(content)\n",
    "        role = blk.role\n",
    "        content = blk.parts[0].text\n",
    "    if not isinstance(content, list): content=[content]\n",
    "    return dict(role=role, parts=content, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924dece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'parts': [\"I'm Faisal\"]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm Faisal\"\n",
    "m = mk_msg(prompt)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Faisal, it's nice to meet you! Is there anything I can help you with today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"Hi Faisal, it's nice to meet you! Is there anything I can help you with today?\\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.17574180256236682\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 5\n",
       "- candidates_token_count: 22\n",
       "- total_token_count: 27\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi Faisal, it's nice to meet you! Is there anything I can help you with today?\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.17574180256236682\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 5,\n",
       "        \"candidates_token_count\": 22,\n",
       "        \"total_token_count\": 27\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.generate_content([m], generation_config=GenerationConfig(max_output_tokens=100))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ede3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': [\"I'm Faisal\"]},\n",
       " {'role': 'model',\n",
       "  'parts': [\"Hi Faisal, it's nice to meet you! Is there anything I can help you with today?\\n\"]},\n",
       " {'role': 'user', 'parts': ['I forgot my name. Can you remind me please?']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [mk_msg(prompt), mk_msg(r), mk_msg('I forgot my name. Can you remind me please?')]\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708fc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, Faisal, I can help with that! You just told me your name is **Faisal**. Don't worry, it happens to everyone sometimes! Is there anything else I can help you with?\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"Okay, Faisal, I can help with that! You just told me your name is **Faisal**. Don't worry, it happens to everyone sometimes! Is there anything else I can help you with?\\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.27289701062579486\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 40\n",
       "- candidates_token_count: 43\n",
       "- total_token_count: 83\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Okay, Faisal, I can help with that! You just told me your name is **Faisal**. Don't worry, it happens to everyone sometimes! Is there anything else I can help you with?\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.27289701062579486\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 40,\n",
       "        \"candidates_token_count\": 43,\n",
       "        \"total_token_count\": 83\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli.generate_content(msgs, generation_config=GenerationConfig(max_output_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a573d",
   "metadata": {},
   "source": [
    "Let's make this a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08352a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msgs(msgs:list, **kw):\n",
    "    \"Helper to set 'assistant' role on alternate messages.\"\n",
    "    if isinstance(msgs,str): msgs=[msgs]\n",
    "    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27ee82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': [\"Hi, I'm Faisal!\"]},\n",
       " {'role': 'model',\n",
       "  'parts': [\"Hi Faisal, it's nice to meet you! Is there anything I can help you with today?\\n\"]},\n",
       " {'role': 'user', 'parts': ['I forgot my name. Can you remind me please?']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs([\"Hi, I'm Faisal!\", r, \"I forgot my name. Can you remind me please?\"]); msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcb0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Your name is Faisal. ðŸ˜Š\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Of course! Your name is Faisal. ðŸ˜Š\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.5640183448791504\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 43\n",
       "- candidates_token_count: 10\n",
       "- total_token_count: 53\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Of course! Your name is Faisal. \\ud83d\\ude0a\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.5640183448791504\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 43,\n",
       "        \"candidates_token_count\": 10,\n",
       "        \"total_token_count\": 53\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli.generate_content(msgs, generation_config=GenerationConfig(max_output_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec007f7f",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05864cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Client:\n",
    "    def __init__(self, model, cli=None, sp=None):\n",
    "        \"Basic LLM messages client.\"\n",
    "        self.model,self.use = model,usage(0,0)\n",
    "        self.sp = sp\n",
    "        self.c = (cli or genai.GenerativeModel(model, system_instruction=sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db82494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 0; Out: 0; Total: 0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(model)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198eac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _r(self:Client, r:GenerateContentResponse):\n",
    "    \"Store the result of the message and accrue total usage.\"\n",
    "    self.result = r\n",
    "    if getattr(r,'usage_metadata',None): self.use += r.usage_metadata\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8466e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 22; Total: 27"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._r(r)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c87c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stream(r):\n",
    "    for o in r:\n",
    "        o = contents(o)\n",
    "        if o and isinstance(o, str): yield(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _set_sp(self:Client, sp:str):\n",
    "    if sp != self.sp:\n",
    "        self.sp = sp\n",
    "        self.c = genai.GenerativeModel(self.model, system_instruction=self.sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5166bd",
   "metadata": {},
   "source": [
    "Gemini cli requires passing the system prompt when creating the client, so we recreate the client for now.\n",
    "\n",
    "TODO: Ask Google to surface this option to generate_content function, since they're passing the system prompt to each request anyways [under the hood](https://github.com/googleapis/python-aiplatform/blob/f89df1f30822d260176487f74c3743cab88a38fd/vertexai/generative_models/_generative_models.py#L446)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba14a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _precall(self:Client, msgs):\n",
    "    if not isinstance(msgs,list): msgs = [msgs]\n",
    "    msgs = mk_msgs(msgs)\n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bac993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(genai.GenerativeModel.generate_content)\n",
    "def __call__(self:Client,\n",
    "             msgs:list, # List of messages in the dialog\n",
    "             sp:str=None, # System prompt\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream:bool=False, # Stream response?\n",
    "             **kwargs):\n",
    "    \"Make a call to LLM.\"\n",
    "    if sp: self._set_sp(sp)\n",
    "    msgs = self._precall(msgs)\n",
    "    gc_params = inspect.signature(GenerationConfig.__init__).parameters\n",
    "    gc_kwargs = {k: v for k, v in kwargs.items() if k in gc_params}\n",
    "    gen_config = GenerationConfig(max_output_tokens=maxtok, **gc_kwargs)\n",
    "    gen_params = inspect.signature(self.c.generate_content).parameters\n",
    "    gen_kwargs = {k: v for k, v in kwargs.items() if k in gen_params}\n",
    "    r = self.c.generate_content(\n",
    "        contents=msgs, generation_config=gen_config, stream=stream, **gen_kwargs)\n",
    "    if not stream: return self._r(r)\n",
    "    else: return get_stream(map(self._r, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today?\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.c.generate_content('hi').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50775d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Hi there! How can I help you today?\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.017421615394678982\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 2\n",
       "- candidates_token_count: 11\n",
       "- total_token_count: 13\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi there! How can I help you today?\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.017421615394678982\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 2,\n",
       "        \"candidates_token_count\": 11,\n",
       "        \"total_token_count\": 13\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb4ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 7; Out: 33; Total: 40"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97727a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "for o in c(msgs, stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37358f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 11; Out: 44; Total: 55"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c36641",
   "metadata": {},
   "source": [
    "Gemini cli requires passing the system prompt when creating the client, but we didn't pass one at creation time.\n",
    "Let's make sure that it gets set properly when we call the client later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"Respond only in emojis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a978d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ðŸ‘‹\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'ðŸ‘‹\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.00037874732515774667\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 6\n",
       "- candidates_token_count: 2\n",
       "- total_token_count: 8\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"\\ud83d\\udc4b\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.00037874732515774667\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 6,\n",
       "        \"candidates_token_count\": 2,\n",
       "        \"total_token_count\": 8\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs, sp=sysp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f168bb",
   "metadata": {},
   "source": [
    "We've shown the token usage but we really care about is pricing. Let's extract the latest [pricing](https://ai.google.dev/pricing#1_5pro) from Google into a pricing dict. Currently, the experimental version Gemini 2.0 Flash is free, and the pricing of other experimental models is not entirely clear. Since there's rumors they are two experimental version of the upcoming Gemini 2.0 Pro, they are priced as 1.5 Pro. Better safe than sorry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "pricing = {  # model type: $ / million tokens (input, output, cache, input_long, output_long, cache_long)\n",
    "    'gemini-2.0-flash-exp': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0),\n",
    "    'gemini-1.5-pro': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n",
    "    'gemini-1.5-flash': (0.075, 0.30, 0.01875, 0.15, 0.60, 0.0375),\n",
    "    'gemini-exp-1206': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n",
    "    'gemini-exp-1121': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n",
    "    'learnlm-1.5-pro-experimental': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab805798",
   "metadata": {},
   "source": [
    "Now let's add a cost prop to `Client` to calculate the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_pricing(m, u):\n",
    "    return pricing[m][:3] if u.prompt_token_count < 128_000 else pricing[m][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch(as_prop=True)\n",
    "def cost(self:Client):\n",
    "    inp_cost, out_cost, cache_cost = get_pricing(self.model.split('-exp-')[0], self.use)\n",
    "    return (self.use.prompt_token_count * inp_cost + self.use.candidates_token_count * out_cost + self.use.cached_content_token_count * cache_cost) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a48912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:Client):\n",
    "    if not hasattr(self,'result'): return 'No results yet'\n",
    "    msg = contents(self.result)\n",
    "    inp_cost,out_cost,_ = get_pricing(self.model.split('-exp-')[0], self.use)\n",
    "    in_cost = self.use.prompt_token_count * inp_cost/1e6\n",
    "    out_cost = self.use.candidates_token_count * out_cost/1e6\n",
    "    cache_cost = self.use.cached_content_token_count * out_cost/1e6\n",
    "    return f\"\"\"{msg}\n",
    "\n",
    "| Metric | Count | Cost (USD) |\n",
    "|--------|------:|-----:|\n",
    "| Input tokens | {self.use.prompt_token_count:,} | {in_cost:.6f} |\n",
    "| Output tokens | {self.use.candidates_token_count:,} | {out_cost:.6f} |\n",
    "| Cache tokens | {self.use.cached_content_token_count:,} | {cache_cost:.6f} |\n",
    "| **Total** | **{self.use.total:,}** | **${self.cost:.6f}** |\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8304456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ðŸ‘‹\n",
       "\n",
       "\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 17 | 0.000000 |\n",
       "| Output tokens | 46 | 0.000000 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **63** | **$0.000000** |"
      ],
      "text/plain": [
       "<__main__.Client>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dee9a",
   "metadata": {},
   "source": [
    "## Tool Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(\n",
    "    a:int,  # First thing to sum\n",
    "    b:int # Second thing to sum\n",
    ") -> int: # The sum of the inputs\n",
    "    \"Adds a + b.\"\n",
    "    print(f\"Finding the sum of {a} and {b}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 604542,6458932\n",
    "pr = f\"What is {a}+{b}?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ec83c",
   "metadata": {},
   "source": [
    "Google's Genai API handles schema exatraction under the hood, so we can just directly pass the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f714d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 604542.0, 'b': 6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -5.444032770659153e-06\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 86\n",
       "- candidates_token_count: 3\n",
       "- total_token_count: 89\n",
       "- cached_content_token_count: 0"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": 6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -5.444032770659153e-06\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 86,\n",
       "        \"candidates_token_count\": 3,\n",
       "        \"total_token_count\": 89\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = c(pr, sp=sysp, tools=[sums])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ea030",
   "metadata": {},
   "source": [
    "Looks like our output isn't pretty anymore. Let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc5048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def contents(r):\n",
    "    \"Helper to get the contents from response `r`.\"\n",
    "    blk = find_block(r)\n",
    "    if not blk: return r\n",
    "    \n",
    "    if hasattr(blk, 'parts'):\n",
    "        part = blk.parts[0]\n",
    "        if 'text' in part:\n",
    "            return part.text\n",
    "        else:\n",
    "            return part\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d509ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a9585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 604542.0, 'b': 6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -5.444032770659153e-06\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 86\n",
       "- candidates_token_count: 3\n",
       "- total_token_count: 89\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": 6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -5.444032770659153e-06\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 86,\n",
       "        \"candidates_token_count\": 3,\n",
       "        \"total_token_count\": 89\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163e293",
   "metadata": {},
   "source": [
    "B-e-a-utiful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441a121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parts {\n",
       "  function_call {\n",
       "    name: \"sums\"\n",
       "    args {\n",
       "      fields {\n",
       "        key: \"b\"\n",
       "        value {\n",
       "          number_value: 6458932\n",
       "        }\n",
       "      }\n",
       "      fields {\n",
       "        key: \"a\"\n",
       "        value {\n",
       "          number_value: 604542\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "role: \"model\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = find_block(r); m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"sums\"\n",
       "args {\n",
       "  fields {\n",
       "    key: \"b\"\n",
       "    value {\n",
       "      number_value: 6458932\n",
       "    }\n",
       "  }\n",
       "  fields {\n",
       "    key: \"a\"\n",
       "    value {\n",
       "      number_value: 604542\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = m.parts[0].function_call; func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365966dd",
   "metadata": {},
   "source": [
    "Let's get the returned function call into a format that is expected by `call_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 604542.0\n",
      "b 6458932.0\n"
     ]
    }
   ],
   "source": [
    "for k,v in func.args.items(): print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36851c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_args(args): return {k: v for k,v in args.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb429a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 604542.0, 'b': 6458932.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_args(func.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def convert_func(f): return AttrDict(name=f.name, inputs=mk_args(f.args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977445b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{'inputs': {'a': 604542.0, 'b': 6458932.0}, 'name': 'sums'}\n",
       "```"
      ],
      "text/plain": [
       "{'name': 'sums', 'inputs': {'a': 604542.0, 'b': 6458932.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = convert_func(func); func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9127c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sums': <function __main__.sums(a: int, b: int) -> int>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = mk_ns(sums); ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d387b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542.0 and 6458932.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7063474.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = call_func(func.name, func.inputs, ns); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb08798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msg(content, role='user', **kw):\n",
    "    if isinstance(content, GenerateContentResponse): role,content = 'model',contents(content)\n",
    "    if isinstance(content, dict): role,content = content['role'],content['parts']\n",
    "    if not isinstance(content, list): content=[content]\n",
    "    return dict(role=role, parts=content, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b788a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_toolres(\n",
    "    r:abc.Mapping, # Tool use request response\n",
    "    ns, # Namespace to search for tools\n",
    "    ):\n",
    "    \"Create a `tool_result` message from response `r`.\"\n",
    "    parts = find_block(r).parts\n",
    "    tcs = [p.function_call for p in parts if hasattr(p, 'function_call')]\n",
    "    res = [mk_msg(r)]\n",
    "    tc_res = []\n",
    "    for func in (tcs or []):\n",
    "        if not func: continue\n",
    "        func = convert_func(func)\n",
    "        cts = call_func(func.name, func.inputs, ns=ns)\n",
    "        tc_res.append(FunctionResponse(name=func.name, response={'result': cts}))\n",
    "    if tc_res: res.append(mk_msg(tc_res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542.0 and 6458932.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'parts': [name: \"sums\"\n",
       "  response {\n",
       "    fields {\n",
       "      key: \"result\"\n",
       "      value {\n",
       "        number_value: 7063474\n",
       "      }\n",
       "    }\n",
       "  }]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r, ns=ns)\n",
    "tr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609ed0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is 604542+6458932?',\n",
       " {'role': 'model', 'parts': [function_call {\n",
       "     name: \"sums\"\n",
       "     args {\n",
       "       fields {\n",
       "         key: \"b\"\n",
       "         value {\n",
       "           number_value: 6458932\n",
       "         }\n",
       "       }\n",
       "       fields {\n",
       "         key: \"a\"\n",
       "         value {\n",
       "           number_value: 604542\n",
       "         }\n",
       "       }\n",
       "     }\n",
       "   }]},\n",
       " {'role': 'user',\n",
       "  'parts': [name: \"sums\"\n",
       "   response {\n",
       "     fields {\n",
       "       key: \"result\"\n",
       "       value {\n",
       "         number_value: 7063474\n",
       "       }\n",
       "     }\n",
       "   }]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [pr] + tr; msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msgs(msgs:list, **kw):\n",
    "    \"Helper to set 'assistant' role on alternate messages.\"\n",
    "    if isinstance(msgs,str): msgs=[msgs]\n",
    "    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7848bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': ['What is 604542+6458932?']},\n",
       " {'role': 'model',\n",
       "  'parts': [function_call {\n",
       "     name: \"sums\"\n",
       "     args {\n",
       "       fields {\n",
       "         key: \"b\"\n",
       "         value {\n",
       "           number_value: 6458932\n",
       "         }\n",
       "       }\n",
       "       fields {\n",
       "         key: \"a\"\n",
       "         value {\n",
       "           number_value: 604542\n",
       "         }\n",
       "       }\n",
       "     }\n",
       "   }]},\n",
       " {'role': 'user',\n",
       "  'parts': [name: \"sums\"\n",
       "   response {\n",
       "     fields {\n",
       "       key: \"result\"\n",
       "       value {\n",
       "         number_value: 7063474\n",
       "       }\n",
       "     }\n",
       "   }]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_msgs(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1a15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "7063474\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': '7063474\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.09744896739721298\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 136\n",
       "- candidates_token_count: 8\n",
       "- total_token_count: 144\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"7063474\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.09744896739721298\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 136,\n",
       "        \"candidates_token_count\": 8,\n",
       "        \"total_token_count\": 144\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(msgs, sp=sysp, tools=[sums])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e310c",
   "metadata": {},
   "source": [
    "We can also force a particular set of tools to be used using, `tool_config`. Here's an example of how to do that for genai api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1badc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tool_config(choose: list)->dict:\n",
    "    return {\"function_calling_config\": {\"mode\": \"ANY\", \"allowed_function_names\": [x.__name__ for x in choose]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dc557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_calling_config': {'mode': 'ANY',\n",
       "  'allowed_function_names': ['sums']}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_config = mk_tool_config([sums]); tool_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba07ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 2\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 1.0, 'b': 2.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.00484422439088424\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 70\n",
       "- candidates_token_count: 3\n",
       "- total_token_count: 73\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 1.0,\n",
       "                    \"b\": 2.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.00484422439088424\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 70,\n",
       "        \"candidates_token_count\": 3,\n",
       "        \"total_token_count\": 73\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c('Howdy!', tools=[sums], tool_config=tool_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50421e42",
   "metadata": {},
   "source": [
    "## Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787ff76",
   "metadata": {},
   "source": [
    "We can also use tool calling to force the model to return structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "def structured(self:Client,\n",
    "               msgs:list, # The prompt or list of prompts\n",
    "               tools:list, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    if not isinstance(msgs, list): msgs = [msgs]\n",
    "    if not isinstance(tools, list): tools = [tools]\n",
    "    kwargs['tools'] = tools\n",
    "    kwargs['tool_config'] = mk_tool_config(tools)\n",
    "    res = self(msgs, **kwargs)\n",
    "    ns=mk_ns(*tools)\n",
    "    parts = find_block(res).parts\n",
    "    funcs = [convert_func(p.function_call) for p in parts if hasattr(p, 'function_call')]\n",
    "    tcs = [call_func(func.name, func.inputs, ns=ns) for func in funcs]\n",
    "    return tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15813bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe(BasicRepr):\n",
    "    \"A structure for representing recipes.\"\n",
    "    def __init__(self, recipe_name: str, ingredients: list[str]): store_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6def5",
   "metadata": {},
   "source": [
    "Gemini API schema extraction doesn't work very well for Class definitions so we define a factory method as a workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7083831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recipe(recipe_name='chocolate chip cookies', ingredients=['butter', 'sugar', 'egg', 'vanilla extract', 'flour', 'baking soda', 'salt', 'chocolate chips'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = \"Give me a receipe for chocolate chip cookies\"\n",
    "recipe = c.structured(pr, tools=[Recipe], sp=sysp)[0]; recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b51ff0",
   "metadata": {},
   "source": [
    "This works great, however, to handle to complex structured output usecases we need to manually create the schema objects to feed to the GenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Turn(BasicRepr):\n",
    "    \"Turn in the conversation\"\n",
    "    def __init__(self, msg_a: str, msg_b: str): store_attr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation(BasicRepr):\n",
    "    \"A conversation between two people\"\n",
    "    def __init__(self, turns: list[Turn]): store_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9b992",
   "metadata": {},
   "source": [
    "Let's do this manually using GenAI's special protobuf schema types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c95a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn = genai.protos.Schema(\n",
    "    type = genai.protos.Type.OBJECT,\n",
    "    properties = {\n",
    "        'msg_a':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
    "        'msg_b':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
    "    },\n",
    "    required=['msg_a', 'msg_b']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ca8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = genai.protos.Schema(\n",
    "    type = genai.protos.Type.OBJECT,\n",
    "    properties = {\n",
    "        'turns':  genai.protos.Schema(type=genai.protos.Type.ARRAY, items=turn)\n",
    "    },\n",
    "    required=['turns']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f81ed",
   "metadata": {},
   "source": [
    "Great, now, let's wrap this into the necessary function declaration that will then be used as a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11391714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"create_convo\"\n",
       "description: \"Creates a conversation\"\n",
       "parameters {\n",
       "  type_: OBJECT\n",
       "  properties {\n",
       "    key: \"turns\"\n",
       "    value {\n",
       "      type_: ARRAY\n",
       "      items {\n",
       "        type_: OBJECT\n",
       "        properties {\n",
       "          key: \"msg_b\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        properties {\n",
       "          key: \"msg_a\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        required: \"msg_a\"\n",
       "        required: \"msg_b\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  required: \"turns\"\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_convo = genai.protos.FunctionDeclaration(\n",
    "    name=\"create_convo\",\n",
    "    description=\"Creates a conversation\",\n",
    "    parameters=convo\n",
    "); create_convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = genai.GenerativeModel(model_name=model, tools = [create_convo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11c4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"create_convo\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"turns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            struct_value {\n",
       "              fields {\n",
       "                key: \"msg_b\"\n",
       "                value {\n",
       "                  string_value: \"hi\"\n",
       "                }\n",
       "              }\n",
       "              fields {\n",
       "                key: \"msg_a\"\n",
       "                value {\n",
       "                  string_value: \"hello\"\n",
       "                }\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'create_convo', 'args': {'turns': [{'msg_b': 'hi', 'msg_a': 'hello'}]}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.2613382706275353\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 109\n",
       "- candidates_token_count: 13\n",
       "- total_token_count: 122\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"create_convo\",\n",
       "                  \"args\": {\n",
       "                    \"turns\": [\n",
       "                      {\n",
       "                        \"msg_b\": \"hi\",\n",
       "                        \"msg_a\": \"hello\"\n",
       "                      }\n",
       "                    ]\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.2613382706275353\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 109,\n",
       "        \"candidates_token_count\": 13,\n",
       "        \"total_token_count\": 122\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = gen_model.generate_content(pr, tool_config={'function_calling_config':'ANY'}); result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1cf965",
   "metadata": {},
   "source": [
    "Great, now let's start by taking our normal JSON schema that we get from the helper function `get_schema` and converting it to a protobuf schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "j2p_map = {\n",
    "    'string': protos.Type.STRING,\n",
    "    'array': protos.Type.ARRAY,\n",
    "    'object': protos.Type.OBJECT,\n",
    "    'integer': protos.Type.INTEGER,\n",
    "    'number': protos.Type.NUMBER,\n",
    "    'boolean': protos.Type.BOOLEAN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00344ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Conversation',\n",
       " 'description': 'A conversation between two people',\n",
       " 'input_schema': {'type': 'object',\n",
       "  'properties': {'turns': {'type': 'array',\n",
       "    'description': '',\n",
       "    'items': {'$ref': '#/$defs/Turn'}}},\n",
       "  'title': 'Conversation',\n",
       "  'required': ['turns'],\n",
       "  '$defs': {'Turn': {'type': 'object',\n",
       "    'properties': {'msg_a': {'type': 'string', 'description': ''},\n",
       "     'msg_b': {'type': 'string', 'description': ''}},\n",
       "    'title': 'Turn',\n",
       "    'required': ['msg_a', 'msg_b']}}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_schema = get_schema(Conversation); json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def json2proto(schema_dict):\n",
    "    \"Convert JSON schema to protobuf schema\"\n",
    "    def _convert_type(t):\n",
    "        return {'string': protos.Type.STRING, 'array': protos.Type.ARRAY, 'object': protos.Type.OBJECT}.get(t, protos.Type.TYPE_UNSPECIFIED)\n",
    "    \n",
    "    def _convert_property(prop, depth=0):\n",
    "        schema = protos.Schema(type=j2p_map.get(prop.get('type'), protos.Type.TYPE_UNSPECIFIED))\n",
    "        if 'items' in prop:\n",
    "            ref = prop['items'].get('$ref')\n",
    "            schema.items = _convert_property(schema_dict['input_schema']['$defs'][ref.split('/')[-1]], depth+1) if ref else _convert_property(prop['items'], depth+1)\n",
    "        if 'properties' in prop: schema.properties = {k: _convert_property(v, depth+1) for k,v in prop['properties'].items()}\n",
    "        if 'required' in prop: schema.required.extend(prop['required'])\n",
    "        return schema\n",
    "    \n",
    "    return _convert_property(schema_dict['input_schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227adccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_: OBJECT\n",
       "properties {\n",
       "  key: \"turns\"\n",
       "  value {\n",
       "    type_: ARRAY\n",
       "    items {\n",
       "      type_: OBJECT\n",
       "      properties {\n",
       "        key: \"msg_b\"\n",
       "        value {\n",
       "          type_: STRING\n",
       "        }\n",
       "      }\n",
       "      properties {\n",
       "        key: \"msg_a\"\n",
       "        value {\n",
       "          type_: STRING\n",
       "        }\n",
       "      }\n",
       "      required: \"msg_a\"\n",
       "      required: \"msg_b\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "required: \"turns\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the Conversation schema\n",
    "proto_schema = json2proto(json_schema); proto_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2a641",
   "metadata": {},
   "source": [
    "Let's now make it into a proper tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b37434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cls2tool(c) -> genai.protos.FunctionDeclaration:\n",
    "    json_schema = get_schema(c)\n",
    "    schema = json2proto(json_schema)\n",
    "    return genai.protos.FunctionDeclaration(\n",
    "        name=json_schema['name'],\n",
    "        description=json_schema['description'],\n",
    "        parameters=schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56b482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"Conversation\"\n",
       "description: \"A conversation between two people\"\n",
       "parameters {\n",
       "  type_: OBJECT\n",
       "  properties {\n",
       "    key: \"turns\"\n",
       "    value {\n",
       "      type_: ARRAY\n",
       "      items {\n",
       "        type_: OBJECT\n",
       "        properties {\n",
       "          key: \"msg_b\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        properties {\n",
       "          key: \"msg_a\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        required: \"msg_a\"\n",
       "        required: \"msg_b\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  required: \"turns\"\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_convo = cls2tool(Conversation); create_convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26648df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"create_convo\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"turns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            struct_value {\n",
       "              fields {\n",
       "                key: \"msg_b\"\n",
       "                value {\n",
       "                  string_value: \"Okay, here is a recipe:\\\\n\\\\nIngredients:\\\\n\\\\n1 cup (2 sticks) unsalted butter, softened\\\\n1 cup granulated sugar\\\\n1 cup packed brown sugar\\\\n2 teaspoons pure vanilla extract\\\\n2 large eggs\\\\n3 cups all-purpose flour\\\\n1 teaspoon baking soda\\\\n1 teaspoon salt\\\\n2 cups chocolate chips\\\\n\\\\nInstructions\\\\n\\\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\\\nBeat in the vanilla extract, then the eggs one at a time.\\\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\\\nStir in the chocolate chips.\\\\nDrop by rounded tablespoons onto the prepared baking sheets.\\\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.\"\n",
       "                }\n",
       "              }\n",
       "              fields {\n",
       "                key: \"msg_a\"\n",
       "                value {\n",
       "                  string_value: \"Hey, I\\'d love a chocolate chip cookie recipe.\"\n",
       "                }\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'create_convo', 'args': {'turns': [{'msg_b': 'Okay, here is a recipe:\\\\n\\\\nIngredients:\\\\n\\\\n1 cup (2 sticks) unsalted butter, softened\\\\n1 cup granulated sugar\\\\n1 cup packed brown sugar\\\\n2 teaspoons pure vanilla extract\\\\n2 large eggs\\\\n3 cups all-purpose flour\\\\n1 teaspoon baking soda\\\\n1 teaspoon salt\\\\n2 cups chocolate chips\\\\n\\\\nInstructions\\\\n\\\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\\\nBeat in the vanilla extract, then the eggs one at a time.\\\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\\\nStir in the chocolate chips.\\\\nDrop by rounded tablespoons onto the prepared baking sheets.\\\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.', 'msg_a': \"Hey, I'd love a chocolate chip cookie recipe.\"}]}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- citation_metadata: {'citation_sources': [{'start_index': 925, 'end_index': 1046, 'uri': 'https://dinneronceagain.com/tag/cooking/feed/'}]}\n",
       "- avg_logprobs: -0.09633702817170517\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 109\n",
       "- candidates_token_count: 253\n",
       "- total_token_count: 362\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"create_convo\",\n",
       "                  \"args\": {\n",
       "                    \"turns\": [\n",
       "                      {\n",
       "                        \"msg_b\": \"Okay, here is a recipe:\\\\n\\\\nIngredients:\\\\n\\\\n1 cup (2 sticks) unsalted butter, softened\\\\n1 cup granulated sugar\\\\n1 cup packed brown sugar\\\\n2 teaspoons pure vanilla extract\\\\n2 large eggs\\\\n3 cups all-purpose flour\\\\n1 teaspoon baking soda\\\\n1 teaspoon salt\\\\n2 cups chocolate chips\\\\n\\\\nInstructions\\\\n\\\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\\\nBeat in the vanilla extract, then the eggs one at a time.\\\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\\\nStir in the chocolate chips.\\\\nDrop by rounded tablespoons onto the prepared baking sheets.\\\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.\",\n",
       "                        \"msg_a\": \"Hey, I'd love a chocolate chip cookie recipe.\"\n",
       "                      }\n",
       "                    ]\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"citation_metadata\": {\n",
       "            \"citation_sources\": [\n",
       "              {\n",
       "                \"start_index\": 925,\n",
       "                \"end_index\": 1046,\n",
       "                \"uri\": \"https://dinneronceagain.com/tag/cooking/feed/\"\n",
       "              }\n",
       "            ]\n",
       "          },\n",
       "          \"avg_logprobs\": -0.09633702817170517\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 109,\n",
       "        \"candidates_token_count\": 253,\n",
       "        \"total_token_count\": 362\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = gen_model.generate_content(pr, tool_config={'function_calling_config':'ANY'}); result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afcb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"create_convo\"\n",
       "args {\n",
       "  fields {\n",
       "    key: \"turns\"\n",
       "    value {\n",
       "      list_value {\n",
       "        values {\n",
       "          struct_value {\n",
       "            fields {\n",
       "              key: \"msg_b\"\n",
       "              value {\n",
       "                string_value: \"Okay, here is a recipe:\\\\n\\\\nIngredients:\\\\n\\\\n1 cup (2 sticks) unsalted butter, softened\\\\n1 cup granulated sugar\\\\n1 cup packed brown sugar\\\\n2 teaspoons pure vanilla extract\\\\n2 large eggs\\\\n3 cups all-purpose flour\\\\n1 teaspoon baking soda\\\\n1 teaspoon salt\\\\n2 cups chocolate chips\\\\n\\\\nInstructions\\\\n\\\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\\\nBeat in the vanilla extract, then the eggs one at a time.\\\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\\\nStir in the chocolate chips.\\\\nDrop by rounded tablespoons onto the prepared baking sheets.\\\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.\"\n",
       "              }\n",
       "            }\n",
       "            fields {\n",
       "              key: \"msg_a\"\n",
       "              value {\n",
       "                string_value: \"Hey, I\\'d love a chocolate chip cookie recipe.\"\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = contents(result).function_call; func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96499fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<proto.marshal.collections.maps.MapComposite>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e017f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'turns': [<proto.marshal.collections.maps.MapComposite object>]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = mk_args(func.args); args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133459e6",
   "metadata": {},
   "source": [
    "Let's update `mk_args` to handle nested proto objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01eb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _convert_proto(o):\n",
    "    \"Convert proto objects to Python dicts and lists\"\n",
    "    if isinstance(o, (dict,MapComposite)): return {k:_convert_proto(v) for k,v in o.items()}\n",
    "    elif isinstance(o, (list,RepeatedComposite)): return [_convert_proto(v) for v in o]\n",
    "    elif hasattr(o, 'DESCRIPTOR'): return {k.name:_convert_proto(getattr(o,k.name)) for k in o.DESCRIPTOR.fields}\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_args(args):\n",
    "    if isinstance(args, MapComposite): return _convert_proto(args)\n",
    "    return {k: v for k,v in args.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d8989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'turns': [{'msg_b': 'Okay, here is a recipe:\\\\n\\\\nIngredients:\\\\n\\\\n1 cup (2 sticks) unsalted butter, softened\\\\n1 cup granulated sugar\\\\n1 cup packed brown sugar\\\\n2 teaspoons pure vanilla extract\\\\n2 large eggs\\\\n3 cups all-purpose flour\\\\n1 teaspoon baking soda\\\\n1 teaspoon salt\\\\n2 cups chocolate chips\\\\n\\\\nInstructions\\\\n\\\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\\\nBeat in the vanilla extract, then the eggs one at a time.\\\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\\\nStir in the chocolate chips.\\\\nDrop by rounded tablespoons onto the prepared baking sheets.\\\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.',\n",
       "   'msg_a': \"Hey, I'd love a chocolate chip cookie recipe.\"}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = mk_args(func.args); args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_tool_config(choose: list)->dict:\n",
    "    return {\"function_calling_config\": {\"mode\": \"ANY\", \"allowed_function_names\":\n",
    "    [x.__name__ if hasattr(x, '__name__') else x.name for x in choose]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f72d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "def structured(self:Client,\n",
    "               msgs:list, # The prompt or list of prompts\n",
    "               tools:list, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    if not isinstance(msgs, list): msgs = [msgs]\n",
    "    if not isinstance(tools, list): tools = [tools]\n",
    "    kwargs['tools'] = [cls2tool(x) for x in tools]\n",
    "    kwargs['tool_config'] = mk_tool_config(kwargs['tools'])\n",
    "    res = self(msgs, **kwargs)\n",
    "    ns=mk_ns(*tools)\n",
    "    parts = find_block(res).parts\n",
    "    funcs = [convert_func(p.function_call) for p in parts if hasattr(p, 'function_call')]\n",
    "    tcs = [call_func(func.name, func.inputs, ns=ns) for func in funcs]\n",
    "    return tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation(turns=[{'msg_b': 'Robert J. Oppenheimer: Indeed, Professor Einstein. Its mysteries both fascinate and, at times, trouble me. Especially those we are now starting to unravel with the atom.', 'msg_a': \"Albert Einstein: The universe is a wondrous place, wouldn't you agree?\"}, {'msg_b': \"Robert J. Oppenheimer: Precisely. The potential for both creation and destruction weighs heavily on my mind. It's a double-edged sword.\", 'msg_a': \"Albert Einstein: Ah yes, the atom. A source of immense power, but also grave responsibility, wouldn't you say?\"}, {'msg_b': 'Robert J. Oppenheimer: I concur. Yet, the forces of politics and conflict often seem to overshadow reason and progress. I fear our work might be used for ends we never intended.', 'msg_a': 'Albert Einstein: It is our duty, as scientists, to guide humanity toward the beneficial application of these discoveries, to prevent it from turning against itself.'}, {'msg_b': 'Robert J. Oppenheimer: I wish I had your certainty, Professor. The path ahead feels fraught with peril. Still, I am grateful for the pursuit of knowledge, even amidst the uncertainty.', 'msg_a': 'Albert Einstein: We must never lose hope that reason and understanding will prevail. The universe operates on elegant principles; if we remain dedicated to truth, we will find our way. '}, {'msg_b': 'Robert J. Oppenheimer: Perhaps, Professor. Perhaps.', 'msg_a': 'Albert Einstein: As am I, my friend. For it is through the exploration of these great mysteries that we glimpse the true nature of the universe and perhaps, our place within it.'}])\n"
     ]
    }
   ],
   "source": [
    "pr = \"Create a conversation between Albert Einstein and Robert J. Oppenheimer\"\n",
    "convo = c.structured(pr, tools=[Conversation], sp=sysp)[0]; print(convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eaed74",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8b25c",
   "metadata": {},
   "source": [
    "We'll create a Chat class that will handle formatting of messages and passing along system prompts and tools, so we don't have to worry about doing that manually each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Chat:\n",
    "    def __init__(self,\n",
    "                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n",
    "                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n",
    "                 sp=None, # Optional system prompt\n",
    "                 tools:Optional[list]=None,  # List of tools to make available\n",
    "                 tool_config:Optional[str]=None): # Forced tool choice\n",
    "        \"Gemini chat client.\"\n",
    "        assert model or cli\n",
    "        self.c = (cli or Client(model, sp=sp))\n",
    "        self.h,self.sp,self.tools,self.tool_config = [],sp,tools,tool_config\n",
    "\n",
    "    @property\n",
    "    def use(self): return self.c.use\n",
    "    @property\n",
    "    def cost(self): return self.c.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567fcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(In: 0; Out: 0; Total: 0, [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"Never mention what tools you use.\"\n",
    "chat = Chat(model, sp=sp)\n",
    "chat.use, chat.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _stream(self:Chat, res):\n",
    "    yield from res\n",
    "    self.h += mk_toolres(self.c.result, ns=self.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aade98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _post_pr(self:Chat, pr, prev_role):\n",
    "    if pr is None and prev_role == 'assistant':\n",
    "        raise ValueError(\"Prompt must be given after assistant completion, or use `self.cont_pr`.\")\n",
    "    if pr: self.h.append(mk_msg(pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _append_pr(self:Chat,\n",
    "               pr=None,  # Prompt / message\n",
    "              ):\n",
    "    prev_role = nested_idx(self.h, -1, 'role') if self.h else 'assistant' # First message should be 'user'\n",
    "    if pr and prev_role == 'user': self() # already user request pending\n",
    "    self._post_pr(pr, prev_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(genai.GenerativeModel.generate_content)\n",
    "def __call__(self:Chat,\n",
    "             pr=None,  # Prompt / message\n",
    "             temp=0, # Temperature\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream=False, # Stream response?\n",
    "             **kwargs):\n",
    "    if isinstance(pr,str): pr = pr.strip()\n",
    "    self._append_pr(pr)\n",
    "    if self.tools: kwargs['tools'] = self.tools\n",
    "    # NOTE: Gemini specifies tool_choice via tool_config\n",
    "    if self.tool_config: kwargs['tool_config'] = mk_tool_config(self.tool_config)\n",
    "    res = self.c(self.h, stream=stream, sp=self.sp, temp=temp, maxtok=maxtok, **kwargs)\n",
    "    if stream: return self._stream(res)\n",
    "    self.h += mk_toolres(self.c.result, ns=self.tools)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778295e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It's nice to meet you, Faisal.\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"It's nice to meet you, Faisal.\\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.014608133922923695\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 12\n",
       "- candidates_token_count: 11\n",
       "- total_token_count: 23\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"It's nice to meet you, Faisal.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.014608133922923695\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 12,\n",
       "        \"candidates_token_count\": 11,\n",
       "        \"total_token_count\": 23\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"I'm Faisal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8006a",
   "metadata": {},
   "source": [
    "Now let's make sure that context is passed properly to subsequent calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5374fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Faisal.\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Your name is Faisal.\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -2.8847726449991267e-05\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 31\n",
       "- candidates_token_count: 6\n",
       "- total_token_count: 37\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Your name is Faisal.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -2.8847726449991267e-05\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 31,\n",
       "        \"candidates_token_count\": 6,\n",
       "        \"total_token_count\": 37\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b6819",
   "metadata": {},
   "source": [
    "We can check our uage with the `use` property. As you can see it keeps track of the history of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260680a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 43; Out: 17; Total: 60"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81175c6",
   "metadata": {},
   "source": [
    "Let's make a nice markdown representation for our docs and jupyter notebooks of our chat object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def _repr_markdown_(self:Chat):\n",
    "    if not hasattr(self.c, 'result'): return 'No results yet'\n",
    "    last_msg = contents(self.c.result)\n",
    "    history = '\\n\\n'.join(f\"**{m['role']}**: {m['parts'][0] if isinstance(m['parts'][0],str) else m['parts'][0].text}\" \n",
    "                         for m in self.h if m['role'] in ('user','model'))\n",
    "    det = self.c._repr_markdown_().split('\\n\\n')[-1]\n",
    "    return f\"\"\"{last_msg}\n",
    "\n",
    "<details>\n",
    "<summary>History</summary>\n",
    "\n",
    "{history}\n",
    "</details>\n",
    "{det}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d3e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Faisal.\n",
       "\n",
       "\n",
       "<details>\n",
       "<summary>History</summary>\n",
       "\n",
       "**user**: I'm Faisal\n",
       "\n",
       "**model**: It's nice to meet you, Faisal.\n",
       "\n",
       "\n",
       "**user**: What's my name?\n",
       "\n",
       "**model**: Your name is Faisal.\n",
       "\n",
       "</details>\n",
       "\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 43 | 0.000000 |\n",
       "| Output tokens | 17 | 0.000000 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **60** | **$0.000000** |"
      ],
      "text/plain": [
       "<__main__.Chat>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37f240",
   "metadata": {},
   "source": [
    "Let's also make sure that streaming works correctly with the Chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26cbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you, Faisal.\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(model, sp=sp)\n",
    "for o in chat(\"I'm Faisal\", stream=True):\n",
    "    o = contents(o)\n",
    "    if o and isinstance(o, str): print(o, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35d8ea",
   "metadata": {},
   "source": [
    "Let's also make sure that tool use works with the Chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a192f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 604542+6458932?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = f\"What is {a}+{b}?\"; pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3b278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Faisal, how can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Hello Faisal, how can I help you today?\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.03243409503589977\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 72\n",
       "- candidates_token_count: 11\n",
       "- total_token_count: 83\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hello Faisal, how can I help you today?\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.03243409503589977\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 72,\n",
       "        \"candidates_token_count\": 11,\n",
       "        \"total_token_count\": 83\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\n",
    "chat = Chat(model, sp=sp, tools=[sums])\n",
    "r = chat(\"I'm Faisal\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18228faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542.0 and 6458932.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 604542.0, 'b': 6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -5.046702123460515e-06\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 103\n",
       "- candidates_token_count: 3\n",
       "- total_token_count: 106\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": 6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -5.046702123460515e-06\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 103,\n",
       "        \"candidates_token_count\": 3,\n",
       "        \"total_token_count\": 106\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408b9bd",
   "metadata": {},
   "source": [
    "The model correctly calls the right function in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf082b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'parts': [name: \"sums\"\n",
       "  response {\n",
       "    fields {\n",
       "      key: \"result\"\n",
       "      value {\n",
       "        number_value: 7063474\n",
       "      }\n",
       "    }\n",
       "  }]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.h[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbbf11",
   "metadata": {},
   "source": [
    "If we inspect the history, we can see that the result of the function call has already been added. We can simply call `chat()` to pass this to the model and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f61a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "7063474\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': '7063474\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.10447429120540619\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 153\n",
       "- candidates_token_count: 8\n",
       "- total_token_count: 161\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"7063474\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.10447429120540619\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 153,\n",
       "        \"candidates_token_count\": 8,\n",
       "        \"total_token_count\": 161\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d1b5a",
   "metadata": {},
   "source": [
    "Now let's make sure that `tool_config` works correctly by forcing the model to pick a particular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fe00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(\n",
    "    a:int, # The number to subtract from\n",
    "    b:int # The amount to subtract\n",
    ") -> int: # Result of subtracting b from a\n",
    "    \"Returns a - b.\"\n",
    "    print(f\"Finding the diff of {a} and {b}\")\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ef89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the diff of 604542.0 and -6458932.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"diff\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: -6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'diff', 'args': {'a': 604542.0, 'b': -6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.005156605504453182\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 132\n",
       "- candidates_token_count: 3\n",
       "- total_token_count: 135\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"diff\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": -6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.005156605504453182\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 132,\n",
       "        \"candidates_token_count\": 3,\n",
       "        \"total_token_count\": 135\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\n",
    "chat = Chat(model, sp=sp, tools=[sums, diff], tool_config=[diff])\n",
    "r = chat(f\"What is {a}+{b}?\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f1673",
   "metadata": {},
   "source": [
    "We can see that the model calls the function specified by `tool_config` even though the prompt asks for a summation, which is the expected behvior in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58508b0e",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d787d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "image/jpeg": {
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = Path('./samples/puppy.jpg')\n",
    "display.Image(filename=fn, width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc3a59",
   "metadata": {},
   "source": [
    "Now that we are passing more than just text, will need a helper function to upload media using Gemini's File API, which is the recomended way of passing media to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def media_msg(fn: Path)->dict:\n",
    "    if isinstance(fn, dict): return fn # Already processed\n",
    "    f = genai.upload_file(fn)\n",
    "    return {'file_data': {'mime_type': f.mime_type, 'file_uri': f.uri}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1316e",
   "metadata": {},
   "source": [
    "Let's also update how we pass in text type messages, to be consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def text_msg(s:str)->dict:\n",
    "    return {'text': s}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105009c",
   "metadata": {},
   "source": [
    "And finally lets add a helper function for make content correctly handles text and other media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed657bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _mk_content(src):\n",
    "    \"Create appropriate content data structure based on type of content\"\n",
    "    if isinstance(src,str): return text_msg(src)\n",
    "    if isinstance(src,FunctionResponse): return src\n",
    "    else: return media_msg(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5f034",
   "metadata": {},
   "source": [
    "Now let's make sure it properly handles text vs. Path objects for media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befbdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Hi'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mk_content(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ad3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_data': {'mime_type': 'image/jpeg',\n",
       "  'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/zpoa38voejtt'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mk_content(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8705f",
   "metadata": {},
   "source": [
    "And now we need to update mk_msg to be able to handle multimedia messages correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msg(content, role='user', **kw):\n",
    "    if isinstance(content, GenerateContentResponse): role,content = 'model',contents(content)\n",
    "    if isinstance(content, dict): role,content = content['role'],content['parts']\n",
    "    if not isinstance(content, list): content=[content]\n",
    "    if role == 'user': \n",
    "        if content: ## Gemini errors if the message contains only media and no text\n",
    "            if len(content) == 1 and not isinstance(content[0], str): content.append(' ')\n",
    "            content = [_mk_content(o) for o in content]\n",
    "        else: content = ''\n",
    "    return dict(role=role, parts=content, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msgs(msgs:list, **kw):\n",
    "    \"Helper to set 'assistant' role on alternate messages.\"\n",
    "    if isinstance(msgs,str): msgs=[msgs]\n",
    "    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb995eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/kmbr0020l5k6'}},\n",
       "   {'text': ' '}]},\n",
       " {'role': 'model',\n",
       "  'parts': ['In brief, what color flowers are in this image?']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "mk_msgs([fn, q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c742c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': [{'text': 'Hi'}]},\n",
       " {'role': 'model', 'parts': ['Nice, to meet you. How can I help?']},\n",
       " {'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/sfxeop0j911a'}},\n",
       "   {'text': 'In brief, what color flowers are in this image?'}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_msgs(['Hi', 'Nice, to meet you. How can I help?', [fn, q]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65879c1d",
   "metadata": {},
   "source": [
    "Now, we should just be able to pass a list of multimedia content to our Chat client and it should be able to handle it all under the hood. Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813f5fa-3e9a-4428-8368-ab23c02eed2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This picture features an adorable Cavalier King Charles Spaniel puppy. It has beautiful brown and white fur, and is resting on a grassy area next to a patch of purple flowers. The puppy is looking directly at the camera with a sweet expression.\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'This picture features an adorable Cavalier King Charles Spaniel puppy. It has beautiful brown and white fur, and is resting on a grassy area next to a patch of purple flowers. The puppy is looking directly at the camera with a sweet expression.\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.6654894303302376\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 1183\n",
       "- candidates_token_count: 49\n",
       "- total_token_count: 1232\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"This picture features an adorable Cavalier King Charles Spaniel puppy. It has beautiful brown and white fur, and is resting on a grassy area next to a patch of purple flowers. The puppy is looking directly at the camera with a sweet expression.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.6654894303302376\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 1183,\n",
       "        \"candidates_token_count\": 49,\n",
       "        \"total_token_count\": 1232\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061388df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers in the image are purple.\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'The flowers in the image are purple.\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.011526683138476478\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 913\n",
       "- candidates_token_count: 9\n",
       "- total_token_count: 922\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The flowers in the image are purple.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.011526683138476478\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 913,\n",
       "        \"candidates_token_count\": 9,\n",
       "        \"total_token_count\": 922\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([fn, q])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710a811",
   "metadata": {},
   "source": [
    "Hooray! That works, let's double check the history to make sure that everything is properly formatted and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e5016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/y1u2m1yaahq'}},\n",
       "   {'text': ' '}]},\n",
       " {'role': 'model',\n",
       "  'parts': [\"This is an adorable image! The puppy is a Cavalier King Charles Spaniel with a beautiful coat of white and chestnut brown. It's nestled amongst some lovely purple flowers and green grass. The puppy's big, dark eyes and sweet expression make it irresistibly cute.\"]},\n",
       " {'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/xpvi2ey0087o'}},\n",
       "   {'text': ' '}]},\n",
       " {'role': 'model',\n",
       "  'parts': ['The image shows a charming Cavalier King Charles Spaniel puppy, with its distinctive white and chestnut coat, positioned amidst some blooming purple flowers. The pup is resting on the grass, and its gaze is directed towards the viewer. Its soft fur and gentle expression evoke a sense of warmth and tenderness.']},\n",
       " {'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/q2k4qztinipo'}},\n",
       "   {'text': 'In brief, what color flowers are in this image?'}]},\n",
       " {'role': 'model', 'parts': ['The flowers in the image are purple.\\n']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_msgs(chat.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b810ff",
   "metadata": {},
   "source": [
    "While we are at it, let's update our markdown representation to handle the new messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:Chat):\n",
    "    if not hasattr(self.c, 'result'): return 'No results yet'\n",
    "    last_msg = contents(self.c.result)\n",
    "    \n",
    "    def fmt_part(ps):\n",
    "        if len(ps) == 1: return fmt_single(ps[0])\n",
    "        return '\\n' + '\\n'.join(f'- {fmt_single(p)}' for p in ps)\n",
    "        \n",
    "    def fmt_single(p):\n",
    "        if 'text' in p: return p['text']\n",
    "        if 'file_data' in p: return f\"uploaded media: {p['file_data']['mime_type']}\"\n",
    "        return str(p)\n",
    "        \n",
    "    history = '\\n\\n'.join(f\"**{m['role']}**: {fmt_part(m['parts'])}\" \n",
    "                         for m in self.h if m['role'] in ('user','model'))\n",
    "    det = self.c._repr_markdown_().split('\\n\\n')[-1]\n",
    "    return f\"\"\"{last_msg}\n",
    "\n",
    "<details>\n",
    "<summary>History</summary>\n",
    "\n",
    "{history}\n",
    "</details>\n",
    "{det}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67791706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers in the image are **purple**.\n",
       "\n",
       "<details>\n",
       "<summary>History</summary>\n",
       "\n",
       "**user**: \n",
       "- uploaded media: image/jpeg\n",
       "- In brief, what color flowers are in this image?\n",
       "\n",
       "**model**: The flowers in the image are **purple**.\n",
       "</details>\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 270 | 0.000000 |\n",
       "| Output tokens | 9 | 0.000000 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **279** | **$0.000000** |"
      ],
      "text/plain": [
       "<__main__.Chat>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133fca0",
   "metadata": {},
   "source": [
    "## Other Media (audio, video, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acd644",
   "metadata": {},
   "source": [
    "Unlike ChatGPT and Claude, Gemini models can also handle audio and video inputs. Since we're using Gemini's File API for handling multimedia content, what we have should just work, except we'll need to make one small modification to the `media_msg` function. Also, while we are at it, let us also allow for users to pass in the bytes of the content instead of the path to be consistent with our other LLM provider libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports \n",
    "def media_msg(\n",
    "    media, # Media to process (Path|bytes|dict)\n",
    "    mime=None # Optional mime type\n",
    ")->dict: # Dict for Gemini API\n",
    "    \"Handle media input as either Path or bytes, returning dict for Gemini API\"\n",
    "    if isinstance(media, dict): return media # Already processed\n",
    "    def _upload(f, mime=None):\n",
    "        f = genai.upload_file(f, mime_type=mime)\n",
    "        while f.state.name == \"PROCESSING\": time.sleep(2); f = genai.get_file(f.name)\n",
    "        return {'file_data': {'mime_type': f.mime_type, 'file_uri': f.uri}}\n",
    "    if isinstance(media, (str,Path)): return _upload(media)\n",
    "    if isinstance(media, bytes) and mime is None: mime = ft.guess(media).mime\n",
    "    return _upload(io.BytesIO(media if isinstance(media, bytes) else media.encode()), mime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40638f5e",
   "metadata": {},
   "source": [
    "Since we're uploading potentially larger files, we need to wait for the upload and process to complete so that the media is ready to be consumed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06916192",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model)\n",
    "img = fn.read_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! The flowers in the image are a light purple color.\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Certainly! The flowers in the image are a light purple color.'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.45572607333843523\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 270\n",
       "- candidates_token_count: 13\n",
       "- total_token_count: 283\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Certainly! The flowers in the image are a light purple color.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.45572607333843523\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 270,\n",
       "        \"candidates_token_count\": 13,\n",
       "        \"total_token_count\": 283\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([img, q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc822d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll test this with the example from Gemini's docs\n",
    "video_fn = Path('./samples/selective_attention_test.mp4')\n",
    "prompt = \"Answer the question in the video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The players wearing white pass the basketball 13 times.\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'The players wearing white pass the basketball 13 times.'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.4509180386861165\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 10527\n",
       "- candidates_token_count: 12\n",
       "- total_token_count: 10539\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The players wearing white pass the basketball 13 times.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.4509180386861165\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 10527,\n",
       "        \"candidates_token_count\": 12,\n",
       "        \"total_token_count\": 10539\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "chat([video_fn, prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e841b2",
   "metadata": {},
   "source": [
    "Takes a little while, but works like a charm! Now, let's try an audio file to make sure it also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ae3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fn = Path('./samples/attention_is_all_you_need.mp3')\n",
    "audio = audio_fn.read_bytes()\n",
    "prompt = \"What is the audio about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03856f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The audio is a podcast discussion about a groundbreaking research paper titled \"Attention is All You Need\" by Vaswani et al. The podcast features a machine learning expert who explains the core ideas, motivation, and architecture of the Transformer model introduced in the paper. They discuss the significance of attention mechanisms, how the Transformer differs from previous approaches like RNNs, its remarkable performance on machine translation and other sequence transduction tasks, and the broader implications of the research for machine learning and NLP. They also touch upon the limitations and future directions.\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'The audio is a podcast discussion about a groundbreaking research paper titled \"Attention is All You Need\" by Vaswani et al. The podcast features a machine learning expert who explains the core ideas, motivation, and architecture of the Transformer model introduced in the paper. They discuss the significance of attention mechanisms, how the Transformer differs from previous approaches like RNNs, its remarkable performance on machine translation and other sequence transduction tasks, and the broader implications of the research for machine learning and NLP. They also touch upon the limitations and future directions.'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.48403912498837426\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 18387\n",
       "- candidates_token_count: 105\n",
       "- total_token_count: 18492\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The audio is a podcast discussion about a groundbreaking research paper titled \\\"Attention is All You Need\\\" by Vaswani et al. The podcast features a machine learning expert who explains the core ideas, motivation, and architecture of the Transformer model introduced in the paper. They discuss the significance of attention mechanisms, how the Transformer differs from previous approaches like RNNs, its remarkable performance on machine translation and other sequence transduction tasks, and the broader implications of the research for machine learning and NLP. They also touch upon the limitations and future directions.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.48403912498837426\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 18387,\n",
       "        \"candidates_token_count\": 105,\n",
       "        \"total_token_count\": 18492\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([audio, prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d9a18",
   "metadata": {},
   "source": [
    "Finally, let's check to make sure pdfs work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a8110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's a breakdown of what's in the PDF that wasn't covered in the podcast:\n",
       "\n",
       "**Technical Details of the Transformer:**\n",
       "\n",
       "*   **Detailed Architecture:** The PDF provides a much more detailed breakdown of the Transformer architecture, including the specific arrangement of encoder and decoder layers, sub-layers, residual connections, and layer normalization (see Figure 1 and Section 3.1). The podcast gave a high-level overview, whereas the PDF is more specific about the components used to build the model.\n",
       "\n",
       "*   **Scaled Dot-Product Attention:** The document explains the mechanics of \"Scaled Dot-Product Attention\" (section 3.2.1, Figure 2), and its advantages over additive attention and specifically mentions that dot products are scaled by  `1/sqrt(dk)`. This isn't mentioned in the podcast.\n",
       "\n",
       "*   **Multi-Head Attention:** The PDF delves into the purpose of \"Multi-Head Attention\", stating that it enables the model to attend to different representation subspaces (section 3.2.2, Figure 2). It also explicitly states that the projections are parameter matrices such as `WQ âˆˆ R^(d_model x d_k), WK âˆˆ R^(d_model x d_k)`, etc.. and gives the values for the number of attention layers used. The podcast explained what multi-head attention was, but didn't delve into the math or specific values used.\n",
       "*   **Positional Encodings:** The PDF describes the specific sine and cosine functions used for positional encoding (section 3.5) and why they chose this method. This level of detail about this topic isn't mentioned in the podcast.\n",
       "\n",
       "*   **Point-wise Feed-Forward Networks:** The podcast doesn't go into the details of this topic, but the PDF notes that feed-forward networks in each layer are position-wise, fully connected, and consist of two linear transformations with a ReLU activation in between. It also specifies the dimensionality of input and output as well as the inner layer (section 3.3).\n",
       "\n",
       "*   **Embeddings and Softmax:** The PDF states how learned embeddings are used to convert input/output tokens to vectors (section 3.4), and that the same weight matrix is shared between embeddings and the pre-softmax linear transformation. The embeddings are multiplied by the square root of d_model. These details are not mentioned in the podcast.\n",
       "\n",
       "**Training and Evaluation:**\n",
       "\n",
       "*   **Training Details:** The PDF explains the datasets (WMT 2014 English-German and English-French), the use of byte-pair encoding and word-piece vocabularies, and batching based on approximate sequence lengths (section 5.1). It also specifies how the model is trained using a specific number of training steps, using GPUs, the Adam optimizer and a specific formula for varying the learning rate (sections 5.2, 5.3). The podcast briefly mentions training time, but doesn't give specific dataset, architecture, or learning details.\n",
       "*   **Regularization:** The PDF explicitly mentions the use of residual dropout and label smoothing during training and the parameters used (section 5.4). The podcast didn't discuss this at all.\n",
       "*   **Performance Metrics:** The PDF presents specific BLEU scores for various models on the English-German and English-French translation tasks and lists the training cost in FLOPS (section 6.1, Table 2). It also mentions beam search details like beam size and a length penalty. The podcast only mentions the end results of BLEU score.\n",
       "*   **Model Variations:** The PDF explores various variations of the base model (section 6.2, Table 3), testing different parameters (number of heads, key/value dimensions, dropout rates, etc.), and mentions which variations performed well or poorly. The podcast doesn't mention any of these experiments.\n",
       "*   **English Constituency Parsing Results:** The PDF mentions that the transformer performs well on english constituency parsing with results from the Penn Treebank (section 6.3, Table 4) and compares its results against other models. This is only briefly touched on in the podcast, and none of the results from this section is covered.\n",
       "\n",
       "**Other Points**\n",
       "\n",
       "*   **Computational Complexity Analysis:** The PDF includes a table (Table 1) that compares the computational complexity, sequential operations and maximum path lengths of self-attention, recurrent and convolutional layers. This is not covered in the podcast.\n",
       "*   **Attention Visualizations:** The PDF includes visualization examples of attention heads for certain words (Figure 3, 4, and 5), showcasing the behavior and long range dependencies. These visualizations are not shown nor covered in the podcast.\n",
       "*   **References and Acknowledgements:** The document has a complete list of references and acknowledgements. This was not part of the podcast.\n",
       "\n",
       "**In summary, while the podcast provided a good overview, the PDF offers a deeper technical dive into the Transformer model, along with details about its training, experiments, and evaluations that aren't touched upon in the audio discussion.** The PDF provides specifics that an audience interested in implementing the model or performing experiments would be interested in.\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Okay, here\\'s a breakdown of what\\'s in the PDF that wasn\\'t covered in the podcast:\\n\\n**Technical Details of the Transformer:**\\n\\n*   **Detailed Architecture:** The PDF provides a much more detailed breakdown of the Transformer architecture, including the specific arrangement of encoder and decoder layers, sub-layers, residual connections, and layer normalization (see Figure 1 and Section 3.1). The podcast gave a high-level overview, whereas the PDF is more specific about the components used to build the model.\\n\\n*   **Scaled Dot-Product Attention:** The document explains the mechanics of \"Scaled Dot-Product Attention\" (section 3.2.1, Figure 2), and its advantages over additive attention and specifically mentions that dot products are scaled by  `1/sqrt(dk)`. This isn\\'t mentioned in the podcast.\\n\\n*   **Multi-Head Attention:** The PDF delves into the purpose of \"Multi-Head Attention\", stating that it enables the model to attend to different representation subspaces (section 3.2.2, Figure 2). It also explicitly states that the projections are parameter matrices such as `WQ âˆˆ R^(d_model x d_k), WK âˆˆ R^(d_model x d_k)`, etc.. and gives the values for the number of attention layers used. The podcast explained what multi-head attention was, but didn\\'t delve into the math or specific values used.\\n*   **Positional Encodings:** The PDF describes the specific sine and cosine functions used for positional encoding (section 3.5) and why they chose this method. This level of detail about this topic isn\\'t mentioned in the podcast.\\n\\n*   **Point-wise Feed-Forward Networks:** The podcast doesn\\'t go into the details of this topic, but the PDF notes that feed-forward networks in each layer are position-wise, fully connected, and consist of two linear transformations with a ReLU activation in between. It also specifies the dimensionality of input and output as well as the inner layer (section 3.3).\\n\\n*   **Embeddings and Softmax:** The PDF states how learned embeddings are used to convert input/output tokens to vectors (section 3.4), and that the same weight matrix is shared between embeddings and the pre-softmax linear transformation. The embeddings are multiplied by the square root of d_model. These details are not mentioned in the podcast.\\n\\n**Training and Evaluation:**\\n\\n*   **Training Details:** The PDF explains the datasets (WMT 2014 English-German and English-French), the use of byte-pair encoding and word-piece vocabularies, and batching based on approximate sequence lengths (section 5.1). It also specifies how the model is trained using a specific number of training steps, using GPUs, the Adam optimizer and a specific formula for varying the learning rate (sections 5.2, 5.3). The podcast briefly mentions training time, but doesn\\'t give specific dataset, architecture, or learning details.\\n*   **Regularization:** The PDF explicitly mentions the use of residual dropout and label smoothing during training and the parameters used (section 5.4). The podcast didn\\'t discuss this at all.\\n*   **Performance Metrics:** The PDF presents specific BLEU scores for various models on the English-German and English-French translation tasks and lists the training cost in FLOPS (section 6.1, Table 2). It also mentions beam search details like beam size and a length penalty. The podcast only mentions the end results of BLEU score.\\n*   **Model Variations:** The PDF explores various variations of the base model (section 6.2, Table 3), testing different parameters (number of heads, key/value dimensions, dropout rates, etc.), and mentions which variations performed well or poorly. The podcast doesn\\'t mention any of these experiments.\\n*   **English Constituency Parsing Results:** The PDF mentions that the transformer performs well on english constituency parsing with results from the Penn Treebank (section 6.3, Table 4) and compares its results against other models. This is only briefly touched on in the podcast, and none of the results from this section is covered.\\n\\n**Other Points**\\n\\n*   **Computational Complexity Analysis:** The PDF includes a table (Table 1) that compares the computational complexity, sequential operations and maximum path lengths of self-attention, recurrent and convolutional layers. This is not covered in the podcast.\\n*   **Attention Visualizations:** The PDF includes visualization examples of attention heads for certain words (Figure 3, 4, and 5), showcasing the behavior and long range dependencies. These visualizations are not shown nor covered in the podcast.\\n*   **References and Acknowledgements:** The document has a complete list of references and acknowledgements. This was not part of the podcast.\\n\\n**In summary, while the podcast provided a good overview, the PDF offers a deeper technical dive into the Transformer model, along with details about its training, experiments, and evaluations that aren\\'t touched upon in the audio discussion.** The PDF provides specifics that an audience interested in implementing the model or performing experiments would be interested in.\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.8560131542336379\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 33441\n",
       "- candidates_token_count: 1081\n",
       "- total_token_count: 34522\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Okay, here's a breakdown of what's in the PDF that wasn't covered in the podcast:\\n\\n**Technical Details of the Transformer:**\\n\\n*   **Detailed Architecture:** The PDF provides a much more detailed breakdown of the Transformer architecture, including the specific arrangement of encoder and decoder layers, sub-layers, residual connections, and layer normalization (see Figure 1 and Section 3.1). The podcast gave a high-level overview, whereas the PDF is more specific about the components used to build the model.\\n\\n*   **Scaled Dot-Product Attention:** The document explains the mechanics of \\\"Scaled Dot-Product Attention\\\" (section 3.2.1, Figure 2), and its advantages over additive attention and specifically mentions that dot products are scaled by  `1/sqrt(dk)`. This isn't mentioned in the podcast.\\n\\n*   **Multi-Head Attention:** The PDF delves into the purpose of \\\"Multi-Head Attention\\\", stating that it enables the model to attend to different representation subspaces (section 3.2.2, Figure 2). It also explicitly states that the projections are parameter matrices such as `WQ \\u2208 R^(d_model x d_k), WK \\u2208 R^(d_model x d_k)`, etc.. and gives the values for the number of attention layers used. The podcast explained what multi-head attention was, but didn't delve into the math or specific values used.\\n*   **Positional Encodings:** The PDF describes the specific sine and cosine functions used for positional encoding (section 3.5) and why they chose this method. This level of detail about this topic isn't mentioned in the podcast.\\n\\n*   **Point-wise Feed-Forward Networks:** The podcast doesn't go into the details of this topic, but the PDF notes that feed-forward networks in each layer are position-wise, fully connected, and consist of two linear transformations with a ReLU activation in between. It also specifies the dimensionality of input and output as well as the inner layer (section 3.3).\\n\\n*   **Embeddings and Softmax:** The PDF states how learned embeddings are used to convert input/output tokens to vectors (section 3.4), and that the same weight matrix is shared between embeddings and the pre-softmax linear transformation. The embeddings are multiplied by the square root of d_model. These details are not mentioned in the podcast.\\n\\n**Training and Evaluation:**\\n\\n*   **Training Details:** The PDF explains the datasets (WMT 2014 English-German and English-French), the use of byte-pair encoding and word-piece vocabularies, and batching based on approximate sequence lengths (section 5.1). It also specifies how the model is trained using a specific number of training steps, using GPUs, the Adam optimizer and a specific formula for varying the learning rate (sections 5.2, 5.3). The podcast briefly mentions training time, but doesn't give specific dataset, architecture, or learning details.\\n*   **Regularization:** The PDF explicitly mentions the use of residual dropout and label smoothing during training and the parameters used (section 5.4). The podcast didn't discuss this at all.\\n*   **Performance Metrics:** The PDF presents specific BLEU scores for various models on the English-German and English-French translation tasks and lists the training cost in FLOPS (section 6.1, Table 2). It also mentions beam search details like beam size and a length penalty. The podcast only mentions the end results of BLEU score.\\n*   **Model Variations:** The PDF explores various variations of the base model (section 6.2, Table 3), testing different parameters (number of heads, key/value dimensions, dropout rates, etc.), and mentions which variations performed well or poorly. The podcast doesn't mention any of these experiments.\\n*   **English Constituency Parsing Results:** The PDF mentions that the transformer performs well on english constituency parsing with results from the Penn Treebank (section 6.3, Table 4) and compares its results against other models. This is only briefly touched on in the podcast, and none of the results from this section is covered.\\n\\n**Other Points**\\n\\n*   **Computational Complexity Analysis:** The PDF includes a table (Table 1) that compares the computational complexity, sequential operations and maximum path lengths of self-attention, recurrent and convolutional layers. This is not covered in the podcast.\\n*   **Attention Visualizations:** The PDF includes visualization examples of attention heads for certain words (Figure 3, 4, and 5), showcasing the behavior and long range dependencies. These visualizations are not shown nor covered in the podcast.\\n*   **References and Acknowledgements:** The document has a complete list of references and acknowledgements. This was not part of the podcast.\\n\\n**In summary, while the podcast provided a good overview, the PDF offers a deeper technical dive into the Transformer model, along with details about its training, experiments, and evaluations that aren't touched upon in the audio discussion.** The PDF provides specifics that an audience interested in implementing the model or performing experiments would be interested in.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.8560131542336379\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 33441,\n",
       "        \"candidates_token_count\": 1081,\n",
       "        \"total_token_count\": 34522\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_fn = Path('./samples/attention_is_all_you_need.pdf')\n",
    "prompt = \"What's mentioned in this pdf that's not mentioned in the previous podcast?\"\n",
    "chat([pdf_fn, prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314b9b7",
   "metadata": {},
   "source": [
    "Gemini does a pretty good job here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ad692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's the transcript of the first minute of the podcast, based on your request:\n",
       "\n",
       "\"Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing 'Attention is All You Need' by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they're so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.\"\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Okay, here\\'s the transcript of the first minute of the podcast, based on your request:\\n\\n\"Welcome to our podcast, where we dive into groundbreaking research papers. Today, we\\'re discussing \\'Attention is All You Need\\' by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I\\'m excited to discuss this revolutionary paper. Let\\'s start with the core idea. What\\'s the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they\\'re so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.\"\\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- avg_logprobs: -0.07141794775524278\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- prompt_token_count: 34540\n",
       "- candidates_token_count: 274\n",
       "- total_token_count: 34814\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Okay, here's the transcript of the first minute of the podcast, based on your request:\\n\\n\\\"Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing 'Attention is All You Need' by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they're so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.\\\"\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"avg_logprobs\": -0.07141794775524278\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 34540,\n",
       "        \"candidates_token_count\": 274,\n",
       "        \"total_token_count\": 34814\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = \"Can you generate an exact transcript of the first minute or so of the podcast.\"\n",
    "chat(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's the transcript of the first minute of the podcast, based on your request:\n",
       "\n",
       "\"Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing 'Attention is All You Need' by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they're so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.\"\n",
       "\n",
       "\n",
       "<details>\n",
       "<summary>History</summary>\n",
       "\n",
       "**user**: \n",
       "- uploaded media: video/mp4\n",
       "- Answer the question in the video\n",
       "\n",
       "**model**: The players wearing white pass the basketball 13 times.\n",
       "\n",
       "**user**: \n",
       "- uploaded media: audio/mpeg\n",
       "- What is the audio about?\n",
       "\n",
       "**model**: The audio is a podcast discussion about a groundbreaking research paper titled \"Attention is All You Need\" by Vaswani et al. The podcast features a machine learning expert who explains the core ideas, motivation, and architecture of the Transformer model introduced in the paper. They discuss the significance of attention mechanisms, how the Transformer differs from previous approaches like RNNs, its remarkable performance on machine translation and other sequence transduction tasks, and the broader implications of the research for machine learning and NLP. They also touch upon the limitations and future directions.\n",
       "\n",
       "**user**: \n",
       "- uploaded media: application/pdf\n",
       "- What's mentioned in this pdf that's not mentioned in the previous podcast?\n",
       "\n",
       "**model**: Okay, here's a breakdown of what's in the PDF that wasn't covered in the podcast:\n",
       "\n",
       "**Technical Details of the Transformer:**\n",
       "\n",
       "*   **Detailed Architecture:** The PDF provides a much more detailed breakdown of the Transformer architecture, including the specific arrangement of encoder and decoder layers, sub-layers, residual connections, and layer normalization (see Figure 1 and Section 3.1). The podcast gave a high-level overview, whereas the PDF is more specific about the components used to build the model.\n",
       "\n",
       "*   **Scaled Dot-Product Attention:** The document explains the mechanics of \"Scaled Dot-Product Attention\" (section 3.2.1, Figure 2), and its advantages over additive attention and specifically mentions that dot products are scaled by  `1/sqrt(dk)`. This isn't mentioned in the podcast.\n",
       "\n",
       "*   **Multi-Head Attention:** The PDF delves into the purpose of \"Multi-Head Attention\", stating that it enables the model to attend to different representation subspaces (section 3.2.2, Figure 2). It also explicitly states that the projections are parameter matrices such as `WQ âˆˆ R^(d_model x d_k), WK âˆˆ R^(d_model x d_k)`, etc.. and gives the values for the number of attention layers used. The podcast explained what multi-head attention was, but didn't delve into the math or specific values used.\n",
       "*   **Positional Encodings:** The PDF describes the specific sine and cosine functions used for positional encoding (section 3.5) and why they chose this method. This level of detail about this topic isn't mentioned in the podcast.\n",
       "\n",
       "*   **Point-wise Feed-Forward Networks:** The podcast doesn't go into the details of this topic, but the PDF notes that feed-forward networks in each layer are position-wise, fully connected, and consist of two linear transformations with a ReLU activation in between. It also specifies the dimensionality of input and output as well as the inner layer (section 3.3).\n",
       "\n",
       "*   **Embeddings and Softmax:** The PDF states how learned embeddings are used to convert input/output tokens to vectors (section 3.4), and that the same weight matrix is shared between embeddings and the pre-softmax linear transformation. The embeddings are multiplied by the square root of d_model. These details are not mentioned in the podcast.\n",
       "\n",
       "**Training and Evaluation:**\n",
       "\n",
       "*   **Training Details:** The PDF explains the datasets (WMT 2014 English-German and English-French), the use of byte-pair encoding and word-piece vocabularies, and batching based on approximate sequence lengths (section 5.1). It also specifies how the model is trained using a specific number of training steps, using GPUs, the Adam optimizer and a specific formula for varying the learning rate (sections 5.2, 5.3). The podcast briefly mentions training time, but doesn't give specific dataset, architecture, or learning details.\n",
       "*   **Regularization:** The PDF explicitly mentions the use of residual dropout and label smoothing during training and the parameters used (section 5.4). The podcast didn't discuss this at all.\n",
       "*   **Performance Metrics:** The PDF presents specific BLEU scores for various models on the English-German and English-French translation tasks and lists the training cost in FLOPS (section 6.1, Table 2). It also mentions beam search details like beam size and a length penalty. The podcast only mentions the end results of BLEU score.\n",
       "*   **Model Variations:** The PDF explores various variations of the base model (section 6.2, Table 3), testing different parameters (number of heads, key/value dimensions, dropout rates, etc.), and mentions which variations performed well or poorly. The podcast doesn't mention any of these experiments.\n",
       "*   **English Constituency Parsing Results:** The PDF mentions that the transformer performs well on english constituency parsing with results from the Penn Treebank (section 6.3, Table 4) and compares its results against other models. This is only briefly touched on in the podcast, and none of the results from this section is covered.\n",
       "\n",
       "**Other Points**\n",
       "\n",
       "*   **Computational Complexity Analysis:** The PDF includes a table (Table 1) that compares the computational complexity, sequential operations and maximum path lengths of self-attention, recurrent and convolutional layers. This is not covered in the podcast.\n",
       "*   **Attention Visualizations:** The PDF includes visualization examples of attention heads for certain words (Figure 3, 4, and 5), showcasing the behavior and long range dependencies. These visualizations are not shown nor covered in the podcast.\n",
       "*   **References and Acknowledgements:** The document has a complete list of references and acknowledgements. This was not part of the podcast.\n",
       "\n",
       "**In summary, while the podcast provided a good overview, the PDF offers a deeper technical dive into the Transformer model, along with details about its training, experiments, and evaluations that aren't touched upon in the audio discussion.** The PDF provides specifics that an audience interested in implementing the model or performing experiments would be interested in.\n",
       "\n",
       "\n",
       "**user**: Can you generate an exact transcript of the first minute or so of the podcast.\n",
       "\n",
       "**model**: Okay, here's the transcript of the first minute of the podcast, based on your request:\n",
       "\n",
       "\"Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing 'Attention is All You Need' by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they're so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.\"\n",
       "\n",
       "</details>\n",
       "\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 96,895 | 0.000000 |\n",
       "| Output tokens | 1,472 | 0.000000 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **98,367** | **$0.000000** |"
      ],
      "text/plain": [
       "<__main__.Chat>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea161a4",
   "metadata": {},
   "source": [
    "All of these also work with `Client` and can be combined with `structured` to get structured responses using multi-media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMetadata(BasicRepr):\n",
    "    \"\"\"Class to hold metadata for audio files\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_speakers:int, # Number of speakers\n",
    "        topic:str, # Topic discussed\n",
    "        summary:str, # 100 word summary\n",
    "        transcript:list[str], # Transcript of the audio segmented by speaker\n",
    "    ): store_attr()\n",
    "pr = \"Extract the necessary information from the audio.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(model)\n",
    "audio_md = c.structured(mk_msgs([[audio_fn, pr]]), tools=[AudioMetadata])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcb6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speakers: 2.0\n",
      "Topic: Machine Learning, Natural Language Processing\n",
      "Summary: This podcast discusses the Attention is All You Need research paper, focusing on the Transformer model's architecture, attention mechanisms, performance, and broader implications.\n",
      "Transcript: Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing attention is all you need by Vaswani at all. Joining us is an expert in machine learning. Welcome.\n",
      "-Thanks for having me. I'm excited to discuss this revolutionary paper.\n",
      "-Let's start with the core idea. What's the main thrust of this research?\n",
      "-The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models.\n",
      "-That sounds like a significant departure from previous approaches. What motivated this radical change?\n",
      "-The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences.\n",
      "-Could you explain what attention mechanisms are and why they're so crucial in this model?\n",
      "-Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot-product attention and extend it to multi-head attention, which lets the model jointly attend to information from different representation sub-spaces.\n",
      "-Fascinating. How does the Transformer's architecture differ from previous models?\n",
      "-The Transformer uses a stack of identical layers for both the encoder and decoder. Each layer has two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. This structure allows for more parallelization and efficient computation.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of speakers: {audio_md.n_speakers}')\n",
    "print(f'Topic: {audio_md.topic}')\n",
    "print(f'Summary: {audio_md.summary}')\n",
    "transcript = '\\n-'.join(list(audio_md.transcript)[:10])\n",
    "print(f'Transcript: {transcript}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd1cd6",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850535a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
