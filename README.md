# Gaspard


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Install

``` sh
pip install gaspard
```

## Getting started

Follow the [instructions](https://aistudio.google.com/app/apikey) to
generate an API key, and set it as an evironment variable as shown
below:

``` sh
export GEMINI_API_KEY=YOUR_API_KEY
```

Geminiâ€™s Python SDK will automatically be installed with Gaspard, if you
donâ€™t already have it.

``` python
from gaspard import *
```

Gaspard provides models, which lists the models available in the SDK

``` python
models
```

    ('gemini-1.5-pro-exp-0827',
     'gemini-1.5-flash-exp-0827',
     'gemini-1.5-pro',
     'gemini-1.5-flash')

For our examples weâ€™ll use `gemini-1.5-flash` since itâ€™s relatively
faster and cheaper.

``` python
model = models[-1]
```

## Chat

The main interface to Gaspard is the
[`Chat`](https://AnswerDotAI.github.io/gaspard/core.html#chat) class
which provides a stateful interface to the models

``` python
chat = Chat(model, sp="""You are a helpful and concise assistant.""")
chat("I'm Faisal")
```

Nice to meet you, Faisal! ğŸ˜Š How can I help you today?

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜Nice to meet you, Faisal! ğŸ˜Š How can I
  help you today? â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 13
- candidates_token_count: 16
- total_token_count: 29
- cached_content_token_count: 0

</details>

``` python
r = chat("What's my name?")
r
```

Your name is Faisal. ğŸ˜Š

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜Your name is Faisal. ğŸ˜Š â€™}\], â€˜roleâ€™:
  â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 39
- candidates_token_count: 6
- total_token_count: 45
- cached_content_token_count: 0

</details>

As you see above, displaying the results of a call in a notebook shows
just the message contents, with the other details hidden behind a
collapsible section. Alternatively you can print the details:

``` python
print(r)
```

    response:
    GenerateContentResponse(
        done=True,
        iterator=None,
        result=protos.GenerateContentResponse({
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "Your name is Faisal. \ud83d\ude0a \n"
                  }
                ],
                "role": "model"
              },
              "finish_reason": "STOP",
              "index": 0,
              "safety_ratings": [
                {
                  "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                  "probability": "NEGLIGIBLE"
                },
                {
                  "category": "HARM_CATEGORY_HATE_SPEECH",
                  "probability": "NEGLIGIBLE"
                },
                {
                  "category": "HARM_CATEGORY_HARASSMENT",
                  "probability": "NEGLIGIBLE"
                },
                {
                  "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                  "probability": "NEGLIGIBLE"
                }
              ]
            }
          ],
          "usage_metadata": {
            "prompt_token_count": 39,
            "candidates_token_count": 6,
            "total_token_count": 45
          }
        }),
    )

You can use stream=True to stream the results as soon as they arrive
(although you will only see the gradual generation if you execute the
notebook yourself, of course!)

``` python
for o in chat("What's your name?", stream=True): print(o, end='')
```

    I don't have a name! I'm a large language model, so I'm not a person.  But you can call me Bard if you'd like. ğŸ˜Š 

## Tool use

Tool use lets the model use external tools.

We use docments to make defining Python functions as ergonomic as
possible. Each parameter (and the return value) should have a type, and
a docments comment with the description of what it is. As an example
weâ€™ll write a simple function that adds numbers together, and will tell
us when itâ€™s being called:

``` python
def sums(
    a:int,  # First thing to sum
    b:int=1 # Second thing to sum
) -> int: # The sum of the inputs
    "Adds a + b."
    print(f"Finding the sum of {a} and {b}")
    return a + b
```

Sometimes the model will say something like â€œaccording to the sums tool
the answer isâ€ â€“ generally weâ€™d rather it just tells the user the
answer, so we can use a system prompt to help with this:

``` python
sp = "Never mention what tools you use."
```

Weâ€™ll get the model to add up some long numbers:

``` python
a,b = 604542,6458932
pr = f"What is {a}+{b}?"
pr
```

    'What is 604542+6458932?'

To use tools, pass a list of them to Chat:

``` python
chat = Chat(model, sp=sp, tools=[sums])
```

Now when we call that with our prompt, the model doesnâ€™t return the
answer, but instead returns a `function_call` message, which means we
have to call the named function (tool) with the provided parameters:

``` python
r = chat(pr)
r
```

    Finding the sum of 604542.0 and 6458932.0

- content: {â€˜partsâ€™: \[{â€˜function_callâ€™: {â€˜nameâ€™: â€˜sumsâ€™, â€˜argsâ€™: {â€˜aâ€™:
  604542.0, â€˜bâ€™: 6458932.0}}}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 7, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 10, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 9,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1,
  â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 76
- candidates_token_count: 29
- total_token_count: 105
- cached_content_token_count: 0

Gaspard handles all that for us â€“ we just have to pass along the
message, and it all happens automatically:

``` python
chat()
```

7063474.0

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜7063474.0 â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 128
- candidates_token_count: 9
- total_token_count: 137
- cached_content_token_count: 0

</details>

We can inspect the history to see what happens under the hood. Gaspard
calls the tool with the appropriate variables returned by the
`function_call` message from the model. The result of calling the
function is then sent back to the model, which uses that to respond to
the user.

``` python
chat.h[-3:]
```

    [parts {
       function_call {
         name: "sums"
         args {
           fields {
             key: "b"
             value {
               number_value: 6458932
             }
           }
           fields {
             key: "a"
             value {
               number_value: 604542
             }
           }
         }
       }
     }
     role: "model",
     {'role': 'function',
      'parts': [name: "sums"
       response {
         fields {
           key: "result"
           value {
             string_value: "7063474.0"
           }
         }
       }]},
     parts {
       text: "7063474.0 \n"
     }
     role: "model"]

You can see how many tokens have been used at any time by checking the
`use` property.

``` python
chat.use
```

    In: 204; Out: 38; Total: 242

## Tool loop

We can do everything needed to use tools in a single step, by using
Chat.toolloop. This can even call multiple tools as needed solve a
problem. For example, letâ€™s define a tool to handle multiplication:

``` python
def mults(
    a:int,  # First thing to multiply
    b:int=1 # Second thing to multiply
) -> int: # The product of the inputs
    "Multiplies a * b."
    print(f"Finding the product of {a} and {b}")
    return a * b
```

Now with a single call we can calculate `(a+b)*2` â€“ by passing
`show_trace` we can see each response from the model in the process:

``` python
chat = Chat(model, sp=sp, tools=[sums,mults])
pr = f'Calculate ({a}+{b})*2'
pr
```

    'Calculate (604542+6458932)*2'

``` python
def pchoice(r): print(r.parts[0])
```

``` python
r = chat.toolloop(pr, trace_func=pchoice)
```

    Finding the sum of 604542.0 and 6458932.0
    function_call {
      name: "sums"
      args {
        fields {
          key: "b"
          value {
            number_value: 6458932
          }
        }
        fields {
          key: "a"
          value {
            number_value: 604542
          }
        }
      }
    }

    Finding the product of 7063474.0 and 2.0
    function_call {
      name: "mults"
      args {
        fields {
          key: "b"
          value {
            number_value: 2
          }
        }
        fields {
          key: "a"
          value {
            number_value: 7063474
          }
        }
      }
    }

    text: "The answer is 14126948.0. \n"

We can see from the trace above that the model correctly calls the sums
function first to add the numbers inside the parenthesis and then calls
the mults function to multiply the result of the summation by `2`. The
response sent back to the user is the actual result after performing the
chained tool calls, shown below:

``` python
r
```

The answer is 14126948.0.

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜The answer is 14126948.0. â€™}\], â€˜roleâ€™:
  â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 230
- candidates_token_count: 15
- total_token_count: 245
- cached_content_token_count: 0

</details>

## Images

As everyone knows, when testing image APIs you have to use a cute puppy.

``` python
img_fn = Path('samples/puppy.jpg')
display.Image(filename=img_fn, width=200)
```

<img src="index_files/figure-commonmark/cell-22-output-1.jpeg"
width="200" />

We create a
[`Chat`](https://AnswerDotAI.github.io/gaspard/core.html#chat) object as
before:

``` python
chat = Chat(model)
```

For Gaspard, we can simply pass `Path` objects that repsent the path of
the images. To pass multi-part messages, such as an image along with a
prompt, we simply pass in a list of items. Note that Gaspard expects
each item to be a text or a `Path` object.

``` python
chat([img_fn, "In brief, what color flowers are in this image?"])
```

    Uploading media...

The flowers in the image are purple.

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜The flowers in the image are purple.
  â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 270
- candidates_token_count: 8
- total_token_count: 278
- cached_content_token_count: 0

</details>

Under the hood, Gaspard uploads the image using Geminiâ€™s `File API` and
passes a reference to the model. Gemini API will automatically infer the
MIME type, and convert it appropriately. NOTE that the image is also
included in input tokens.

``` python
chat.use
```

    In: 270; Out: 8; Total: 278

Alternatively, Gaspard supports creating a multi-stage chat with
separate image and text prompts. For instance, you can pass just the
image as the initial prompt (in which case the model will make some
general comments about what it sees), and then follow up with questions
in additional prompts:

``` python
chat = Chat(model)
chat(img_fn)
```

    Uploading media...

This is a cute puppy! It looks like a Cavalier King Charles Spaniel.
They are known for being friendly, playful, and affectionate dogs. ğŸ¶ğŸ’•

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜This is a cute puppy! It looks like a
  Cavalier King Charles Spaniel. They are known for being friendly,
  playful, and affectionate dogs. ğŸ¶ğŸ’• â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 259
- candidates_token_count: 32
- total_token_count: 291
- cached_content_token_count: 0

</details>

``` python
chat('What direction is the puppy facing?')
```

The puppy is facing towards the right side of the image.

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜The puppy is facing towards the right
  side of the image. â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 302
- candidates_token_count: 12
- total_token_count: 314
- cached_content_token_count: 0

</details>

``` python
chat('What color is it?')
```

The puppy is white with brown markings.

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜The puppy is white with brown markings.
  â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 323
- candidates_token_count: 8
- total_token_count: 331
- cached_content_token_count: 0

</details>

Note that the image is passed in again for every input in the dialog,
via the chat history, so the number of input tokens increases quickly
with this kind of chat.

``` python
chat.use
```

    In: 884; Out: 52; Total: 936

## Other Media

Beyond images, we can also pass in other kind of media to Gaspard, such
as audio file, video files, documents, etc.

For example, letâ€™s try to send a pdf file to the model.

``` python
pdf_fn = Path('samples/attention_is_all_you_need.pdf')
```

``` python
chat = Chat(model)
```

``` python
chat([pdf_fn, "In brief, what are the main ideas of this paper?"])
```

    Uploading media...

This paper proposes a new architecture for sequence transduction models
called the Transformer, which relies entirely on attention mechanisms to
draw global dependencies between input and output. The Transformer
eschews recurrence and convolutions entirely, making it significantly
more parallelizable and requiring less time to train. Experiments on two
machine translation tasks show that the Transformer models are superior
in quality to existing models, including ensembles, while also being
more parallelizable and requiring significantly less time to train. The
authors also show that the Transformer generalizes well to other tasks
by applying it successfully to English constituency parsing both with
large and limited training data.

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜This paper proposes a new architecture
  for sequence transduction models called the Transformer, which relies
  entirely on attention mechanisms to draw global dependencies between
  input and output. The Transformer eschews recurrence and convolutions
  entirely, making it significantly more parallelizable and requiring
  less time to train. Experiments on two machine translation tasks show
  that the Transformer models are superior in quality to existing
  models, including ensembles, while also being more parallelizable and
  requiring significantly less time to train. The authors also show that
  the Transformer generalizes well to other tasks by applying it
  successfully to English constituency parsing both with large and
  limited training data.â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- citation_metadata: {â€˜citation_sourcesâ€™: \[{â€˜start_indexâ€™: 586,
  â€˜end_indexâ€™: 739, â€˜uriâ€™:
  â€˜https://swethatanamala.github.io/2018/12/20/nlp-attention-is-all-you-need/â€™,
  â€˜license\_â€™: â€™â€™}\]}
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 14923
- candidates_token_count: 119
- total_token_count: 15042
- cached_content_token_count: 0

</details>

We can pass in audio files in the same way.

``` python
audio_fn = Path('samples/attention_is_all_you_need.mp3')
```

``` python
pr = "This is a podcast about the same paper. What important details from the paper are not in the podcast?"
```

``` python
chat([audio_fn, pr])
```

    Uploading media...

The podcast focuses on the core ideas and implications of the paper
â€œAttention Is All You Need,â€ but it doesnâ€™t delve into the detailed
technical aspects, such as the mathematical formulation of the scaled
dot-product attention mechanism, the specific hyperparameters used for
training, or the experimental setup and analysis methods.

<details>

- content: {â€˜partsâ€™: \[{â€˜textâ€™: â€˜The podcast focuses on the core ideas
  and implications of the paper â€œAttention Is All You Need,â€ but it
  doesn't delve into the detailed technical aspects, such as the
  mathematical formulation of the scaled dot-product attention
  mechanism, the specific hyperparameters used for training, or the
  experimental setup and analysis methods. â€™}\], â€˜roleâ€™: â€˜modelâ€™}
- finish_reason: 1
- index: 0
- safety_ratings: \[{â€˜categoryâ€™: 9, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False},
  {â€˜categoryâ€™: 8, â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 7,
  â€˜probabilityâ€™: 1, â€˜blockedâ€™: False}, {â€˜categoryâ€™: 10, â€˜probabilityâ€™:
  1, â€˜blockedâ€™: False}\]
- token_count: 0
- grounding_attributions: \[\]
- prompt_token_count: 22905
- candidates_token_count: 61
- total_token_count: 22966
- cached_content_token_count: 0

</details>

You should be careful and monitor usage as the token usage rack up
really fast!

``` python
chat.use
```

    In: 37828; Out: 180; Total: 38008
