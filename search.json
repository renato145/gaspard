[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gaspard",
    "section": "",
    "text": "pip install gaspard",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Gaspard",
    "section": "",
    "text": "pip install gaspard",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Gaspard",
    "section": "Getting started",
    "text": "Getting started\nFollow the instructions to generate an API key, and set it as an evironment variable as shown below:\nexport GEMINI_API_KEY=YOUR_API_KEY\nGemini’s Python SDK will automatically be installed with Gaspard, if you don’t already have it.\n\nfrom gaspard import *\n\nGaspard provides models, which lists the models available in the SDK\n\nmodels\n\n('gemini-2.0-flash-exp',\n 'gemini-exp-1206',\n 'learnlm-1.5-pro-experimental',\n 'gemini-exp-1121',\n 'gemini-1.5-pro',\n 'gemini-1.5-flash',\n 'gemini-1.5-flash-8b')\n\n\nFor our examples we’ll use gemini-2.0-flash-exp since it’s awesome, has a 1M context window and is currently free while in the experimental stage.\n\nmodel = models[0]",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#chat",
    "href": "index.html#chat",
    "title": "Gaspard",
    "section": "Chat",
    "text": "Chat\nThe main interface to Gaspard is the Chat class which provides a stateful interface to the models\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat(\"I'm Faisal\")\n\nHi Faisal, it’s nice to meet you!\n\n\ncontent: {‘parts’: [{‘text’: “Hi Faisal, it’s nice to meet you!”}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.04827792942523956\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 14\ncandidates_token_count: 12\ntotal_token_count: 26\ncached_content_token_count: 0\n\n\n\n\n\nr = chat(\"What's my name?\")\nr\n\nYour name is Faisal.\n\n\ncontent: {‘parts’: [{‘text’: ‘Your name is Faisal.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -1.644962443000016e-05\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 35\ncandidates_token_count: 6\ntotal_token_count: 41\ncached_content_token_count: 0\n\n\n\n\nAs you see above, displaying the results of a call in a notebook shows just the message contents, with the other details hidden behind a collapsible section. Alternatively you can print the details:\n\nprint(r)\n\nresponse:\nGenerateContentResponse(\n    done=True,\n    iterator=None,\n    result=protos.GenerateContentResponse({\n      \"candidates\": [\n        {\n          \"content\": {\n            \"parts\": [\n              {\n                \"text\": \"Your name is Faisal.\\n\"\n              }\n            ],\n            \"role\": \"model\"\n          },\n          \"finish_reason\": \"STOP\",\n          \"safety_ratings\": [\n            {\n              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n              \"probability\": \"NEGLIGIBLE\"\n            }\n          ],\n          \"avg_logprobs\": -1.644962443000016e-05\n        }\n      ],\n      \"usage_metadata\": {\n        \"prompt_token_count\": 35,\n        \"candidates_token_count\": 6,\n        \"total_token_count\": 41\n      }\n    }),\n)\n\n\nYou can use stream=True to stream the results as soon as they arrive (although you will only see the gradual generation if you execute the notebook yourself, of course!)\n\nchat.h\n\n[{'role': 'user', 'parts': [{'text': \"I'm Faisal\"}, ' ']},\n {'role': 'model', 'parts': [\"Hi Faisal, it's nice to meet you!\\n\"]},\n {'role': 'user', 'parts': [{'text': \"What's my name?\"}, ' ']},\n {'role': 'model', 'parts': ['Your name is Faisal.\\n']}]\n\n\n\nfor o in chat(\"What's your name? Tell me your story\", stream=True): print(o, end='')\n\nI don't have a name or a personal story in the way a human does. I am a large language model, created by Google AI. I was trained on a massive amount of text data to be able to communicate and generate human-like text. I don't have a body, feelings, or memories, but I'm here to help you with information and tasks.\n\n\nWoah, welcome back to the land of the living Bard!",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#tool-use",
    "href": "index.html#tool-use",
    "title": "Gaspard",
    "section": "Tool use",
    "text": "Tool use\nTool use lets the model use external tools.\nWe use docments to make defining Python functions as ergonomic as possible. Each parameter (and the return value) should have a type, and a docments comment with the description of what it is. As an example we’ll write a simple function that adds numbers together, and will tell us when it’s being called:\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int=1 # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\nSometimes the model will say something like “according to the sums tool the answer is” – generally we’d rather it just tells the user the answer, so we can use a system prompt to help with this:\n\nsp = \"Never mention what tools you use.\"\n\nWe’ll get the model to add up some long numbers:\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\nTo use tools, pass a list of them to Chat:\n\nchat = Chat(model, sp=sp, tools=[sums])\n\nNow when we call that with our prompt, the model doesn’t return the answer, but instead returns a function_call message, which means we have to call the named function (tool) with the provided parameters:\n\ntype(pr)\n\nstr\n\n\n\nr = chat(pr); r\n\nFinding the sum of 604542.0 and 6458932.0\n\n\nfunction_call { name: “sums” args { fields { key: “b” value { number_value: 6458932 } } fields { key: “a” value { number_value: 604542 } } } }\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘sums’, ‘args’: {‘a’: 604542.0, ‘b’: 6458932.0}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -9.219200364896096e-06\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 77\ncandidates_token_count: 3\ntotal_token_count: 80\ncached_content_token_count: 0\n\n\n\n\nGaspard handles all that for us – we just have to pass along the message, and it all happens automatically:\n\nchat.h\n\n[{'role': 'user', 'parts': [{'text': 'What is 604542+6458932?'}, ' ']},\n {'role': 'model',\n  'parts': [function_call {\n     name: \"sums\"\n     args {\n       fields {\n         key: \"b\"\n         value {\n           number_value: 6458932\n         }\n       }\n       fields {\n         key: \"a\"\n         value {\n           number_value: 604542\n         }\n       }\n     }\n   }]},\n {'role': 'user',\n  'parts': [name: \"sums\"\n   response {\n     fields {\n       key: \"result\"\n       value {\n         number_value: 7063474\n       }\n     }\n   },\n   {'text': ' '}]}]\n\n\n\nchat()\n\n7063474\n\n\ncontent: {‘parts’: [{‘text’: ‘7063474’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.005276891868561506\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 128\ncandidates_token_count: 8\ntotal_token_count: 136\ncached_content_token_count: 0\n\n\n\n\nWe can inspect the history to see what happens under the hood. Gaspard calls the tool with the appropriate variables returned by the function_call message from the model. The result of calling the function is then sent back to the model, which uses that to respond to the user.\n\nchat.h[-3:]\n\n[{'role': 'model',\n  'parts': [function_call {\n     name: \"sums\"\n     args {\n       fields {\n         key: \"b\"\n         value {\n           number_value: 6458932\n         }\n       }\n       fields {\n         key: \"a\"\n         value {\n           number_value: 604542\n         }\n       }\n     }\n   }]},\n {'role': 'user',\n  'parts': [name: \"sums\"\n   response {\n     fields {\n       key: \"result\"\n       value {\n         number_value: 7063474\n       }\n     }\n   },\n   {'text': ' '}]},\n {'role': 'model', 'parts': ['7063474\\n']}]\n\n\nYou can see how many tokens have been used at any time by checking the use property.\n\nchat.use\n\nIn: 205; Out: 11; Total: 216",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#tool-loop",
    "href": "index.html#tool-loop",
    "title": "Gaspard",
    "section": "Tool loop",
    "text": "Tool loop\nWe can do everything needed to use tools in a single step, by using Chat.toolloop. This can even call multiple tools as needed solve a problem. For example, let’s define a tool to handle multiplication:\n\ndef mults(\n    a:int,  # First thing to multiply\n    b:int=1 # Second thing to multiply\n) -&gt; int: # The product of the inputs\n    \"Multiplies a * b.\"\n    print(f\"Finding the product of {a} and {b}\")\n    return a * b\n\nNow with a single call we can calculate (a+b)*2 – by passing show_trace we can see each response from the model in the process:\n\nchat = Chat(model, sp=sp, tools=[sums,mults])\npr = f'Calculate ({a}+{b})*2'\npr\n\n'Calculate (604542+6458932)*2'\n\n\n\ndef pchoice(r): print(r.parts[0])\n\n\nr = chat.toolloop(pr, trace_func=pchoice)\n\nFinding the sum of 604542.0 and 6458932.0\nfunction_call {\n  name: \"sums\"\n  args {\n    fields {\n      key: \"b\"\n      value {\n        number_value: 6458932\n      }\n    }\n    fields {\n      key: \"a\"\n      value {\n        number_value: 604542\n      }\n    }\n  }\n}\n\nFinding the product of 7063474.0 and 2.0\nfunction_call {\n  name: \"mults\"\n  args {\n    fields {\n      key: \"b\"\n      value {\n        number_value: 2\n      }\n    }\n    fields {\n      key: \"a\"\n      value {\n        number_value: 7063474\n      }\n    }\n  }\n}\n\ntext: \"(604542+6458932)*2 = 14126948\\n\"\n\n\n\nWe can see from the trace above that the model correctly calls the sums function first to add the numbers inside the parenthesis and then calls the mults function to multiply the result of the summation by 2. The response sent back to the user is the actual result after performing the chained tool calls, shown below:\n\nr\n\n(604542+6458932)*2 = 14126948\n\n\ncontent: {‘parts’: [{‘text’: ’(604542+6458932)*2 = 14126948’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.00017791306267359426\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 229\ncandidates_token_count: 28\ntotal_token_count: 257\ncached_content_token_count: 0",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#structured-outputs",
    "href": "index.html#structured-outputs",
    "title": "Gaspard",
    "section": "Structured Outputs",
    "text": "Structured Outputs\nIf you just want the immediate result from a single tool, use Client.structured.\n\ncli = Client(model)\n\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int=1 # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\n\ncli.structured(\"What is 604542+6458932\", sums)\n\nFinding the sum of 604542.0 and 6458932.0\n\n\n[7063474.0]\n\n\nThis is particularly useful for getting back structured information, e.g:\n\nclass President(BasicRepr):\n    \"Information about a president of the United States\"\n    def __init__(self, \n                first:str, # first name\n                last:str, # last name\n                spouse:str, # name of spouse\n                years_in_office:str, # format: \"{start_year}-{end_year}\"\n                birthplace:str, # name of city\n                birth_year:int # year of birth, `0` if unknown\n        ):\n        assert re.match(r'\\d{4}-\\d{4}', years_in_office), \"Invalid format: `years_in_office`\"\n        store_attr()\n\n\ncli.structured(\"Provide key information about the 3rd President of the United States\", President)[0]\n\nPresident(first='Thomas', last='Jefferson', spouse='Martha Wayles Skelton', years_in_office='1801-1809', birthplace='Shadwell', birth_year=1743.0)",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#images",
    "href": "index.html#images",
    "title": "Gaspard",
    "section": "Images",
    "text": "Images\nAs everyone knows, when testing image APIs you have to use a cute puppy. But, that’s boring, so here’s a baby hippo instead.\n\nimg_fn = Path('samples/baby_hippo.jpg')\ndisplay.Image(filename=img_fn, width=200)\n\n\n\n\n\n\n\n\nWe create a Chat object as before:\n\nchat = Chat(model)\n\nFor Gaspard, we can simply pass Path objects that repsent the path of the images. To pass multi-part messages, such as an image along with a prompt, we simply pass in a list of items. Note that Gaspard expects each item to be a text or a Path object.\n\nchat([img_fn, \"In brief, is happening in the photo?\"])\n\nCertainly!\nIn the photo, a person’s hand is gently touching the chin of a baby hippopotamus. The hippo is sitting on the ground and appears to be looking straight at the camera.\n\n\ncontent: {‘parts’: [{‘text’: “Certainly!the photo, a person’s hand is gently touching the chin of a baby hippopotamus. The hippo is sitting on the ground and appears to be looking straight at the camera.”}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.3545633316040039\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 268\ncandidates_token_count: 40\ntotal_token_count: 308\ncached_content_token_count: 0\n\n\n\n\nUnder the hood, Gaspard uploads the image using Gemini’s File API and passes a reference to the model. Gemini API will automatically infer the MIME type, and convert it appropriately. NOTE that the image is also included in input tokens.\n\nchat.use\n\nIn: 268; Out: 40; Total: 308\n\n\nAlternatively, Gaspard supports creating a multi-stage chat with separate image and text prompts. For instance, you can pass just the image as the initial prompt (in which case the model will make some general comments about what it sees, which can be VERY detailed depending on the model and often begin with “Certainly!” for some reason), and then follow up with questions in additional prompts:\n\nchat = Chat(model)\nchat(img_fn)\n\nCertainly! Here’s a description of the image you sent:\nOverall Scene:\nThe image is a close-up shot of a baby hippopotamus being gently petted by a human hand. The scene is heartwarming and focuses on the interaction between the adorable hippo calf and the human.\nBaby Hippo:\n\nAppearance: The hippo is a very young calf with a plump, rounded body. Its skin is a mottled gray color, with hints of pink especially around its neck and cheeks. Its eyes are dark and soulful, giving it an endearing expression. The calf has a small, broad snout and tiny, rounded ears.\nPose: The hippo is sitting with its short legs tucked beneath its body. It’s looking directly at the camera with a slightly curious and passive expression.\nTexture: The hippo’s skin appears smooth and moist, suggesting it might be wet or freshly out of the water.\n\nHuman Hand:\n\nPosition: The hand is placed gently under the hippo’s chin and neck, supporting its head. The fingers are slightly curved and not gripping tightly, demonstrating a caring touch.\nTexture: The skin of the hand appears soft and well-maintained.\n\nBackground:\n\nSetting: The background is out of focus, but it appears to be a rocky, possibly aquatic, environment. The textures in the background are muted and do not detract from the main subjects of the photo.\nDate: There’s a watermark saying “Thailand 9/2024”, indicating the location and date of the image.\n\nMood/Tone:\n\nThe image evokes a sense of gentleness and tenderness. The close-up perspective and the gentle touch of the hand create a very intimate and sweet scene.\nThe calf’s innocent expression adds to the overall cuteness and warmth of the image.\n\nOverall Impression:\nThe image captures a beautiful moment of interaction between a human and a very young, vulnerable animal. The photograph emphasizes the gentle nature of the interaction and the sheer adorableness of the baby hippo, making it a very touching and memorable picture.\nLet me know if you would like a description from another perspective or have any other questions about the image!\n\n\ncontent: {‘parts’: [{‘text’: ’Certainly! Here's a description of the image you sent:*Overall Scene:**image is a close-up shot of a baby hippopotamus being gently petted by a human hand. The scene is heartwarming and focuses on the interaction between the adorable hippo calf and the human. *Baby Hippo:Appearance: The hippo is a very young calf with a plump, rounded body. Its skin is a mottled gray color, with hints of pink especially around its neck and cheeks. Its eyes are dark and soulful, giving it an endearing expression. The calf has a small, broad snout and tiny, rounded ears.Pose: The hippo is sitting with its short legs tucked beneath its body. It's looking directly at the camera with a slightly curious and passive expression. Texture:** The hippo's skin appears smooth and moist, suggesting it might be wet or freshly out of the water.*Human Hand:Position: The hand is placed gently under the hippo's chin and neck, supporting its head. The fingers are slightly curved and not gripping tightly, demonstrating a caring touch.Texture:** The skin of the hand appears soft and well-maintained.*Background:Setting: The background is out of focus, but it appears to be a rocky, possibly aquatic, environment. The textures in the background are muted and do not detract from the main subjects of the photo.Date**: There's a watermark saying “Thailand 9/2024”, indicating the location and date of the image.*Mood/Tone:**The image evokes a sense of gentleness and tenderness. The close-up perspective and the gentle touch of the hand create a very intimate and sweet scene.The calf's innocent expression adds to the overall cuteness and warmth of the image.*Overall Impression:**image captures a beautiful moment of interaction between a human and a very young, vulnerable animal. The photograph emphasizes the gentle nature of the interaction and the sheer adorableness of the baby hippo, making it a very touching and memorable picture.me know if you would like a description from another perspective or have any other questions about the image!’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.7025935932741327\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 260\ncandidates_token_count: 472\ntotal_token_count: 732\ncached_content_token_count: 0\n\n\n\n\n\nchat('What direction is the hippo facing?')\n\nThe hippo is facing directly towards the camera.\n\n\ncontent: {‘parts’: [{‘text’: ‘The hippo is facing directly towards the camera.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.06370497941970825\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 742\ncandidates_token_count: 10\ntotal_token_count: 752\ncached_content_token_count: 0\n\n\n\n\n\nchat('What color is it?')\n\nThe hippo is a mottled gray color, with hints of pink especially around its neck and cheeks.\n\n\ncontent: {‘parts’: [{‘text’: ‘The hippo is a mottled gray color, with hints of pink especially around its neck and cheeks.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.08235452175140381\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 760\ncandidates_token_count: 20\ntotal_token_count: 780\ncached_content_token_count: 0\n\n\n\n\nNote that the image is passed in again for every input in the dialog, via the chat history, so the number of input tokens increases quickly with this kind of chat.\n\nchat.use\n\nIn: 1762; Out: 502; Total: 2264",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "index.html#other-media",
    "href": "index.html#other-media",
    "title": "Gaspard",
    "section": "Other Media",
    "text": "Other Media\nBeyond images, we can also pass in other kind of media to Gaspard, such as audio file, video files, documents, etc.\nFor example, let’s try to send a pdf file to the model.\n\npdf_fn = Path('samples/attention_is_all_you_need.pdf')\n\n\nchat = Chat(model)\n\n\nchat([pdf_fn, \"In brief, what are the main ideas of this paper?\"])\n\nCertainly! Here’s a breakdown of the main ideas presented in the paper “Attention is All You Need”:\nCore Contribution: The Transformer Architecture\n\nRejection of Recurrence and Convolution: The paper proposes a novel neural network architecture called the “Transformer” that moves away from traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs). These are the common architectures for tasks involving sequence data.\nSole Reliance on Attention: The Transformer relies solely on the “attention” mechanism to capture relationships within input and output sequences. This is the core novel idea and is in contrast to models using attention in addition to RNNs or CNNs.\nParallelizable: By removing recurrence, the Transformer is highly parallelizable, which allows for faster training, especially on GPUs.\nAttention-Based Encoder-Decoder: The Transformer uses an encoder-decoder architecture, like other sequence-to-sequence models, but the encoder and decoder are based on self-attention mechanisms rather than RNNs or CNNs.\n\nKey Components of the Transformer:\n\nMulti-Head Attention: The Transformer uses multiple “attention heads,” each learning different dependencies. This allows the model to capture information from different representation sub-spaces.\n\nSelf-Attention: Attention mechanism is applied on the same sequence (e.g. input to input or output to output), to capture relations within the sequence itself.\nEncoder-Decoder Attention: Attention mechanism is applied on two sequences (encoder and decoder output) to align sequences between the source and target.\n\nScaled Dot-Product Attention: A specific form of attention that uses dot products to calculate the attention weights with a scaling factor to stabilize the training.\nPosition-wise Feed-Forward Networks: Fully connected networks are applied to each position separately after attention to add non-linearity.\nPositional Encoding: Since the Transformer doesn’t have inherent recurrence or convolutions, positional encodings are added to the input embeddings to encode the sequence order.\n\nExperimental Results and Impact:\n\nSuperior Translation Quality: The paper demonstrates the effectiveness of the Transformer on machine translation tasks (English-to-German and English-to-French). The models achieve state-of-the-art results with significant BLEU score improvements over existing models including RNN and CNN based approaches.\nFaster Training: They show that the Transformer achieves those state-of-the-art results with much less training time compared to other architectures, showing the benefit of parallelization.\nGeneralization to Other Tasks: The Transformer is also shown to work well on English constituency parsing, highlighting its ability to handle other sequence-based problems.\nInterpretability: Through attention visualizations, the paper also suggests that the model learns to capture structural information in the input, making it more interpretable than recurrent methods.\n\nIn Essence:\nThe paper argues for attention as a foundational building block for sequence processing, dispensing with the need for recurrence and convolutions. It introduces the Transformer, a model that leverages attention mechanisms to achieve both better performance and faster training, setting a new state-of-the-art baseline for many tasks such as machine translation.\nLet me know if you’d like any specific aspect clarified further!\n\n\ncontent: {‘parts’: [{‘text’: ’Certainly! Here's a breakdown of the main ideas presented in the paper “Attention is All You Need”:*Core Contribution: The Transformer ArchitectureRejection of Recurrence and Convolution: The paper proposes a novel neural network architecture called the “Transformer” that moves away from traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs). These are the common architectures for tasks involving sequence data.Sole Reliance on Attention: The Transformer relies solely on the “attention” mechanism to capture relationships within input and output sequences. This is the core novel idea and is in contrast to models using attention in addition to RNNs or CNNs.Parallelizable: By removing recurrence, the Transformer is highly parallelizable, which allows for faster training, especially on GPUs.Attention-Based Encoder-Decoder:** The Transformer uses an encoder-decoder architecture, like other sequence-to-sequence models, but the encoder and decoder are based on self-attention mechanisms rather than RNNs or CNNs.*Key Components of the Transformer:Multi-Head Attention: The Transformer uses multiple “attention heads,” each learning different dependencies. This allows the model to capture information from different representation sub-spaces.Self-Attention: Attention mechanism is applied on the same sequence (e.g. input to input or output to output), to capture relations within the sequence itself.Encoder-Decoder Attention: Attention mechanism is applied on two sequences (encoder and decoder output) to align sequences between the source and target.Scaled Dot-Product Attention: A specific form of attention that uses dot products to calculate the attention weights with a scaling factor to stabilize the training.Position-wise Feed-Forward Networks: Fully connected networks are applied to each position separately after attention to add non-linearity.Positional Encoding:** Since the Transformer doesn't have inherent recurrence or convolutions, positional encodings are added to the input embeddings to encode the sequence order.*Experimental Results and Impact:Superior Translation Quality: The paper demonstrates the effectiveness of the Transformer on machine translation tasks (English-to-German and English-to-French). The models achieve state-of-the-art results with significant BLEU score improvements over existing models including RNN and CNN based approaches.Faster Training: They show that the Transformer achieves those state-of-the-art results with much less training time compared to other architectures, showing the benefit of parallelization.Generalization to Other Tasks: The Transformer is also shown to work well on English constituency parsing, highlighting its ability to handle other sequence-based problems.Interpretability:** Through attention visualizations, the paper also suggests that the model learns to capture structural information in the input, making it more interpretable than recurrent methods.*In Essence:**paper argues for attention as a foundational building block for sequence processing, dispensing with the need for recurrence and convolutions. It introduces the Transformer, a model that leverages attention mechanisms to achieve both better performance and faster training, setting a new state-of-the-art baseline for many tasks such as machine translation.me know if you'd like any specific aspect clarified further!’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.7809832342739763\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 14943\ncandidates_token_count: 696\ntotal_token_count: 15639\ncached_content_token_count: 0\n\n\n\n\nWe can pass in audio files in the same way.\n\naudio_fn = Path('samples/attention_is_all_you_need.mp3')\n\n\npr = \"This is a podcast about the same paper. What important details from the paper are not in the podcast?\"\n\n\nchat([audio_fn, pr])\n\nOkay, let’s analyze what details were missing from the podcast discussion of “Attention is All You Need”. Here are some of the key aspects not fully covered:\n1. Deeper Dive into the Math and Mechanics:\n\nDetailed Attention Formula: The podcast mentions “scaled dot product attention” but doesn’t delve into the actual mathematical formula used to calculate the attention weights:\n\nAttention(Q, K, V) = softmax((QK^T) / sqrt(d_k)) * V (where Q=query, K=key, V=value, and d_k is the dimension of the key)\n\nQuery, Key, Value: While mentioned, the exact nature of how Query, Key and Values are generated from input is never made explicit. How are these generated by linear transformations is an essential aspect.\nThe role of the Mask: The mask in decoder’s self-attention is also not covered in depth. Masking is essential for the auto-regressive nature of the output sequence.\nPositional Encoding Equations: The podcast mentioned positional encoding but not the specific sine and cosine formulas and their purpose which are key to how the model retains position information.\n\nPE(pos, 2i) = sin(pos/10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n\nDetailed explanation of how d_model, d_k, d_v and head dimension relate. This is essential to understanding the parameter counts in the model.\n\n2. Architectural Details and Hyperparameters:\n\nNumber of Layers and Model Dimensions: The paper uses 6 layers both on the encoder and decoder side in their basic and large models. The exact dimensionality of the model itself is also crucial to understanding its capacity. The podcast only mentions that they are stacked.\nFeed Forward Layer Details: The point-wise feed-forward network’s dimensionality is essential for model performance. The podcast does not go into depth about it and the dimensionality being used d_ff=2048 is key.\nDropout and Label Smoothing: They are mentioned as a type of regularization, but the specific rates of 0.1 for the base model are never mentioned nor is the label smoothing rate of 0.1. These details are important for reproducibility and performance.\nOptimization Details: There is also no mention of the Adam Optimizer’s Beta parameters of β₁ = 0.9, β2 = 0.98 and € = 10-9. The paper introduces a specific learning rate decay that is not discussed by name.\n\n3. Analysis and Experiments:\n\nComparison to other attention-based models: The paper explains its motivations in relation to other attention-based models (such as memory networks).\nModel variation experiments: The paper contains detailed experiments varying attention head number and dimensionality, different position encoding options, and impact of dropout and size that are not discussed.\nComputational Complexity: The paper explains detailed complexity analysis for different layer types (Recurrent, Convolutional etc) and their implications for training performance, which is only vaguely discussed in the podcast.\nAttention Interpretations: The paper visually highlights patterns in attention weights which provide intuition on what the model is learning, which is not really discussed by name. This allows insights into how the model handles long-distance dependencies.\n\n4. Technical Implementation:\n\nByte-pair encoding: While mentioned, this subword approach and its impact on vocabulary size and performance is never fully discussed.\nBatching: Batching of training examples using the total sequence length is discussed, but the method of how they are batched in terms of approximately 25000 tokens is not explicit in the podcast.\nEnsemble method: Details about how checkpointing and averaging are used to generate model predictions is missing.\n\n5. Broader Context and Future Work\n\nWhy sinusoidal encodings: The paper specifically states that they hypothesized that using fixed functions for position encoding should be better than learning them, this explanation is not given in the podcast.\nFuture Directions: The paper explicitly lays out plans to extend the transformer to handle larger inputs using locality restrictions and applying them to other modalities, which was alluded to, but not explored with the same depth.\n\nIn Summary:\nThe podcast provides a good overview of the high-level ideas of the paper, but it omits several crucial technical details, mathematical equations, model architecture configurations, and experimental analysis. These omissions are quite critical for fully grasping the novelty and impact of the paper’s findings, and for anyone interested in implementing or extending the model. The podcast lacks the quantitative analysis and model variations that the paper presents.\n\n\ncontent: {‘parts’: [{‘text’: ’Okay, let's analyze what details were missing from the podcast discussion of “Attention is All You Need”. Here are some of the key aspects not fully covered:*1. Deeper Dive into the Math and Mechanics:Detailed Attention Formula: The podcast mentions “scaled dot product attention” but doesn't delve into the actual mathematical formula used to calculate the attention weights:Attention(Q, K, V) = softmax((QK^T) / sqrt(d_k)) * V (where Q=query, K=key, V=value, and d_k is the dimension of the key)Query, Key, Value: While mentioned, the exact nature of how Query, Key and Values are generated from input is never made explicit. How are these generated by linear transformations is an essential aspect.The role of the Mask: The mask in decoder's self-attention is also not covered in depth. Masking is essential for the auto-regressive nature of the output sequence.Positional Encoding Equations: The podcast mentioned positional encoding but not the specific sine and cosine formulas and their purpose which are key to how the model retains position information.PE(pos, 2i) = sin(pos/10000^(2i/d_model))PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))Detailed explanation of how d_model, d_k, d_v and head dimension relate.** This is essential to understanding the parameter counts in the model.*2. Architectural Details and Hyperparameters:Number of Layers and Model Dimensions: The paper uses 6 layers both on the encoder and decoder side in their basic and large models. The exact dimensionality of the model itself is also crucial to understanding its capacity. The podcast only mentions that they are stacked.Feed Forward Layer Details: The point-wise feed-forward network's dimensionality is essential for model performance. The podcast does not go into depth about it and the dimensionality being used d_ff=2048 is key.Dropout and Label Smoothing: They are mentioned as a type of regularization, but the specific rates of 0.1 for the base model are never mentioned nor is the label smoothing rate of 0.1. These details are important for reproducibility and performance.Optimization Details:** There is also no mention of the Adam Optimizer's Beta parameters of β₁ = 0.9, β2 = 0.98 and € = 10-9. The paper introduces a specific learning rate decay that is not discussed by name.*3. Analysis and Experiments:Comparison to other attention-based models: The paper explains its motivations in relation to other attention-based models (such as memory networks).Model variation experiments: The paper contains detailed experiments varying attention head number and dimensionality, different position encoding options, and impact of dropout and size that are not discussed.Computational Complexity: The paper explains detailed complexity analysis for different layer types (Recurrent, Convolutional etc) and their implications for training performance, which is only vaguely discussed in the podcast.Attention Interpretations:** The paper visually highlights patterns in attention weights which provide intuition on what the model is learning, which is not really discussed by name. This allows insights into how the model handles long-distance dependencies.*4. Technical Implementation:Byte-pair encoding: While mentioned, this subword approach and its impact on vocabulary size and performance is never fully discussed.Batching: Batching of training examples using the total sequence length is discussed, but the method of how they are batched in terms of approximately 25000 tokens is not explicit in the podcast.Ensemble method:** Details about how checkpointing and averaging are used to generate model predictions is missing.*5. Broader Context and Future WorkWhy sinusoidal encodings: The paper specifically states that they hypothesized that using fixed functions for position encoding should be better than learning them, this explanation is not given in the podcast.Future Directions:** The paper explicitly lays out plans to extend the transformer to handle larger inputs using locality restrictions and applying them to other modalities, which was alluded to, but not explored with the same depth.*In Summary:**podcast provides a good overview of the high-level ideas of the paper, but it omits several crucial technical details, mathematical equations, model architecture configurations, and experimental analysis. These omissions are quite critical for fully grasping the novelty and impact of the paper's findings, and for anyone interested in implementing or extending the model. The podcast lacks the quantitative analysis and model variations that the paper presents.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -1.2337962527821222\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 23502\ncandidates_token_count: 1039\ntotal_token_count: 24541\ncached_content_token_count: 0\n\n\n\n\nYou should be careful and monitor usage as the token usage rack up really fast!\n\nchat.use\n\nIn: 38445; Out: 1735; Total: 40180\n\n\nWe can also use structured outputs with multi-modal data:\n\nclass AudioMetadata(BasicRepr):\n    \"\"\"Class to hold metadata for audio files\"\"\"\n    def __init__(\n        self,\n        n_speakers:int, # Number of speakers\n        topic:str, # Topic discussed\n        summary:str, # 100 word summary\n        transcript:list[str], # Transcript of the audio segmented by speaker\n    ): store_attr()\npr = \"Extract the necessary information from the audio.\"\n\n\naudio_md = cli.structured(mk_msgs([[audio_fn, pr]]), tools=[AudioMetadata])[0]\n\n\nprint(f'Number of speakers: {audio_md.n_speakers}')\nprint(f'Topic: {audio_md.topic}')\nprint(f'Summary: {audio_md.summary}')\ntranscript = '\\n-'.join(list(audio_md.transcript)[:10])\nprint(f'Transcript: {transcript}')\n\nNumber of speakers: 2.0\nTopic: Machine Learning and NLP\nSummary: This podcast discusses the 'Attention is All You Need' research paper by Vaswani et al., focusing on the Transformer model's architecture, its use of attention mechanisms, and its performance on translation tasks.\nTranscript: Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing 'Attention is all you need' by Vaswani at all. Joining us is an expert in machine learning. Welcome.\n-Thanks for having me. I'm excited to discuss this revolutionary paper.\n-Let's start with the core idea. What's the main thrust of this research?\n-The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models.\n-That sounds like a significant departure from previous approaches. What motivated this radical change?\n-The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences.\n-Could you explain what attention mechanisms are, and why they're so crucial in this model?\n-Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi-head attention, which lets the model jointly attend to information from different representation subspaces.\n-Fascinating. How does the Transformer's architecture differ from previous models?\n-The Transformer uses a stack of identical layers for both the encoder and decoder. Each layer has two main components, a multi-head self-attention mechanism, and a position-wise fully connected feed-forward network. This structure allows for more parallelization and efficient computation.",
    "crumbs": [
      "Gaspard"
    ]
  },
  {
    "objectID": "01_toolloop.html",
    "href": "01_toolloop.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "01_toolloop.html#tool-loop",
    "href": "01_toolloop.html#tool-loop",
    "title": "",
    "section": "Tool loop",
    "text": "Tool loop\n\nimport os\n\n\nmodel = models[-1]\n\n\norders = {\n    \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n    \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n    \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n\ncustomers = {\n    \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n               orders=[orders['O1'], orders['O2']]),\n    \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n               orders=[orders['O3']])\n}\n\n\ndef get_customer_info(\n    customer_id:str # ID of the customer\n): # Customer's name, email, phone number, and list of orders\n    \"Retrieves a customer's information and their orders based on the customer ID\"\n    print(f'- Retrieving customer {customer_id}')\n    return customers.get(customer_id, \"Customer not found\")\n\ndef get_order_details(\n    order_id:str # ID of the order\n): # Order's ID, product name, quantity, price, and order status\n    \"Retrieves the details of a specific order based on the order ID\"\n    print(f'- Retrieving order {order_id}')\n    return orders.get(order_id, \"Order not found\")\n\ndef cancel_order(\n    order_id:str # ID of the order to cancel\n)-&gt;bool: # True if the cancellation is successful\n    \"Cancels an order based on the provided order ID\"\n    print(f'- Cancelling order {order_id}')\n    if order_id not in orders: return False\n    orders[order_id]['status'] = 'Cancelled'\n    return True\n\n\ntools = [get_customer_info, get_order_details, cancel_order]\n\n\nchat = Chat(model, tools=tools)\n\n\nr = chat('Can you tell me the email address for customer C2?')\n\n- Retrieving customer C2\n\n\n\nr = chat()\nr\n\nThe email address for customer C2 is jane@example.com.\n\n\ncontent: {‘parts’: [{‘text’: ‘The email address for customer C2 is jane@example.com. ’}], ‘role’: ‘model’}\nfinish_reason: 1\nindex: 0\nsafety_ratings: [{‘category’: 9, ‘probability’: 1, ‘blocked’: False}, {‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}]\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 292\ncandidates_token_count: 14\ntotal_token_count: 306\ncached_content_token_count: 0\n\n\n\n\n\nchat = Chat(model, tools=tools)\nr = chat('Please cancel all orders for customer C1 for me.')\n\n- Retrieving customer C1\n\n\n\nsource\n\nChat.toolloop\n\n Chat.toolloop (pr, max_steps=10, trace_func:Optional[&lt;built-\n                infunctioncallable&gt;]=None, cont_func:Optional[&lt;built-\n                infunctioncallable&gt;]=&lt;function noop&gt;, generation_config:ge\n                neration_types.GenerationConfigType|None=None, safety_sett\n                ings:safety_types.SafetySettingOptions|None=None,\n                stream:bool=False,\n                tools:content_types.FunctionLibraryType|None=None,\n                tool_config:content_types.ToolConfigType|None=None,\n                request_options:helper_types.RequestOptionsType|None=None)\n\nAdd prompt pr to dialog and get a response from the model, automatically following up with tool_use messages\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\n\n\nPrompt to pass to model\n\n\nmax_steps\nint\n10\nMaximum number of tool requests to loop through\n\n\ntrace_func\nOptional\nNone\nFunction to trace tool use steps (e.g print)\n\n\ncont_func\nOptional\nnoop\nFunction that stops loop if returns False\n\n\ngeneration_config\ngeneration_types.GenerationConfigType | None\nNone\n\n\n\nsafety_settings\nsafety_types.SafetySettingOptions | None\nNone\n\n\n\nstream\nbool\nFalse\n\n\n\ntools\ncontent_types.FunctionLibraryType | None\nNone\n\n\n\ntool_config\ncontent_types.ToolConfigType | None\nNone\n\n\n\nrequest_options\nhelper_types.RequestOptionsType | None\nNone\n\n\n\n\n\n\nExported source\n@patch\n@delegates(genai.GenerativeModel.generate_content)\ndef toolloop(self:Chat,\n             pr, # Prompt to pass to model\n             max_steps=10, # Maximum number of tool requests to loop through\n             trace_func:Optional[callable]=None, # Function to trace tool use steps (e.g `print`)\n             cont_func:Optional[callable]=noop, # Function that stops loop if returns False\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response from the model, automatically following up with `tool_use` messages\"\n    r = self(pr, **kwargs)\n    for i in range(max_steps):\n        pt = r.parts[0]\n        if not pt.function_call: break\n        if trace_func: trace_func(r)\n        r = self(**kwargs)\n        if not (cont_func or noop)(self.h[-2]): break\n    if trace_func: trace_func(r)\n    return r\n\n\n\nchat = Chat(model, tools=tools)\nr = chat.toolloop('Please cancel all orders for customer C1 for me.', trace_func=print)\nr\n\n- Retrieving customer C1\nresponse:\nGenerateContentResponse(\n    done=True,\n    iterator=None,\n    result=protos.GenerateContentResponse({\n      \"candidates\": [\n        {\n          \"content\": {\n            \"parts\": [\n              {\n                \"function_call\": {\n                  \"name\": \"get_customer_info\",\n                  \"args\": {\n                    \"customer_id\": \"C1\"\n                  }\n                }\n              }\n            ],\n            \"role\": \"model\"\n          },\n          \"finish_reason\": \"STOP\",\n          \"index\": 0,\n          \"safety_ratings\": [\n            {\n              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            }\n          ]\n        }\n      ],\n      \"usage_metadata\": {\n        \"prompt_token_count\": 162,\n        \"candidates_token_count\": 20,\n        \"total_token_count\": 182\n      }\n    }),\n)\n- Cancelling order O2\nresponse:\nGenerateContentResponse(\n    done=True,\n    iterator=None,\n    result=protos.GenerateContentResponse({\n      \"candidates\": [\n        {\n          \"content\": {\n            \"parts\": [\n              {\n                \"function_call\": {\n                  \"name\": \"cancel_order\",\n                  \"args\": {\n                    \"order_id\": \"O2\"\n                  }\n                }\n              }\n            ],\n            \"role\": \"model\"\n          },\n          \"finish_reason\": \"STOP\",\n          \"index\": 0,\n          \"safety_ratings\": [\n            {\n              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n              \"probability\": \"NEGLIGIBLE\"\n            }\n          ]\n        }\n      ],\n      \"usage_metadata\": {\n        \"prompt_token_count\": 336,\n        \"candidates_token_count\": 18,\n        \"total_token_count\": 354\n      }\n    }),\n)\nresponse:\nGenerateContentResponse(\n    done=True,\n    iterator=None,\n    result=protos.GenerateContentResponse({\n      \"candidates\": [\n        {\n          \"content\": {\n            \"parts\": [\n              {\n                \"text\": \"OK. I have cancelled order O2 for customer C1. Is there anything else? \\n\"\n              }\n            ],\n            \"role\": \"model\"\n          },\n          \"finish_reason\": \"STOP\",\n          \"index\": 0,\n          \"safety_ratings\": [\n            {\n              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            },\n            {\n              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n              \"probability\": \"NEGLIGIBLE\"\n            }\n          ]\n        }\n      ],\n      \"usage_metadata\": {\n        \"prompt_token_count\": 371,\n        \"candidates_token_count\": 18,\n        \"total_token_count\": 389\n      }\n    }),\n)\n\n\nOK. I have cancelled order O2 for customer C1. Is there anything else?\n\n\ncontent: {‘parts’: [{‘text’: ‘OK. I have cancelled order O2 for customer C1. Is there anything else? ’}], ‘role’: ‘model’}\nfinish_reason: 1\nindex: 0\nsafety_ratings: [{‘category’: 9, ‘probability’: 1, ‘blocked’: False}, {‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}]\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 371\ncandidates_token_count: 18\ntotal_token_count: 389\ncached_content_token_count: 0\n\n\n\n\n\nchat.toolloop('What is the status of order O2?')\n\n- Retrieving order O2\n\n\nOrder O2 is now cancelled.\n\n\ncontent: {‘parts’: [{‘text’: ‘Order O2 is now cancelled. ’}], ‘role’: ‘model’}\nfinish_reason: 1\nindex: 0\nsafety_ratings: [{‘category’: 9, ‘probability’: 1, ‘blocked’: False}, {‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}]\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 485\ncandidates_token_count: 7\ntotal_token_count: 492\ncached_content_token_count: 0",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "",
    "section": "0.0.1",
    "text": "0.0.1\n\nInit release"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Gaspard’s source",
    "section": "",
    "text": "Exported source\nmodels = ('gemini-2.0-flash-exp',\n          'gemini-exp-1206',\n          'learnlm-1.5-pro-experimental',\n          'gemini-exp-1121',\n          'gemini-1.5-pro',\n          'gemini-1.5-flash',\n          'gemini-1.5-flash-8b'\n         )\n\n\nThese are the latest version of Gemini models available at the time of writing.\n\nmodel = models[0]\n\nWe’ll use the new gemini-2.0-flash for the examples since it’s awesome, faster and cheaper.",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "Gaspard’s source",
    "section": "",
    "text": "Exported source\nmodels = ('gemini-2.0-flash-exp',\n          'gemini-exp-1206',\n          'learnlm-1.5-pro-experimental',\n          'gemini-exp-1121',\n          'gemini-1.5-pro',\n          'gemini-1.5-flash',\n          'gemini-1.5-flash-8b'\n         )\n\n\nThese are the latest version of Gemini models available at the time of writing.\n\nmodel = models[0]\n\nWe’ll use the new gemini-2.0-flash for the examples since it’s awesome, faster and cheaper.",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#gemini-sdk",
    "href": "core.html#gemini-sdk",
    "title": "Gaspard’s source",
    "section": "Gemini SDK",
    "text": "Gemini SDK\nFollow the instructions to generate an API key, and set it as an evironment variable.\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\n\ncli = genai.GenerativeModel(model)\n\n\nr = cli.generate_content(\"Hi, I'm Faisal!\")\nr\n\nHi Faisal! It’s nice to meet you. I’m an AI, and I’m here to help. What can I do for you today?\n\n\ncontent: {‘parts’: [{‘text’: “Hi Faisal! It’s nice to meet you. I’m an AI, and I’m here to help. What can I do for you today?”}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.23590449725880341\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 8\ncandidates_token_count: 34\ntotal_token_count: 42\ncached_content_token_count: 0",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#formatting-output",
    "href": "core.html#formatting-output",
    "title": "Gaspard’s source",
    "section": "Formatting output",
    "text": "Formatting output\n\nsource\n\nfind_block\n\n find_block (r:collections.abc.Mapping)\n\nFind the content in r.\n\n\n\n\nType\nDetails\n\n\n\n\nr\nMapping\nThe message to look in\n\n\n\n\n\nExported source\ndef find_block(r:abc.Mapping, # The message to look in\n              ):\n    \"Find the content in `r`.\"\n    m = nested_idx(r, 'candidates', 0)\n    if not m: return m\n    if hasattr(m, 'content'): return m.content \n    else: return m\n\n\n\nfind_block(r)\n\nparts {\n  text: \"Hi Faisal! It\\'s nice to meet you. I\\'m an AI, and I\\'m here to help. What can I do for you today?\\n\"\n}\nrole: \"model\"\n\n\n\nsource\n\n\ncontents\n\n contents (r)\n\nHelper to get the contents from response r.\n\n\nExported source\ndef contents(r):\n    \"Helper to get the contents from response `r`.\"\n    blk = find_block(r)\n    if not blk: return r\n    if hasattr(blk, 'parts'): return getattr(blk,'parts')[0].text\n    return blk\n\n\n\ncontents(r)\n\n\"Hi Faisal! It's nice to meet you. I'm an AI, and I'm here to help. What can I do for you today?\\n\"\n\n\n\n\nExported source\n@patch()\ndef _repr_markdown_(self:GenerateContentResponse):\n    met = list(self.to_dict()['candidates'][0].items()) + list(self.to_dict()['usage_metadata'].items())\n    det = '\\n- '.join(f'{k}: {v}' for k,v in met)\n    res = contents(self)\n    if not res: return f\"- {det}\"\n    return f\"\"\"{contents(self)}\\n&lt;details&gt;\\n\\n- {det}\\n\\n&lt;/details&gt;\"\"\"\n\n\n\nr\n\nHi Faisal! It’s nice to meet you. I’m an AI, and I’m here to help. What can I do for you today?\n\n\ncontent: {‘parts’: [{‘text’: “Hi Faisal! It’s nice to meet you. I’m an AI, and I’m here to help. What can I do for you today?”}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.23590449725880341\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 8\ncandidates_token_count: 34\ntotal_token_count: 42\ncached_content_token_count: 0\n\n\n\n\n\nr.usage_metadata\n\nIn: 8; Out: 34; Total: 42\n\n\n\nsource\n\n\nusage\n\n usage (inp=0, out=0)\n\nSlightly more concise version of Usage.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\nint\n0\nNumber of input tokens\n\n\nout\nint\n0\nNumber of output tokens\n\n\n\n\n\nExported source\ndef usage(inp=0, # Number of input tokens\n          out=0  # Number of output tokens\n         ):\n    \"Slightly more concise version of `Usage`.\"\n    return UsageMetadata(prompt_token_count=inp, candidates_token_count=out)\n\n\n\nusage(5)\n\nIn: 5; Out: 0; Total: 5\n\n\n\nsource\n\n\nUsageMetadata.total\n\n UsageMetadata.total ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef total(self:UsageMetadata): return self.prompt_token_count+self.candidates_token_count\n\n\n\nsource\n\n\nUsageMetadata.__repr__\n\n UsageMetadata.__repr__ ()\n\nReturn repr(self).\n\n\nExported source\n@patch\ndef __repr__(self:UsageMetadata): return f'In: {self.prompt_token_count}; Out: {self.candidates_token_count}; Total: {self.total}'\n\n\n\nr.usage_metadata\n\nIn: 8; Out: 34; Total: 42\n\n\n\nsource\n\n\nUsageMetadata.__add__\n\n UsageMetadata.__add__ (b)\n\nAdd together each of input_tokens and output_tokens\n\n\nExported source\n@patch\ndef __add__(self:UsageMetadata, b):\n    \"Add together each of `input_tokens` and `output_tokens`\"\n    return usage(self.prompt_token_count+b.prompt_token_count, self.candidates_token_count+b.candidates_token_count)\n\n\n\nr.usage_metadata+r.usage_metadata\n\nIn: 16; Out: 68; Total: 84",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#creating-messages",
    "href": "core.html#creating-messages",
    "title": "Gaspard’s source",
    "section": "Creating messages",
    "text": "Creating messages\n\ndef mk_msg(content, role='user', **kw):\n    if isinstance(content, GenerateContentResponse):\n        blk = find_block(content)\n        role = blk.role\n        content = blk.parts[0].text\n    if not isinstance(content, list): content=[content]\n    return dict(role=role, parts=content, **kw)\n\n\nprompt = \"I'm Faisal\"\nm = mk_msg(prompt)\nm\n\n{'role': 'user', 'parts': [\"I'm Faisal\"]}\n\n\n\nr = cli.generate_content([m], generation_config=GenerationConfig(max_output_tokens=100))\nr\n\nHi Faisal, it’s nice to meet you! Is there anything I can help you with today?\n\n\ncontent: {‘parts’: [{‘text’: “Hi Faisal, it’s nice to meet you! Is there anything I can help you with today?”}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.17574180256236682\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 5\ncandidates_token_count: 22\ntotal_token_count: 27\ncached_content_token_count: 0\n\n\n\n\n\nmsgs = [mk_msg(prompt), mk_msg(r), mk_msg('I forgot my name. Can you remind me please?')]\nmsgs\n\n[{'role': 'user', 'parts': [\"I'm Faisal\"]},\n {'role': 'model',\n  'parts': [\"Hi Faisal, it's nice to meet you! Is there anything I can help you with today?\\n\"]},\n {'role': 'user', 'parts': ['I forgot my name. Can you remind me please?']}]\n\n\n\ncli.generate_content(msgs, generation_config=GenerationConfig(max_output_tokens=100))\n\nOkay, Faisal, I can help with that! You just told me your name is Faisal. Don’t worry, it happens to everyone sometimes! Is there anything else I can help you with?\n\n\ncontent: {‘parts’: [{‘text’: “Okay, Faisal, I can help with that! You just told me your name is Faisal. Don’t worry, it happens to everyone sometimes! Is there anything else I can help you with?”}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.27289701062579486\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 40\ncandidates_token_count: 43\ntotal_token_count: 83\ncached_content_token_count: 0\n\n\n\n\nLet’s make this a bit easier.\n\nsource\n\nmk_msgs\n\n mk_msgs (msgs:list, **kw)\n\nHelper to set ‘assistant’ role on alternate messages.\n\n\nExported source\ndef mk_msgs(msgs:list, **kw):\n    \"Helper to set 'assistant' role on alternate messages.\"\n    if isinstance(msgs,str): msgs=[msgs]\n    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]\n\n\n\nmsgs = mk_msgs([\"Hi, I'm Faisal!\", r, \"I forgot my name. Can you remind me please?\"]); msgs\n\n[{'role': 'user', 'parts': [\"Hi, I'm Faisal!\"]},\n {'role': 'model',\n  'parts': [\"Hi Faisal, it's nice to meet you! Is there anything I can help you with today?\\n\"]},\n {'role': 'user', 'parts': ['I forgot my name. Can you remind me please?']}]\n\n\n\ncli.generate_content(msgs, generation_config=GenerationConfig(max_output_tokens=100))\n\nOf course! Your name is Faisal. 😊\n\n\ncontent: {‘parts’: [{‘text’: ‘Of course! Your name is Faisal. 😊’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.5640183448791504\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 43\ncandidates_token_count: 10\ntotal_token_count: 53\ncached_content_token_count: 0",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#client",
    "href": "core.html#client",
    "title": "Gaspard’s source",
    "section": "Client",
    "text": "Client\n\nsource\n\nClient\n\n Client (model, cli=None, sp=None)\n\nBasic LLM messages client.\n\n\nExported source\nclass Client:\n    def __init__(self, model, cli=None, sp=None):\n        \"Basic LLM messages client.\"\n        self.model,self.use = model,usage(0,0)\n        self.sp = sp\n        self.c = (cli or genai.GenerativeModel(model, system_instruction=sp))\n\n\n\nc = Client(model)\nc.use\n\nIn: 0; Out: 0; Total: 0\n\n\n\n\nExported source\n@patch\ndef _r(self:Client, r:GenerateContentResponse):\n    \"Store the result of the message and accrue total usage.\"\n    self.result = r\n    if getattr(r,'usage_metadata',None): self.use += r.usage_metadata\n    return r\n\n\n\nc._r(r)\nc.use\n\nIn: 5; Out: 22; Total: 27\n\n\n\nsource\n\n\nget_stream\n\n get_stream (r)\n\nGemini cli requires passing the system prompt when creating the client, so we recreate the client for now.\nTODO: Ask Google to surface this option to generate_content function, since they’re passing the system prompt to each request anyways under the hood.\n\nsource\n\n\nClient.__call__\n\n Client.__call__ (msgs:list, sp:str=None, maxtok=4096, stream:bool=False,\n                  generation_config:generation_types.GenerationConfigType|\n                  None=None, safety_settings:safety_types.SafetySettingOpt\n                  ions|None=None,\n                  tools:content_types.FunctionLibraryType|None=None,\n                  tool_config:content_types.ToolConfigType|None=None, requ\n                  est_options:helper_types.RequestOptionsType|None=None)\n\nMake a call to LLM.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nList of messages in the dialog\n\n\nsp\nstr\nNone\nSystem prompt\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\ngeneration_config\ngeneration_types.GenerationConfigType | None\nNone\n\n\n\nsafety_settings\nsafety_types.SafetySettingOptions | None\nNone\n\n\n\ntools\ncontent_types.FunctionLibraryType | None\nNone\n\n\n\ntool_config\ncontent_types.ToolConfigType | None\nNone\n\n\n\nrequest_options\nhelper_types.RequestOptionsType | None\nNone\n\n\n\n\n\n\nExported source\n@patch\ndef _precall(self:Client, msgs):\n    if not isinstance(msgs,list): msgs = [msgs]\n    msgs = mk_msgs(msgs)\n    return msgs\n\n\n\n\nExported source\n@patch\n@delegates(genai.GenerativeModel.generate_content)\ndef __call__(self:Client,\n             msgs:list, # List of messages in the dialog\n             sp:str=None, # System prompt\n             maxtok=4096, # Maximum tokens\n             stream:bool=False, # Stream response?\n             **kwargs):\n    \"Make a call to LLM.\"\n    if sp: self._set_sp(sp)\n    msgs = self._precall(msgs)\n    gc_params = inspect.signature(GenerationConfig.__init__).parameters\n    gc_kwargs = {k: v for k, v in kwargs.items() if k in gc_params}\n    gen_config = GenerationConfig(max_output_tokens=maxtok, **gc_kwargs)\n    gen_params = inspect.signature(self.c.generate_content).parameters\n    gen_kwargs = {k: v for k, v in kwargs.items() if k in gen_params}\n    r = self.c.generate_content(\n        contents=msgs, generation_config=gen_config, stream=stream, **gen_kwargs)\n    if not stream: return self._r(r)\n    else: return get_stream(map(self._r, r))\n\n\n\nc.c.generate_content('hi').text\n\n'Hi there! How can I help you today?\\n'\n\n\n\nmsgs = ['hi']\n\n\nc(msgs)\n\nHi there! How can I help you today?\n\n\ncontent: {‘parts’: [{‘text’: ‘Hi there! How can I help you today?’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.017421615394678982\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 2\ncandidates_token_count: 11\ntotal_token_count: 13\ncached_content_token_count: 0\n\n\n\n\n\nc.use\n\nIn: 7; Out: 33; Total: 40\n\n\n\nfor o in c(msgs, stream=True): print(o, end='')\n\nHi there! How can I help you today?\n\n\n\nc.use\n\nIn: 11; Out: 44; Total: 55\n\n\nGemini cli requires passing the system prompt when creating the client, but we didn’t pass one at creation time. Let’s make sure that it gets set properly when we call the client later.\n\nsysp = \"Respond only in emojis\"\n\n\nc(msgs, sp=sysp)\n\n👋\n\n\ncontent: {‘parts’: [{‘text’: ‘👋’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.00037874732515774667\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 6\ncandidates_token_count: 2\ntotal_token_count: 8\ncached_content_token_count: 0\n\n\n\n\nWe’ve shown the token usage but we really care about is pricing. Let’s extract the latest pricing from Google into a pricing dict. Currently, the experimental version Gemini 2.0 Flash is free, and the pricing of other experimental models is not entirely clear. Since there’s rumors they are two experimental version of the upcoming Gemini 2.0 Pro, they are priced as 1.5 Pro. Better safe than sorry.\n\n\nExported source\npricing = {  # model type: $ / million tokens (input, output, cache, input_long, output_long, cache_long)\n    'gemini-2.0-flash-exp': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0),\n    'gemini-1.5-pro': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n    'gemini-1.5-flash': (0.075, 0.30, 0.01875, 0.15, 0.60, 0.0375),\n    'gemini-exp-1206': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n    'gemini-exp-1121': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n    'learnlm-1.5-pro-experimental': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625)\n    \n}\n\n\nNow let’s add a cost prop to Client to calculate the total cost.\n\nsource\n\n\nget_pricing\n\n get_pricing (m, u)\n\n\n\nExported source\ndef get_pricing(m, u):\n    return pricing[m][:3] if u.prompt_token_count &lt; 128_000 else pricing[m][3:]\n\n\n\nsource\n\n\nClient.cost\n\n Client.cost ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef cost(self:Client):\n    inp_cost, out_cost, cache_cost = get_pricing(self.model.split('-exp-')[0], self.use)\n    return (self.use.prompt_token_count * inp_cost + self.use.candidates_token_count * out_cost + self.use.cached_content_token_count * cache_cost) / 1e6\n\n\n\nc.cost\n\n0.0\n\n\n\n\nExported source\n@patch\ndef _repr_markdown_(self:Client):\n    if not hasattr(self,'result'): return 'No results yet'\n    msg = contents(self.result)\n    inp_cost,out_cost,_ = get_pricing(self.model.split('-exp-')[0], self.use)\n    in_cost = self.use.prompt_token_count * inp_cost/1e6\n    out_cost = self.use.candidates_token_count * out_cost/1e6\n    cache_cost = self.use.cached_content_token_count * out_cost/1e6\n    return f\"\"\"{msg}\n\n| Metric | Count | Cost (USD) |\n|--------|------:|-----:|\n| Input tokens | {self.use.prompt_token_count:,} | {in_cost:.6f} |\n| Output tokens | {self.use.candidates_token_count:,} | {out_cost:.6f} |\n| Cache tokens | {self.use.cached_content_token_count:,} | {cache_cost:.6f} |\n| **Total** | **{self.use.total:,}** | **${self.cost:.6f}** |\"\"\"\n\n\n\nc\n\n👋\n\n\n\nMetric\nCount\nCost (USD)\n\n\n\n\nInput tokens\n17\n0.000000\n\n\nOutput tokens\n46\n0.000000\n\n\nCache tokens\n0\n0.000000\n\n\nTotal\n63\n$0.000000",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#tool-use",
    "href": "core.html#tool-use",
    "title": "Gaspard’s source",
    "section": "Tool Use",
    "text": "Tool Use\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\n\nsysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\n\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\n\nGoogle’s Genai API handles schema exatraction under the hood, so we can just directly pass the functions\n\nr = c(pr, sp=sysp, tools=[sums])\nr\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘sums’, ‘args’: {‘a’: 604542.0, ‘b’: 6458932.0}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -5.444032770659153e-06\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 86\ncandidates_token_count: 3\ntotal_token_count: 89\ncached_content_token_count: 0\n\n\n\nLooks like our output isn’t pretty anymore. Let’s fix that\n\ncontents(r)\n\n''\n\n\n\nsource\n\ncontents\n\n contents (r)\n\nHelper to get the contents from response r.\n\n\nExported source\ndef contents(r):\n    \"Helper to get the contents from response `r`.\"\n    blk = find_block(r)\n    if not blk: return r\n    \n    if hasattr(blk, 'parts'):\n        part = blk.parts[0]\n        if 'text' in part:\n            return part.text\n        else:\n            return part\n    return blk\n\n\n\ncontents(r)\n\nfunction_call {\n  name: \"sums\"\n  args {\n    fields {\n      key: \"b\"\n      value {\n        number_value: 6458932\n      }\n    }\n    fields {\n      key: \"a\"\n      value {\n        number_value: 604542\n      }\n    }\n  }\n}\n\n\n\nr\n\nfunction_call { name: “sums” args { fields { key: “b” value { number_value: 6458932 } } fields { key: “a” value { number_value: 604542 } } } }\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘sums’, ‘args’: {‘a’: 604542.0, ‘b’: 6458932.0}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -5.444032770659153e-06\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 86\ncandidates_token_count: 3\ntotal_token_count: 89\ncached_content_token_count: 0\n\n\n\n\nB-e-a-utiful…\n\nm = find_block(r); m\n\nparts {\n  function_call {\n    name: \"sums\"\n    args {\n      fields {\n        key: \"b\"\n        value {\n          number_value: 6458932\n        }\n      }\n      fields {\n        key: \"a\"\n        value {\n          number_value: 604542\n        }\n      }\n    }\n  }\n}\nrole: \"model\"\n\n\n\nfunc = m.parts[0].function_call; func\n\nname: \"sums\"\nargs {\n  fields {\n    key: \"b\"\n    value {\n      number_value: 6458932\n    }\n  }\n  fields {\n    key: \"a\"\n    value {\n      number_value: 604542\n    }\n  }\n}\n\n\nLet’s get the returned function call into a format that is expected by call_func.\n\nfor k,v in func.args.items(): print(k, v)\n\na 604542.0\nb 6458932.0\n\n\n\ndef mk_args(args): return {k: v for k,v in args.items()}\n\n\nmk_args(func.args)\n\n{'a': 604542.0, 'b': 6458932.0}\n\n\n\nsource\n\n\nconvert_func\n\n convert_func (f)\n\n\n\nExported source\ndef convert_func(f): return AttrDict(name=f.name, inputs=mk_args(f.args))\n\n\n\nfunc = convert_func(func); func\n\n{'inputs': {'a': 604542.0, 'b': 6458932.0}, 'name': 'sums'}\n\n\n\nns = mk_ns(sums); ns\n\n{'sums': &lt;function __main__.sums(a: int, b: int) -&gt; int&gt;}\n\n\n\nres = call_func(func.name, func.inputs, ns); res\n\nFinding the sum of 604542.0 and 6458932.0\n\n\n7063474.0\n\n\n\ndef mk_msg(content, role='user', **kw):\n    if isinstance(content, GenerateContentResponse): role,content = 'model',contents(content)\n    if isinstance(content, dict): role,content = content['role'],content['parts']\n    if not isinstance(content, list): content=[content]\n    return dict(role=role, parts=content, **kw)\n\n\nsource\n\n\nmk_toolres\n\n mk_toolres (r:collections.abc.Mapping, ns)\n\nCreate a tool_result message from response r.\n\n\n\n\nType\nDetails\n\n\n\n\nr\nMapping\nTool use request response\n\n\nns\n\nNamespace to search for tools\n\n\n\n\n\nExported source\ndef mk_toolres(\n    r:abc.Mapping, # Tool use request response\n    ns, # Namespace to search for tools\n    ):\n    \"Create a `tool_result` message from response `r`.\"\n    parts = find_block(r).parts\n    tcs = [p.function_call for p in parts if hasattr(p, 'function_call')]\n    res = [mk_msg(r)]\n    tc_res = []\n    for func in (tcs or []):\n        if not func: continue\n        func = convert_func(func)\n        cts = call_func(func.name, func.inputs, ns=ns)\n        tc_res.append(FunctionResponse(name=func.name, response={'result': cts}))\n    if tc_res: res.append(mk_msg(tc_res))\n    return res\n\n\n\ntr = mk_toolres(r, ns=ns)\ntr[1]\n\nFinding the sum of 604542.0 and 6458932.0\n\n\n{'role': 'user',\n 'parts': [name: \"sums\"\n  response {\n    fields {\n      key: \"result\"\n      value {\n        number_value: 7063474\n      }\n    }\n  }]}\n\n\n\nmsgs = [pr] + tr; msgs\n\n['What is 604542+6458932?',\n {'role': 'model', 'parts': [function_call {\n     name: \"sums\"\n     args {\n       fields {\n         key: \"b\"\n         value {\n           number_value: 6458932\n         }\n       }\n       fields {\n         key: \"a\"\n         value {\n           number_value: 604542\n         }\n       }\n     }\n   }]},\n {'role': 'user',\n  'parts': [name: \"sums\"\n   response {\n     fields {\n       key: \"result\"\n       value {\n         number_value: 7063474\n       }\n     }\n   }]}]\n\n\n\nsource\n\n\nmk_msgs\n\n mk_msgs (msgs:list, **kw)\n\nHelper to set ‘assistant’ role on alternate messages.\n\n\nExported source\ndef mk_msgs(msgs:list, **kw):\n    \"Helper to set 'assistant' role on alternate messages.\"\n    if isinstance(msgs,str): msgs=[msgs]\n    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]\n\n\n\nmk_msgs(msgs)\n\n[{'role': 'user', 'parts': ['What is 604542+6458932?']},\n {'role': 'model',\n  'parts': [function_call {\n     name: \"sums\"\n     args {\n       fields {\n         key: \"b\"\n         value {\n           number_value: 6458932\n         }\n       }\n       fields {\n         key: \"a\"\n         value {\n           number_value: 604542\n         }\n       }\n     }\n   }]},\n {'role': 'user',\n  'parts': [name: \"sums\"\n   response {\n     fields {\n       key: \"result\"\n       value {\n         number_value: 7063474\n       }\n     }\n   }]}]\n\n\n\nres = c(msgs, sp=sysp, tools=[sums])\nres\n\n7063474\n\n\ncontent: {‘parts’: [{‘text’: ‘7063474’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.09744896739721298\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 136\ncandidates_token_count: 8\ntotal_token_count: 144\ncached_content_token_count: 0\n\n\n\n\nWe can also force a particular set of tools to be used using, tool_config. Here’s an example of how to do that for genai api.\n\ndef mk_tool_config(choose: list)-&gt;dict:\n    return {\"function_calling_config\": {\"mode\": \"ANY\", \"allowed_function_names\": [x.__name__ for x in choose]}}\n\n\ntool_config = mk_tool_config([sums]); tool_config\n\n{'function_calling_config': {'mode': 'ANY',\n  'allowed_function_names': ['sums']}}\n\n\n\nc('Howdy!', tools=[sums], tool_config=tool_config)\n\nfunction_call { name: “sums” args { fields { key: “b” value { number_value: 2 } } fields { key: “a” value { number_value: 1 } } } }\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘sums’, ‘args’: {‘a’: 1.0, ‘b’: 2.0}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.00484422439088424\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 70\ncandidates_token_count: 3\ntotal_token_count: 73\ncached_content_token_count: 0",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#structured-outputs",
    "href": "core.html#structured-outputs",
    "title": "Gaspard’s source",
    "section": "Structured Outputs",
    "text": "Structured Outputs\nWe can also use tool calling to force the model to return structured outputs.\n\n@patch\n@delegates(Client.__call__)\ndef structured(self:Client,\n               msgs:list, # The prompt or list of prompts\n               tools:list, # Namespace to search for tools\n               **kwargs):\n    \"Return the value of all tool calls (generally used for structured outputs)\"\n    if not isinstance(msgs, list): msgs = [msgs]\n    if not isinstance(tools, list): tools = [tools]\n    kwargs['tools'] = tools\n    kwargs['tool_config'] = mk_tool_config(tools)\n    res = self(msgs, **kwargs)\n    ns=mk_ns(*tools)\n    parts = find_block(res).parts\n    funcs = [convert_func(p.function_call) for p in parts if hasattr(p, 'function_call')]\n    tcs = [call_func(func.name, func.inputs, ns=ns) for func in funcs]\n    return tcs\n\n\nclass Recipe(BasicRepr):\n    \"A structure for representing recipes.\"\n    def __init__(self, recipe_name: str, ingredients: list[str]): store_attr()\n\nGemini API schema extraction doesn’t work very well for Class definitions so we define a factory method as a workaround.\n\npr = \"Give me a receipe for chocolate chip cookies\"\nrecipe = c.structured(pr, tools=[Recipe], sp=sysp)[0]; recipe\n\nRecipe(recipe_name='chocolate chip cookies', ingredients=['butter', 'sugar', 'egg', 'vanilla extract', 'flour', 'baking soda', 'salt', 'chocolate chips'])\n\n\nThis works great, however, to handle to complex structured output usecases we need to manually create the schema objects to feed to the GenAI API.\n\nclass Turn(BasicRepr):\n    \"Turn in the conversation\"\n    def __init__(self, msg_a: str, msg_b: str): store_attr()\n\n\nclass Conversation(BasicRepr):\n    \"A conversation between two people\"\n    def __init__(self, turns: list[Turn]): store_attr()\n\nLet’s do this manually using GenAI’s special protobuf schema types.\n\nturn = genai.protos.Schema(\n    type = genai.protos.Type.OBJECT,\n    properties = {\n        'msg_a':  genai.protos.Schema(type=genai.protos.Type.STRING),\n        'msg_b':  genai.protos.Schema(type=genai.protos.Type.STRING),\n    },\n    required=['msg_a', 'msg_b']\n)\n\n\nconvo = genai.protos.Schema(\n    type = genai.protos.Type.OBJECT,\n    properties = {\n        'turns':  genai.protos.Schema(type=genai.protos.Type.ARRAY, items=turn)\n    },\n    required=['turns']\n)\n\nGreat, now, let’s wrap this into the necessary function declaration that will then be used as a tool.\n\ncreate_convo = genai.protos.FunctionDeclaration(\n    name=\"create_convo\",\n    description=\"Creates a conversation\",\n    parameters=convo\n); create_convo\n\nname: \"create_convo\"\ndescription: \"Creates a conversation\"\nparameters {\n  type_: OBJECT\n  properties {\n    key: \"turns\"\n    value {\n      type_: ARRAY\n      items {\n        type_: OBJECT\n        properties {\n          key: \"msg_b\"\n          value {\n            type_: STRING\n          }\n        }\n        properties {\n          key: \"msg_a\"\n          value {\n            type_: STRING\n          }\n        }\n        required: \"msg_a\"\n        required: \"msg_b\"\n      }\n    }\n  }\n  required: \"turns\"\n}\n\n\n\ngen_model = genai.GenerativeModel(model_name=model, tools = [create_convo])\n\n\nresult = gen_model.generate_content(pr, tool_config={'function_calling_config':'ANY'}); result\n\nfunction_call { name: “create_convo” args { fields { key: “turns” value { list_value { values { struct_value { fields { key: “msg_b” value { string_value: “hi” } } fields { key: “msg_a” value { string_value: “hello” } } } } } } } } }\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘create_convo’, ‘args’: {‘turns’: [{‘msg_b’: ‘hi’, ‘msg_a’: ‘hello’}]}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.2613382706275353\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 109\ncandidates_token_count: 13\ntotal_token_count: 122\ncached_content_token_count: 0\n\n\n\n\nGreat, now let’s start by taking our normal JSON schema that we get from the helper function get_schema and converting it to a protobuf schema.\n\njson_schema = get_schema(Conversation); json_schema\n\n{'name': 'Conversation',\n 'description': 'A conversation between two people',\n 'input_schema': {'type': 'object',\n  'properties': {'turns': {'type': 'array',\n    'description': '',\n    'items': {'$ref': '#/$defs/Turn'}}},\n  'title': 'Conversation',\n  'required': ['turns'],\n  '$defs': {'Turn': {'type': 'object',\n    'properties': {'msg_a': {'type': 'string', 'description': ''},\n     'msg_b': {'type': 'string', 'description': ''}},\n    'title': 'Turn',\n    'required': ['msg_a', 'msg_b']}}}}\n\n\n\nsource\n\njson2proto\n\n json2proto (schema_dict)\n\nConvert JSON schema to protobuf schema\n\n# Convert the Conversation schema\nproto_schema = json2proto(json_schema); proto_schema\n\ntype_: OBJECT\nproperties {\n  key: \"turns\"\n  value {\n    type_: ARRAY\n    items {\n      type_: OBJECT\n      properties {\n        key: \"msg_b\"\n        value {\n          type_: STRING\n        }\n      }\n      properties {\n        key: \"msg_a\"\n        value {\n          type_: STRING\n        }\n      }\n      required: \"msg_a\"\n      required: \"msg_b\"\n    }\n  }\n}\nrequired: \"turns\"\n\n\nLet’s now make it into a proper tool.\n\nsource\n\n\ncls2tool\n\n cls2tool (c)\n\n\ncreate_convo = cls2tool(Conversation); create_convo\n\nname: \"Conversation\"\ndescription: \"A conversation between two people\"\nparameters {\n  type_: OBJECT\n  properties {\n    key: \"turns\"\n    value {\n      type_: ARRAY\n      items {\n        type_: OBJECT\n        properties {\n          key: \"msg_b\"\n          value {\n            type_: STRING\n          }\n        }\n        properties {\n          key: \"msg_a\"\n          value {\n            type_: STRING\n          }\n        }\n        required: \"msg_a\"\n        required: \"msg_b\"\n      }\n    }\n  }\n  required: \"turns\"\n}\n\n\n\nresult = gen_model.generate_content(pr, tool_config={'function_calling_config':'ANY'}); result\n\nfunction_call { name: “create_convo” args { fields { key: “turns” value { list_value { values { struct_value { fields { key: “msg_b” value { string_value: “Okay, here is a recipe:\\n\\nIngredients:\\n\\n1 cup (2 sticks) unsalted butter, softened\\n1 cup granulated sugar\\n1 cup packed brown sugar\\n2 teaspoons pure vanilla extract\\n2 large eggs\\n3 cups all-purpose flour\\n1 teaspoon baking soda\\n1 teaspoon salt\\n2 cups chocolate chips\\n\\nInstructions\\n\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\nBeat in the vanilla extract, then the eggs one at a time.\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\nStir in the chocolate chips.\\nDrop by rounded tablespoons onto the prepared baking sheets.\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.” } } fields { key: “msg_a” value { string_value: “Hey, I'd love a chocolate chip cookie recipe.” } } } } } } } } }\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘create_convo’, ‘args’: {‘turns’: [{‘msg_b’: ‘Okay, here is a recipe:\\n\\nIngredients:\\n\\n1 cup (2 sticks) unsalted butter, softened\\n1 cup granulated sugar\\n1 cup packed brown sugar\\n2 teaspoons pure vanilla extract\\n2 large eggs\\n3 cups all-purpose flour\\n1 teaspoon baking soda\\n1 teaspoon salt\\n2 cups chocolate chips\\n\\nInstructions\\n\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\nBeat in the vanilla extract, then the eggs one at a time.\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\nStir in the chocolate chips.\\nDrop by rounded tablespoons onto the prepared baking sheets.\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.’, ‘msg_a’: “Hey, I’d love a chocolate chip cookie recipe.”}]}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\ncitation_metadata: {‘citation_sources’: [{‘start_index’: 925, ‘end_index’: 1046, ‘uri’: ‘https://dinneronceagain.com/tag/cooking/feed/’}]}\navg_logprobs: -0.09633702817170517\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 109\ncandidates_token_count: 253\ntotal_token_count: 362\ncached_content_token_count: 0\n\n\n\n\n\nfunc = contents(result).function_call; func\n\nname: \"create_convo\"\nargs {\n  fields {\n    key: \"turns\"\n    value {\n      list_value {\n        values {\n          struct_value {\n            fields {\n              key: \"msg_b\"\n              value {\n                string_value: \"Okay, here is a recipe:\\\\n\\\\nIngredients:\\\\n\\\\n1 cup (2 sticks) unsalted butter, softened\\\\n1 cup granulated sugar\\\\n1 cup packed brown sugar\\\\n2 teaspoons pure vanilla extract\\\\n2 large eggs\\\\n3 cups all-purpose flour\\\\n1 teaspoon baking soda\\\\n1 teaspoon salt\\\\n2 cups chocolate chips\\\\n\\\\nInstructions\\\\n\\\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\\\nBeat in the vanilla extract, then the eggs one at a time.\\\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\\\nStir in the chocolate chips.\\\\nDrop by rounded tablespoons onto the prepared baking sheets.\\\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.\"\n              }\n            }\n            fields {\n              key: \"msg_a\"\n              value {\n                string_value: \"Hey, I\\'d love a chocolate chip cookie recipe.\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nfunc.args\n\n&lt;proto.marshal.collections.maps.MapComposite&gt;\n\n\n\nargs = mk_args(func.args); args\n\n{'turns': [&lt;proto.marshal.collections.maps.MapComposite object&gt;]}\n\n\nLet’s update mk_args to handle nested proto objects.\n\nsource\n\n\nmk_args\n\n mk_args (args)\n\n\n\nExported source\ndef _convert_proto(o):\n    \"Convert proto objects to Python dicts and lists\"\n    if isinstance(o, (dict,MapComposite)): return {k:_convert_proto(v) for k,v in o.items()}\n    elif isinstance(o, (list,RepeatedComposite)): return [_convert_proto(v) for v in o]\n    elif hasattr(o, 'DESCRIPTOR'): return {k.name:_convert_proto(getattr(o,k.name)) for k in o.DESCRIPTOR.fields}\n    return o\n\n\n\n\nExported source\ndef mk_args(args):\n    if isinstance(args, MapComposite): return _convert_proto(args)\n    return {k: v for k,v in args.items()}\n\n\n\nargs = mk_args(func.args); args\n\n{'turns': [{'msg_b': 'Okay, here is a recipe:\\\\n\\\\nIngredients:\\\\n\\\\n1 cup (2 sticks) unsalted butter, softened\\\\n1 cup granulated sugar\\\\n1 cup packed brown sugar\\\\n2 teaspoons pure vanilla extract\\\\n2 large eggs\\\\n3 cups all-purpose flour\\\\n1 teaspoon baking soda\\\\n1 teaspoon salt\\\\n2 cups chocolate chips\\\\n\\\\nInstructions\\\\n\\\\nPreheat oven to 375 degrees F (190 degrees C). Line baking sheets with parchment paper.\\\\nIn a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.\\\\nBeat in the vanilla extract, then the eggs one at a time.\\\\nIn a separate bowl, whisk together the flour, baking soda, and salt.\\\\nGradually add the dry ingredients to the wet ingredients, mixing until just combined.\\\\nStir in the chocolate chips.\\\\nDrop by rounded tablespoons onto the prepared baking sheets.\\\\nBake for 9-11 minutes, or until golden brown.Let cool on baking sheets for a few minutes before transferring to a wire rack to cool completely.',\n   'msg_a': \"Hey, I'd love a chocolate chip cookie recipe.\"}]}\n\n\n\nsource\n\n\nmk_tool_config\n\n mk_tool_config (choose:list)\n\n\n\nExported source\ndef mk_tool_config(choose: list)-&gt;dict:\n    return {\"function_calling_config\": {\"mode\": \"ANY\", \"allowed_function_names\":\n    [x.__name__ if hasattr(x, '__name__') else x.name for x in choose]}}\n\n\n\nsource\n\n\nClient.structured\n\n Client.structured (msgs:list, tools:list, sp:str=None, maxtok=4096,\n                    stream:bool=False, generation_config:generation_types.\n                    GenerationConfigType|None=None, safety_settings:safety\n                    _types.SafetySettingOptions|None=None,\n                    tool_config:content_types.ToolConfigType|None=None, re\n                    quest_options:helper_types.RequestOptionsType|None=Non\n                    e)\n\nReturn the value of all tool calls (generally used for structured outputs)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nThe prompt or list of prompts\n\n\ntools\nlist\n\nNamespace to search for tools\n\n\nsp\nstr\nNone\nSystem prompt\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\ngeneration_config\ngeneration_types.GenerationConfigType | None\nNone\n\n\n\nsafety_settings\nsafety_types.SafetySettingOptions | None\nNone\n\n\n\ntool_config\ncontent_types.ToolConfigType | None\nNone\n\n\n\nrequest_options\nhelper_types.RequestOptionsType | None\nNone\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Client.__call__)\ndef structured(self:Client,\n               msgs:list, # The prompt or list of prompts\n               tools:list, # Namespace to search for tools\n               **kwargs):\n    \"Return the value of all tool calls (generally used for structured outputs)\"\n    if not isinstance(msgs, list): msgs = [msgs]\n    if not isinstance(tools, list): tools = [tools]\n    kwargs['tools'] = [cls2tool(x) for x in tools]\n    kwargs['tool_config'] = mk_tool_config(kwargs['tools'])\n    res = self(msgs, **kwargs)\n    ns=mk_ns(*tools)\n    parts = find_block(res).parts\n    funcs = [convert_func(p.function_call) for p in parts if hasattr(p, 'function_call')]\n    tcs = [call_func(func.name, func.inputs, ns=ns) for func in funcs]\n    return tcs\n\n\n\npr = \"Create a conversation between Albert Einstein and Robert J. Oppenheimer\"\nconvo = c.structured(pr, tools=[Conversation], sp=sysp)[0]; print(convo)\n\nConversation(turns=[{'msg_b': 'Robert J. Oppenheimer: Indeed, Professor Einstein. Its mysteries both fascinate and, at times, trouble me. Especially those we are now starting to unravel with the atom.', 'msg_a': \"Albert Einstein: The universe is a wondrous place, wouldn't you agree?\"}, {'msg_b': \"Robert J. Oppenheimer: Precisely. The potential for both creation and destruction weighs heavily on my mind. It's a double-edged sword.\", 'msg_a': \"Albert Einstein: Ah yes, the atom. A source of immense power, but also grave responsibility, wouldn't you say?\"}, {'msg_b': 'Robert J. Oppenheimer: I concur. Yet, the forces of politics and conflict often seem to overshadow reason and progress. I fear our work might be used for ends we never intended.', 'msg_a': 'Albert Einstein: It is our duty, as scientists, to guide humanity toward the beneficial application of these discoveries, to prevent it from turning against itself.'}, {'msg_b': 'Robert J. Oppenheimer: I wish I had your certainty, Professor. The path ahead feels fraught with peril. Still, I am grateful for the pursuit of knowledge, even amidst the uncertainty.', 'msg_a': 'Albert Einstein: We must never lose hope that reason and understanding will prevail. The universe operates on elegant principles; if we remain dedicated to truth, we will find our way. '}, {'msg_b': 'Robert J. Oppenheimer: Perhaps, Professor. Perhaps.', 'msg_a': 'Albert Einstein: As am I, my friend. For it is through the exploration of these great mysteries that we glimpse the true nature of the universe and perhaps, our place within it.'}])",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#chat",
    "href": "core.html#chat",
    "title": "Gaspard’s source",
    "section": "Chat",
    "text": "Chat\nWe’ll create a Chat class that will handle formatting of messages and passing along system prompts and tools, so we don’t have to worry about doing that manually each time.\n\nsource\n\nChat\n\n Chat (model:Optional[str]=None, cli:Optional[__main__.Client]=None,\n       sp=None, tools:Optional[list]=None, tool_config:Optional[str]=None)\n\nGemini chat client.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nOptional\nNone\nModel to use (leave empty if passing cli)\n\n\ncli\nOptional\nNone\nClient to use (leave empty if passing model)\n\n\nsp\nNoneType\nNone\nOptional system prompt\n\n\ntools\nOptional\nNone\nList of tools to make available\n\n\ntool_config\nOptional\nNone\nForced tool choice\n\n\n\n\n\nExported source\nclass Chat:\n    def __init__(self,\n                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n                 sp=None, # Optional system prompt\n                 tools:Optional[list]=None,  # List of tools to make available\n                 tool_config:Optional[str]=None): # Forced tool choice\n        \"Gemini chat client.\"\n        assert model or cli\n        self.c = (cli or Client(model, sp=sp))\n        self.h,self.sp,self.tools,self.tool_config = [],sp,tools,tool_config\n\n    @property\n    def use(self): return self.c.use\n    @property\n    def cost(self): return self.c.cost\n\n\n\nsp = \"Never mention what tools you use.\"\nchat = Chat(model, sp=sp)\nchat.use, chat.h\n\n(In: 0; Out: 0; Total: 0, [])\n\n\n\nsource\n\n\nChat.__call__\n\n Chat.__call__ (pr=None, temp=0, maxtok=4096, stream=False,\n                generation_config:generation_types.GenerationConfigType|No\n                ne=None, safety_settings:safety_types.SafetySettingOptions\n                |None=None,\n                tools:content_types.FunctionLibraryType|None=None,\n                tool_config:content_types.ToolConfigType|None=None,\n                request_options:helper_types.RequestOptionsType|None=None)\n\nCall self as a function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\nNoneType\nNone\nPrompt / message\n\n\ntemp\nint\n0\nTemperature\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\ngeneration_config\ngeneration_types.GenerationConfigType | None\nNone\n\n\n\nsafety_settings\nsafety_types.SafetySettingOptions | None\nNone\n\n\n\ntools\ncontent_types.FunctionLibraryType | None\nNone\n\n\n\ntool_config\ncontent_types.ToolConfigType | None\nNone\n\n\n\nrequest_options\nhelper_types.RequestOptionsType | None\nNone\n\n\n\n\n\n\nExported source\n@patch\ndef _stream(self:Chat, res):\n    yield from res\n    self.h += mk_toolres(self.c.result, ns=self.tools)\n\n\n\n\nExported source\n@patch\ndef _post_pr(self:Chat, pr, prev_role):\n    if pr is None and prev_role == 'assistant':\n        raise ValueError(\"Prompt must be given after assistant completion, or use `self.cont_pr`.\")\n    if pr: self.h.append(mk_msg(pr))\n\n\n\n\nExported source\n@patch\ndef _append_pr(self:Chat,\n               pr=None,  # Prompt / message\n              ):\n    prev_role = nested_idx(self.h, -1, 'role') if self.h else 'assistant' # First message should be 'user'\n    if pr and prev_role == 'user': self() # already user request pending\n    self._post_pr(pr, prev_role)\n\n\n\n\nExported source\n@patch\n@delegates(genai.GenerativeModel.generate_content)\ndef __call__(self:Chat,\n             pr=None,  # Prompt / message\n             temp=0, # Temperature\n             maxtok=4096, # Maximum tokens\n             stream=False, # Stream response?\n             **kwargs):\n    if isinstance(pr,str): pr = pr.strip()\n    self._append_pr(pr)\n    if self.tools: kwargs['tools'] = self.tools\n    # NOTE: Gemini specifies tool_choice via tool_config\n    if self.tool_config: kwargs['tool_config'] = mk_tool_config(self.tool_config)\n    res = self.c(self.h, stream=stream, sp=self.sp, temp=temp, maxtok=maxtok, **kwargs)\n    if stream: return self._stream(res)\n    self.h += mk_toolres(self.c.result, ns=self.tools)\n    return res\n\n\n\nchat(\"I'm Faisal\")\n\nIt’s nice to meet you, Faisal.\n\n\ncontent: {‘parts’: [{‘text’: “It’s nice to meet you, Faisal.”}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.014608133922923695\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 12\ncandidates_token_count: 11\ntotal_token_count: 23\ncached_content_token_count: 0\n\n\n\n\nNow let’s make sure that context is passed properly to subsequent calls\n\nchat(\"What's my name?\")\n\nYour name is Faisal.\n\n\ncontent: {‘parts’: [{‘text’: ‘Your name is Faisal.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -2.8847726449991267e-05\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 31\ncandidates_token_count: 6\ntotal_token_count: 37\ncached_content_token_count: 0\n\n\n\n\nWe can check our uage with the use property. As you can see it keeps track of the history of the conversation.\n\nchat.use\n\nIn: 43; Out: 17; Total: 60\n\n\nLet’s make a nice markdown representation for our docs and jupyter notebooks of our chat object.\n\n@patch\ndef _repr_markdown_(self:Chat):\n    if not hasattr(self.c, 'result'): return 'No results yet'\n    last_msg = contents(self.c.result)\n    history = '\\n\\n'.join(f\"**{m['role']}**: {m['parts'][0] if isinstance(m['parts'][0],str) else m['parts'][0].text}\" \n                         for m in self.h if m['role'] in ('user','model'))\n    det = self.c._repr_markdown_().split('\\n\\n')[-1]\n    return f\"\"\"{last_msg}\n\n&lt;details&gt;\n&lt;summary&gt;History&lt;/summary&gt;\n\n{history}\n&lt;/details&gt;\n{det}\"\"\"\n\n\nchat\n\nYour name is Faisal.\n\n\nHistory\n\nuser: I’m Faisal\nmodel: It’s nice to meet you, Faisal.\nuser: What’s my name?\nmodel: Your name is Faisal.\n\n\n\n\nMetric\nCount\nCost (USD)\n\n\n\n\nInput tokens\n43\n0.000000\n\n\nOutput tokens\n17\n0.000000\n\n\nCache tokens\n0\n0.000000\n\n\nTotal\n60\n$0.000000\n\n\n\n\n\nLet’s also make sure that streaming works correctly with the Chat interface\n\nchat = Chat(model, sp=sp)\nfor o in chat(\"I'm Faisal\", stream=True):\n    o = contents(o)\n    if o and isinstance(o, str): print(o, end='')\n\nIt's nice to meet you, Faisal.\n\n\nLet’s also make sure that tool use works with the Chat interface\n\npr = f\"What is {a}+{b}?\"; pr\n\n'What is 604542+6458932?'\n\n\n\nsp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\nchat = Chat(model, sp=sp, tools=[sums])\nr = chat(\"I'm Faisal\")\nr\n\nHello Faisal, how can I help you today?\n\n\ncontent: {‘parts’: [{‘text’: ‘Hello Faisal, how can I help you today?’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.03243409503589977\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 72\ncandidates_token_count: 11\ntotal_token_count: 83\ncached_content_token_count: 0\n\n\n\n\n\nchat(pr)\n\nFinding the sum of 604542.0 and 6458932.0\n\n\nfunction_call { name: “sums” args { fields { key: “b” value { number_value: 6458932 } } fields { key: “a” value { number_value: 604542 } } } }\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘sums’, ‘args’: {‘a’: 604542.0, ‘b’: 6458932.0}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -5.046702123460515e-06\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 103\ncandidates_token_count: 3\ntotal_token_count: 106\ncached_content_token_count: 0\n\n\n\n\nThe model correctly calls the right function in this case.\n\nchat.h[-1]\n\n{'role': 'user',\n 'parts': [name: \"sums\"\n  response {\n    fields {\n      key: \"result\"\n      value {\n        number_value: 7063474\n      }\n    }\n  }]}\n\n\nIf we inspect the history, we can see that the result of the function call has already been added. We can simply call chat() to pass this to the model and get a response.\n\nchat()\n\n7063474\n\n\ncontent: {‘parts’: [{‘text’: ‘7063474’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.10447429120540619\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 153\ncandidates_token_count: 8\ntotal_token_count: 161\ncached_content_token_count: 0\n\n\n\n\nNow let’s make sure that tool_config works correctly by forcing the model to pick a particular function.\n\ndef diff(\n    a:int, # The number to subtract from\n    b:int # The amount to subtract\n) -&gt; int: # Result of subtracting b from a\n    \"Returns a - b.\"\n    print(f\"Finding the diff of {a} and {b}\")\n    return a - b\n\n\nsp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\nchat = Chat(model, sp=sp, tools=[sums, diff], tool_config=[diff])\nr = chat(f\"What is {a}+{b}?\")\nr\n\nFinding the diff of 604542.0 and -6458932.0\n\n\nfunction_call { name: “diff” args { fields { key: “b” value { number_value: -6458932 } } fields { key: “a” value { number_value: 604542 } } } }\n\n\ncontent: {‘parts’: [{‘function_call’: {‘name’: ‘diff’, ‘args’: {‘a’: 604542.0, ‘b’: -6458932.0}}}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.005156605504453182\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 132\ncandidates_token_count: 3\ntotal_token_count: 135\ncached_content_token_count: 0\n\n\n\n\nWe can see that the model calls the function specified by tool_config even though the prompt asks for a summation, which is the expected behvior in this case.",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#images",
    "href": "core.html#images",
    "title": "Gaspard’s source",
    "section": "Images",
    "text": "Images\n\nfn = Path('./samples/puppy.jpg')\ndisplay.Image(filename=fn, width=200)\n\n\n\n\n\n\n\n\nNow that we are passing more than just text, will need a helper function to upload media using Gemini’s File API, which is the recomended way of passing media to the model.\n\nsource\n\nmedia_msg\n\n media_msg (fn:pathlib.Path)\n\n\n\nExported source\ndef media_msg(fn: Path)-&gt;dict:\n    if isinstance(fn, dict): return fn # Already processed\n    f = genai.upload_file(fn)\n    return {'file_data': {'mime_type': f.mime_type, 'file_uri': f.uri}}\n\n\nLet’s also update how we pass in text type messages, to be consistent\n\nsource\n\n\ntext_msg\n\n text_msg (s:str)\n\n\n\nExported source\ndef text_msg(s:str)-&gt;dict:\n    return {'text': s}\n\n\nAnd finally lets add a helper function for make content correctly handles text and other media.\n\n\nExported source\ndef _mk_content(src):\n    \"Create appropriate content data structure based on type of content\"\n    if isinstance(src,str): return text_msg(src)\n    if isinstance(src,FunctionResponse): return src\n    else: return media_msg(src)\n\n\nNow let’s make sure it properly handles text vs. Path objects for media\n\n_mk_content(\"Hi\")\n\n{'text': 'Hi'}\n\n\n\n_mk_content(fn)\n\n{'file_data': {'mime_type': 'image/jpeg',\n  'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/zpoa38voejtt'}}\n\n\nAnd now we need to update mk_msg to be able to handle multimedia messages correctly.\n\nsource\n\n\nmk_msg\n\n mk_msg (content, role='user', **kw)\n\n\n\nExported source\ndef mk_msg(content, role='user', **kw):\n    if isinstance(content, GenerateContentResponse): role,content = 'model',contents(content)\n    if isinstance(content, dict): role,content = content['role'],content['parts']\n    if not isinstance(content, list): content=[content]\n    if role == 'user': \n        if content: ## Gemini errors if the message contains only media and no text\n            if len(content) == 1 and not isinstance(content[0], str): content.append(' ')\n            content = [_mk_content(o) for o in content]\n        else: content = ''\n    return dict(role=role, parts=content, **kw)\n\n\n\nsource\n\n\nmk_msgs\n\n mk_msgs (msgs:list, **kw)\n\nHelper to set ‘assistant’ role on alternate messages.\n\n\nExported source\ndef mk_msgs(msgs:list, **kw):\n    \"Helper to set 'assistant' role on alternate messages.\"\n    if isinstance(msgs,str): msgs=[msgs]\n    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]\n\n\n\nq = \"In brief, what color flowers are in this image?\"\nmk_msgs([fn, q])\n\n[{'role': 'user',\n  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/kmbr0020l5k6'}},\n   {'text': ' '}]},\n {'role': 'model',\n  'parts': ['In brief, what color flowers are in this image?']}]\n\n\n\nmk_msgs(['Hi', 'Nice, to meet you. How can I help?', [fn, q]])\n\n[{'role': 'user', 'parts': [{'text': 'Hi'}]},\n {'role': 'model', 'parts': ['Nice, to meet you. How can I help?']},\n {'role': 'user',\n  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/sfxeop0j911a'}},\n   {'text': 'In brief, what color flowers are in this image?'}]}]\n\n\nNow, we should just be able to pass a list of multimedia content to our Chat client and it should be able to handle it all under the hood. Let’s test it out.\n\nchat(fn)\n\nThis picture features an adorable Cavalier King Charles Spaniel puppy. It has beautiful brown and white fur, and is resting on a grassy area next to a patch of purple flowers. The puppy is looking directly at the camera with a sweet expression.\n\n\ncontent: {‘parts’: [{‘text’: ‘This picture features an adorable Cavalier King Charles Spaniel puppy. It has beautiful brown and white fur, and is resting on a grassy area next to a patch of purple flowers. The puppy is looking directly at the camera with a sweet expression.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.6654894303302376\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 1183\ncandidates_token_count: 49\ntotal_token_count: 1232\ncached_content_token_count: 0\n\n\n\n\n\nchat([fn, q])\n\nThe flowers in the image are purple.\n\n\ncontent: {‘parts’: [{‘text’: ‘The flowers in the image are purple.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.011526683138476478\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 913\ncandidates_token_count: 9\ntotal_token_count: 922\ncached_content_token_count: 0\n\n\n\n\nHooray! That works, let’s double check the history to make sure that everything is properly formatted and stored.\n\nmk_msgs(chat.h)\n\n[{'role': 'user',\n  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/y1u2m1yaahq'}},\n   {'text': ' '}]},\n {'role': 'model',\n  'parts': [\"This is an adorable image! The puppy is a Cavalier King Charles Spaniel with a beautiful coat of white and chestnut brown. It's nestled amongst some lovely purple flowers and green grass. The puppy's big, dark eyes and sweet expression make it irresistibly cute.\"]},\n {'role': 'user',\n  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/xpvi2ey0087o'}},\n   {'text': ' '}]},\n {'role': 'model',\n  'parts': ['The image shows a charming Cavalier King Charles Spaniel puppy, with its distinctive white and chestnut coat, positioned amidst some blooming purple flowers. The pup is resting on the grass, and its gaze is directed towards the viewer. Its soft fur and gentle expression evoke a sense of warmth and tenderness.']},\n {'role': 'user',\n  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/q2k4qztinipo'}},\n   {'text': 'In brief, what color flowers are in this image?'}]},\n {'role': 'model', 'parts': ['The flowers in the image are purple.\\n']}]\n\n\nWhile we are at it, let’s update our markdown representation to handle the new messages.\n\n\nExported source\n@patch\ndef _repr_markdown_(self:Chat):\n    if not hasattr(self.c, 'result'): return 'No results yet'\n    last_msg = contents(self.c.result)\n    \n    def fmt_part(ps):\n        if len(ps) == 1: return fmt_single(ps[0])\n        return '\\n' + '\\n'.join(f'- {fmt_single(p)}' for p in ps)\n        \n    def fmt_single(p):\n        if 'text' in p: return p['text']\n        if 'file_data' in p: return f\"uploaded media: {p['file_data']['mime_type']}\"\n        return str(p)\n        \n    history = '\\n\\n'.join(f\"**{m['role']}**: {fmt_part(m['parts'])}\" \n                         for m in self.h if m['role'] in ('user','model'))\n    det = self.c._repr_markdown_().split('\\n\\n')[-1]\n    return f\"\"\"{last_msg}\n\n&lt;details&gt;\n&lt;summary&gt;History&lt;/summary&gt;\n\n{history}\n&lt;/details&gt;\n{det}\"\"\"\n\n\n\nchat\n\nThe flowers in the image are purple.\n\n\nHistory\n\nuser: - uploaded media: image/jpeg - In brief, what color flowers are in this image?\nmodel: The flowers in the image are purple.\n\n\n\n\nMetric\nCount\nCost (USD)\n\n\n\n\nInput tokens\n270\n0.000000\n\n\nOutput tokens\n9\n0.000000\n\n\nCache tokens\n0\n0.000000\n\n\nTotal\n279\n$0.000000",
    "crumbs": [
      "Gaspard's source"
    ]
  },
  {
    "objectID": "core.html#other-media-audio-video-etc.",
    "href": "core.html#other-media-audio-video-etc.",
    "title": "Gaspard’s source",
    "section": "Other Media (audio, video, etc.)",
    "text": "Other Media (audio, video, etc.)\nUnlike ChatGPT and Claude, Gemini models can also handle audio and video inputs. Since we’re using Gemini’s File API for handling multimedia content, what we have should just work, except we’ll need to make one small modification to the media_msg function. Also, while we are at it, let us also allow for users to pass in the bytes of the content instead of the path to be consistent with our other LLM provider libraries.\n\nsource\n\nmedia_msg\n\n media_msg (media, mime=None)\n\nHandle media input as either Path or bytes, returning dict for Gemini API\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmedia\n\n\nMedia to process (Path|bytes|dict)\n\n\nmime\nNoneType\nNone\nOptional mime type\n\n\nReturns\ndict\n\nDict for Gemini API\n\n\n\n\n\nExported source\ndef media_msg(\n    media, # Media to process (Path|bytes|dict)\n    mime=None # Optional mime type\n)-&gt;dict: # Dict for Gemini API\n    \"Handle media input as either Path or bytes, returning dict for Gemini API\"\n    if isinstance(media, dict): return media # Already processed\n    def _upload(f, mime=None):\n        f = genai.upload_file(f, mime_type=mime)\n        while f.state.name == \"PROCESSING\": time.sleep(2); f = genai.get_file(f.name)\n        return {'file_data': {'mime_type': f.mime_type, 'file_uri': f.uri}}\n    if isinstance(media, (str,Path)): return _upload(media)\n    if isinstance(media, bytes) and mime is None: mime = ft.guess(media).mime\n    return _upload(io.BytesIO(media if isinstance(media, bytes) else media.encode()), mime)\n\n\nSince we’re uploading potentially larger files, we need to wait for the upload and process to complete so that the media is ready to be consumed by the model.\n\nchat = Chat(model)\nimg = fn.read_bytes()\n\n\nchat([img, q])\n\nCertainly! The flowers in the image are a light purple color.\n\n\ncontent: {‘parts’: [{‘text’: ‘Certainly! The flowers in the image are a light purple color.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.45572607333843523\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 270\ncandidates_token_count: 13\ntotal_token_count: 283\ncached_content_token_count: 0\n\n\n\n\n\n# We'll test this with the example from Gemini's docs\nvideo_fn = Path('./samples/selective_attention_test.mp4')\nprompt = \"Answer the question in the video\"\n\n\nchat = Chat(model)\nchat([video_fn, prompt])\n\nThe players wearing white pass the basketball 13 times.\n\n\ncontent: {‘parts’: [{‘text’: ‘The players wearing white pass the basketball 13 times.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.4509180386861165\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 10527\ncandidates_token_count: 12\ntotal_token_count: 10539\ncached_content_token_count: 0\n\n\n\n\nTakes a little while, but works like a charm! Now, let’s try an audio file to make sure it also works.\n\naudio_fn = Path('./samples/attention_is_all_you_need.mp3')\naudio = audio_fn.read_bytes()\nprompt = \"What is the audio about?\"\n\n\nchat([audio, prompt])\n\nThe audio is a podcast discussion about a groundbreaking research paper titled “Attention is All You Need” by Vaswani et al. The podcast features a machine learning expert who explains the core ideas, motivation, and architecture of the Transformer model introduced in the paper. They discuss the significance of attention mechanisms, how the Transformer differs from previous approaches like RNNs, its remarkable performance on machine translation and other sequence transduction tasks, and the broader implications of the research for machine learning and NLP. They also touch upon the limitations and future directions.\n\n\ncontent: {‘parts’: [{‘text’: ‘The audio is a podcast discussion about a groundbreaking research paper titled “Attention is All You Need” by Vaswani et al. The podcast features a machine learning expert who explains the core ideas, motivation, and architecture of the Transformer model introduced in the paper. They discuss the significance of attention mechanisms, how the Transformer differs from previous approaches like RNNs, its remarkable performance on machine translation and other sequence transduction tasks, and the broader implications of the research for machine learning and NLP. They also touch upon the limitations and future directions.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.48403912498837426\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 18387\ncandidates_token_count: 105\ntotal_token_count: 18492\ncached_content_token_count: 0\n\n\n\n\nFinally, let’s check to make sure pdfs work as well.\n\npdf_fn = Path('./samples/attention_is_all_you_need.pdf')\nprompt = \"What's mentioned in this pdf that's not mentioned in the previous podcast?\"\nchat([pdf_fn, prompt])\n\nOkay, here’s a breakdown of what’s in the PDF that wasn’t covered in the podcast:\nTechnical Details of the Transformer:\n\nDetailed Architecture: The PDF provides a much more detailed breakdown of the Transformer architecture, including the specific arrangement of encoder and decoder layers, sub-layers, residual connections, and layer normalization (see Figure 1 and Section 3.1). The podcast gave a high-level overview, whereas the PDF is more specific about the components used to build the model.\nScaled Dot-Product Attention: The document explains the mechanics of “Scaled Dot-Product Attention” (section 3.2.1, Figure 2), and its advantages over additive attention and specifically mentions that dot products are scaled by 1/sqrt(dk). This isn’t mentioned in the podcast.\nMulti-Head Attention: The PDF delves into the purpose of “Multi-Head Attention”, stating that it enables the model to attend to different representation subspaces (section 3.2.2, Figure 2). It also explicitly states that the projections are parameter matrices such as WQ ∈ R^(d_model x d_k), WK ∈ R^(d_model x d_k), etc.. and gives the values for the number of attention layers used. The podcast explained what multi-head attention was, but didn’t delve into the math or specific values used.\nPositional Encodings: The PDF describes the specific sine and cosine functions used for positional encoding (section 3.5) and why they chose this method. This level of detail about this topic isn’t mentioned in the podcast.\nPoint-wise Feed-Forward Networks: The podcast doesn’t go into the details of this topic, but the PDF notes that feed-forward networks in each layer are position-wise, fully connected, and consist of two linear transformations with a ReLU activation in between. It also specifies the dimensionality of input and output as well as the inner layer (section 3.3).\nEmbeddings and Softmax: The PDF states how learned embeddings are used to convert input/output tokens to vectors (section 3.4), and that the same weight matrix is shared between embeddings and the pre-softmax linear transformation. The embeddings are multiplied by the square root of d_model. These details are not mentioned in the podcast.\n\nTraining and Evaluation:\n\nTraining Details: The PDF explains the datasets (WMT 2014 English-German and English-French), the use of byte-pair encoding and word-piece vocabularies, and batching based on approximate sequence lengths (section 5.1). It also specifies how the model is trained using a specific number of training steps, using GPUs, the Adam optimizer and a specific formula for varying the learning rate (sections 5.2, 5.3). The podcast briefly mentions training time, but doesn’t give specific dataset, architecture, or learning details.\nRegularization: The PDF explicitly mentions the use of residual dropout and label smoothing during training and the parameters used (section 5.4). The podcast didn’t discuss this at all.\nPerformance Metrics: The PDF presents specific BLEU scores for various models on the English-German and English-French translation tasks and lists the training cost in FLOPS (section 6.1, Table 2). It also mentions beam search details like beam size and a length penalty. The podcast only mentions the end results of BLEU score.\nModel Variations: The PDF explores various variations of the base model (section 6.2, Table 3), testing different parameters (number of heads, key/value dimensions, dropout rates, etc.), and mentions which variations performed well or poorly. The podcast doesn’t mention any of these experiments.\nEnglish Constituency Parsing Results: The PDF mentions that the transformer performs well on english constituency parsing with results from the Penn Treebank (section 6.3, Table 4) and compares its results against other models. This is only briefly touched on in the podcast, and none of the results from this section is covered.\n\nOther Points\n\nComputational Complexity Analysis: The PDF includes a table (Table 1) that compares the computational complexity, sequential operations and maximum path lengths of self-attention, recurrent and convolutional layers. This is not covered in the podcast.\nAttention Visualizations: The PDF includes visualization examples of attention heads for certain words (Figure 3, 4, and 5), showcasing the behavior and long range dependencies. These visualizations are not shown nor covered in the podcast.\nReferences and Acknowledgements: The document has a complete list of references and acknowledgements. This was not part of the podcast.\n\nIn summary, while the podcast provided a good overview, the PDF offers a deeper technical dive into the Transformer model, along with details about its training, experiments, and evaluations that aren’t touched upon in the audio discussion. The PDF provides specifics that an audience interested in implementing the model or performing experiments would be interested in.\n\n\ncontent: {‘parts’: [{‘text’: ’Okay, here's a breakdown of what's in the PDF that wasn't covered in the podcast:*Technical Details of the Transformer:Detailed Architecture: The PDF provides a much more detailed breakdown of the Transformer architecture, including the specific arrangement of encoder and decoder layers, sub-layers, residual connections, and layer normalization (see Figure 1 and Section 3.1). The podcast gave a high-level overview, whereas the PDF is more specific about the components used to build the model.Scaled Dot-Product Attention: The document explains the mechanics of “Scaled Dot-Product Attention” (section 3.2.1, Figure 2), and its advantages over additive attention and specifically mentions that dot products are scaled by 1/sqrt(dk). This isn't mentioned in the podcast.Multi-Head Attention: The PDF delves into the purpose of “Multi-Head Attention”, stating that it enables the model to attend to different representation subspaces (section 3.2.2, Figure 2). It also explicitly states that the projections are parameter matrices such as WQ ∈ R^(d_model x d_k), WK ∈ R^(d_model x d_k), etc.. and gives the values for the number of attention layers used. The podcast explained what multi-head attention was, but didn't delve into the math or specific values used.Positional Encodings: The PDF describes the specific sine and cosine functions used for positional encoding (section 3.5) and why they chose this method. This level of detail about this topic isn't mentioned in the podcast.Point-wise Feed-Forward Networks: The podcast doesn't go into the details of this topic, but the PDF notes that feed-forward networks in each layer are position-wise, fully connected, and consist of two linear transformations with a ReLU activation in between. It also specifies the dimensionality of input and output as well as the inner layer (section 3.3).Embeddings and Softmax:** The PDF states how learned embeddings are used to convert input/output tokens to vectors (section 3.4), and that the same weight matrix is shared between embeddings and the pre-softmax linear transformation. The embeddings are multiplied by the square root of d_model. These details are not mentioned in the podcast.*Training and Evaluation:Training Details: The PDF explains the datasets (WMT 2014 English-German and English-French), the use of byte-pair encoding and word-piece vocabularies, and batching based on approximate sequence lengths (section 5.1). It also specifies how the model is trained using a specific number of training steps, using GPUs, the Adam optimizer and a specific formula for varying the learning rate (sections 5.2, 5.3). The podcast briefly mentions training time, but doesn't give specific dataset, architecture, or learning details.Regularization: The PDF explicitly mentions the use of residual dropout and label smoothing during training and the parameters used (section 5.4). The podcast didn't discuss this at all.Performance Metrics: The PDF presents specific BLEU scores for various models on the English-German and English-French translation tasks and lists the training cost in FLOPS (section 6.1, Table 2). It also mentions beam search details like beam size and a length penalty. The podcast only mentions the end results of BLEU score.Model Variations: The PDF explores various variations of the base model (section 6.2, Table 3), testing different parameters (number of heads, key/value dimensions, dropout rates, etc.), and mentions which variations performed well or poorly. The podcast doesn't mention any of these experiments.English Constituency Parsing Results:** The PDF mentions that the transformer performs well on english constituency parsing with results from the Penn Treebank (section 6.3, Table 4) and compares its results against other models. This is only briefly touched on in the podcast, and none of the results from this section is covered.*Other PointsComputational Complexity Analysis: The PDF includes a table (Table 1) that compares the computational complexity, sequential operations and maximum path lengths of self-attention, recurrent and convolutional layers. This is not covered in the podcast.Attention Visualizations: The PDF includes visualization examples of attention heads for certain words (Figure 3, 4, and 5), showcasing the behavior and long range dependencies. These visualizations are not shown nor covered in the podcast.References and Acknowledgements:** The document has a complete list of references and acknowledgements. This was not part of the podcast.*In summary, while the podcast provided a good overview, the PDF offers a deeper technical dive into the Transformer model, along with details about its training, experiments, and evaluations that aren't touched upon in the audio discussion.** The PDF provides specifics that an audience interested in implementing the model or performing experiments would be interested in.’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.8560131542336379\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 33441\ncandidates_token_count: 1081\ntotal_token_count: 34522\ncached_content_token_count: 0\n\n\n\n\nGemini does a pretty good job here!!\n\npr = \"Can you generate an exact transcript of the first minute or so of the podcast.\"\nchat(pr)\n\nOkay, here’s the transcript of the first minute of the podcast, based on your request:\n“Welcome to our podcast, where we dive into groundbreaking research papers. Today, we’re discussing ‘Attention is All You Need’ by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I’m excited to discuss this revolutionary paper. Let’s start with the core idea. What’s the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they’re so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.”\n\n\ncontent: {‘parts’: [{‘text’: ‘Okay, here's the transcript of the first minute of the podcast, based on your request:“Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing 'Attention is All You Need' by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they're so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.”’}], ‘role’: ‘model’}\nfinish_reason: 1\nsafety_ratings: [{‘category’: 8, ‘probability’: 1, ‘blocked’: False}, {‘category’: 10, ‘probability’: 1, ‘blocked’: False}, {‘category’: 7, ‘probability’: 1, ‘blocked’: False}, {‘category’: 9, ‘probability’: 1, ‘blocked’: False}]\navg_logprobs: -0.07141794775524278\ntoken_count: 0\ngrounding_attributions: []\nprompt_token_count: 34540\ncandidates_token_count: 274\ntotal_token_count: 34814\ncached_content_token_count: 0\n\n\n\n\n\nchat\n\nOkay, here’s the transcript of the first minute of the podcast, based on your request:\n“Welcome to our podcast, where we dive into groundbreaking research papers. Today, we’re discussing ‘Attention is All You Need’ by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I’m excited to discuss this revolutionary paper. Let’s start with the core idea. What’s the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they’re so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.”\n\n\nHistory\n\nuser: - uploaded media: video/mp4 - Answer the question in the video\nmodel: The players wearing white pass the basketball 13 times.\nuser: - uploaded media: audio/mpeg - What is the audio about?\nmodel: The audio is a podcast discussion about a groundbreaking research paper titled “Attention is All You Need” by Vaswani et al. The podcast features a machine learning expert who explains the core ideas, motivation, and architecture of the Transformer model introduced in the paper. They discuss the significance of attention mechanisms, how the Transformer differs from previous approaches like RNNs, its remarkable performance on machine translation and other sequence transduction tasks, and the broader implications of the research for machine learning and NLP. They also touch upon the limitations and future directions.\nuser: - uploaded media: application/pdf - What’s mentioned in this pdf that’s not mentioned in the previous podcast?\nmodel: Okay, here’s a breakdown of what’s in the PDF that wasn’t covered in the podcast:\nTechnical Details of the Transformer:\n\nDetailed Architecture: The PDF provides a much more detailed breakdown of the Transformer architecture, including the specific arrangement of encoder and decoder layers, sub-layers, residual connections, and layer normalization (see Figure 1 and Section 3.1). The podcast gave a high-level overview, whereas the PDF is more specific about the components used to build the model.\nScaled Dot-Product Attention: The document explains the mechanics of “Scaled Dot-Product Attention” (section 3.2.1, Figure 2), and its advantages over additive attention and specifically mentions that dot products are scaled by 1/sqrt(dk). This isn’t mentioned in the podcast.\nMulti-Head Attention: The PDF delves into the purpose of “Multi-Head Attention”, stating that it enables the model to attend to different representation subspaces (section 3.2.2, Figure 2). It also explicitly states that the projections are parameter matrices such as WQ ∈ R^(d_model x d_k), WK ∈ R^(d_model x d_k), etc.. and gives the values for the number of attention layers used. The podcast explained what multi-head attention was, but didn’t delve into the math or specific values used.\nPositional Encodings: The PDF describes the specific sine and cosine functions used for positional encoding (section 3.5) and why they chose this method. This level of detail about this topic isn’t mentioned in the podcast.\nPoint-wise Feed-Forward Networks: The podcast doesn’t go into the details of this topic, but the PDF notes that feed-forward networks in each layer are position-wise, fully connected, and consist of two linear transformations with a ReLU activation in between. It also specifies the dimensionality of input and output as well as the inner layer (section 3.3).\nEmbeddings and Softmax: The PDF states how learned embeddings are used to convert input/output tokens to vectors (section 3.4), and that the same weight matrix is shared between embeddings and the pre-softmax linear transformation. The embeddings are multiplied by the square root of d_model. These details are not mentioned in the podcast.\n\nTraining and Evaluation:\n\nTraining Details: The PDF explains the datasets (WMT 2014 English-German and English-French), the use of byte-pair encoding and word-piece vocabularies, and batching based on approximate sequence lengths (section 5.1). It also specifies how the model is trained using a specific number of training steps, using GPUs, the Adam optimizer and a specific formula for varying the learning rate (sections 5.2, 5.3). The podcast briefly mentions training time, but doesn’t give specific dataset, architecture, or learning details.\nRegularization: The PDF explicitly mentions the use of residual dropout and label smoothing during training and the parameters used (section 5.4). The podcast didn’t discuss this at all.\nPerformance Metrics: The PDF presents specific BLEU scores for various models on the English-German and English-French translation tasks and lists the training cost in FLOPS (section 6.1, Table 2). It also mentions beam search details like beam size and a length penalty. The podcast only mentions the end results of BLEU score.\nModel Variations: The PDF explores various variations of the base model (section 6.2, Table 3), testing different parameters (number of heads, key/value dimensions, dropout rates, etc.), and mentions which variations performed well or poorly. The podcast doesn’t mention any of these experiments.\nEnglish Constituency Parsing Results: The PDF mentions that the transformer performs well on english constituency parsing with results from the Penn Treebank (section 6.3, Table 4) and compares its results against other models. This is only briefly touched on in the podcast, and none of the results from this section is covered.\n\nOther Points\n\nComputational Complexity Analysis: The PDF includes a table (Table 1) that compares the computational complexity, sequential operations and maximum path lengths of self-attention, recurrent and convolutional layers. This is not covered in the podcast.\nAttention Visualizations: The PDF includes visualization examples of attention heads for certain words (Figure 3, 4, and 5), showcasing the behavior and long range dependencies. These visualizations are not shown nor covered in the podcast.\nReferences and Acknowledgements: The document has a complete list of references and acknowledgements. This was not part of the podcast.\n\nIn summary, while the podcast provided a good overview, the PDF offers a deeper technical dive into the Transformer model, along with details about its training, experiments, and evaluations that aren’t touched upon in the audio discussion. The PDF provides specifics that an audience interested in implementing the model or performing experiments would be interested in.\nuser: Can you generate an exact transcript of the first minute or so of the podcast.\nmodel: Okay, here’s the transcript of the first minute of the podcast, based on your request:\n“Welcome to our podcast, where we dive into groundbreaking research papers. Today, we’re discussing ‘Attention is All You Need’ by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I’m excited to discuss this revolutionary paper. Let’s start with the core idea. What’s the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. That sounds like a significant departure from previous approaches. What motivated this radical change? The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences. Could you explain what attention mechanisms are and why they’re so crucial in this model? Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot product attention and extend it to multi head attention, which lets the model jointly attend to information from different representation sub spaces. Fascinating.”\n\n\n\n\nMetric\nCount\nCost (USD)\n\n\n\n\nInput tokens\n96,895\n0.000000\n\n\nOutput tokens\n1,472\n0.000000\n\n\nCache tokens\n0\n0.000000\n\n\nTotal\n98,367\n$0.000000\n\n\n\n\n\nAll of these also work with Client and can be combined with structured to get structured responses using multi-media data.\n\nclass AudioMetadata(BasicRepr):\n    \"\"\"Class to hold metadata for audio files\"\"\"\n    def __init__(\n        self,\n        n_speakers:int, # Number of speakers\n        topic:str, # Topic discussed\n        summary:str, # 100 word summary\n        transcript:list[str], # Transcript of the audio segmented by speaker\n    ): store_attr()\npr = \"Extract the necessary information from the audio.\"\n\n\nc = Client(model)\naudio_md = c.structured(mk_msgs([[audio_fn, pr]]), tools=[AudioMetadata])[0]\n\n\nprint(f'Number of speakers: {audio_md.n_speakers}')\nprint(f'Topic: {audio_md.topic}')\nprint(f'Summary: {audio_md.summary}')\ntranscript = '\\n-'.join(list(audio_md.transcript)[:10])\nprint(f'Transcript: {transcript}')\n\nNumber of speakers: 2.0\nTopic: Machine Learning, Natural Language Processing\nSummary: This podcast discusses the Attention is All You Need research paper, focusing on the Transformer model's architecture, attention mechanisms, performance, and broader implications.\nTranscript: Welcome to our podcast, where we dive into groundbreaking research papers. Today, we're discussing attention is all you need by Vaswani at all. Joining us is an expert in machine learning. Welcome.\n-Thanks for having me. I'm excited to discuss this revolutionary paper.\n-Let's start with the core idea. What's the main thrust of this research?\n-The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models.\n-That sounds like a significant departure from previous approaches. What motivated this radical change?\n-The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs. This sequential computation hindered parallelization and made it challenging to learn long-range dependencies in sequences.\n-Could you explain what attention mechanisms are and why they're so crucial in this model?\n-Certainly. Attention allows the model to focus on different parts of the input sequence when producing each part of the output. In the Transformer, they use a specific type called scaled dot-product attention and extend it to multi-head attention, which lets the model jointly attend to information from different representation sub-spaces.\n-Fascinating. How does the Transformer's architecture differ from previous models?\n-The Transformer uses a stack of identical layers for both the encoder and decoder. Each layer has two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. This structure allows for more parallelization and efficient computation.",
    "crumbs": [
      "Gaspard's source"
    ]
  }
]